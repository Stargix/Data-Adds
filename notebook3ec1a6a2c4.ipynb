{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 120867,
          "databundleVersionId": 14453560,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook3ec1a6a2c4",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "M_cEdDKrGRo4"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "smadex_challenge_predict_the_revenue_path = kagglehub.competition_download('smadex-challenge-predict-the-revenue')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "OXwNdSF3GRo_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "_M_kwcLPGRpC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "dask.config.set({\"dataframe.convert-string\": False})\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import gc\n",
        "\n",
        "TRAIN_PATH = \"/kaggle/input/smadex-challenge-predict-the-revenue/train/train\"\n",
        "TEST_PATH  = \"/kaggle/input/smadex-challenge-predict-the-revenue/test/test\"\n",
        "\n",
        "TARGET_COL = \"iap_revenue_d7\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:25:06.525188Z",
          "iopub.execute_input": "2025-11-15T14:25:06.525606Z",
          "iopub.status.idle": "2025-11-15T14:25:08.864745Z",
          "shell.execute_reply.started": "2025-11-15T14:25:06.525576Z",
          "shell.execute_reply": "2025-11-15T14:25:08.863775Z"
        },
        "id": "vtZioGhSGRpE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Columnas monstruosas que casi seguro van con listas/mapas/histogramas.\n",
        "# Las quitamos del baseline para RAM y simplicidad.\n",
        "ignore_big_cols = [\n",
        "    \"bundles_ins\",\n",
        "    \"user_bundles\",\n",
        "    \"user_bundles_l28d\",\n",
        "    \"city_hist\",\n",
        "    \"country_hist\",\n",
        "    \"region_hist\",\n",
        "    \"dev_language_hist\",\n",
        "    \"dev_osv_hist\",\n",
        "    \"bcat\",\n",
        "    \"bcat_bottom_taxonomy\",\n",
        "    \"bundles_cat\",\n",
        "    \"bundles_cat_bottom_taxonomy\",\n",
        "    \"first_request_ts_bundle\",\n",
        "    \"first_request_ts_category_bottom_taxonomy\",\n",
        "    \"last_buy_ts_bundle\",\n",
        "    \"last_buy_ts_category\",\n",
        "    \"last_install_ts_bundle\",\n",
        "    \"last_install_ts_category\",\n",
        "    \"advertiser_actions_action_count\",\n",
        "    \"advertiser_actions_action_last_timestamp\",\n",
        "    \"user_actions_bundles_action_count\",\n",
        "    \"user_actions_bundles_action_last_timestamp\",\n",
        "    \"new_bundles\",\n",
        "    \"whale_users_bundle_num_buys_prank\",\n",
        "    \"whale_users_bundle_revenue_prank\",\n",
        "    \"whale_users_bundle_total_num_buys\",\n",
        "    \"whale_users_bundle_total_revenue\",\n",
        "]\n",
        "\n",
        "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type == \"float64\":\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "        elif col_type == \"int64\":\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "    return df\n",
        "\n",
        "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
        "    \"\"\"Detecta columnas que contienen listas o dicts.\"\"\"\n",
        "    if cols is None:\n",
        "        cols = df.columns\n",
        "    listlike = []\n",
        "    for c in cols:\n",
        "        sample_vals = df[c].head(100)\n",
        "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
        "            listlike.append(c)\n",
        "    return listlike\n",
        "\n",
        "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
        "    \"\"\"Preprocesado para train/valid.\"\"\"\n",
        "    X_train = X_train.copy()\n",
        "    X_valid = X_valid.copy()\n",
        "\n",
        "    # Numéricas: NaN -> 0\n",
        "    for c in num_cols:\n",
        "        X_train[c] = X_train[c].fillna(0)\n",
        "        X_valid[c] = X_valid[c].fillna(0)\n",
        "\n",
        "    # Categóricas: strings + categorías fijas basadas en train\n",
        "    for c in cat_cols:\n",
        "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_train[c] = X_train[c].astype(\"category\")\n",
        "\n",
        "        cats = X_train[c].cat.categories\n",
        "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_valid[c] = X_valid[c].astype(\n",
        "            pd.api.types.CategoricalDtype(categories=cats)\n",
        "        )\n",
        "\n",
        "    return X_train, X_valid\n",
        "\n",
        "def preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n",
        "    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n",
        "    X_new = X_new.copy()\n",
        "\n",
        "    for c in num_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].fillna(0)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "            cats = cat_ref_df[c].cat.categories\n",
        "            X_new[c] = X_new[c].astype(\n",
        "                pd.api.types.CategoricalDtype(categories=cats)\n",
        "            )\n",
        "\n",
        "    return X_new"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:25:08.865594Z",
          "iopub.execute_input": "2025-11-15T14:25:08.866067Z",
          "iopub.status.idle": "2025-11-15T14:25:08.87851Z",
          "shell.execute_reply.started": "2025-11-15T14:25:08.866047Z",
          "shell.execute_reply": "2025-11-15T14:25:08.87755Z"
        },
        "id": "yxYY_yRtGRpF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train: 1–5 de octubre\n",
        "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
        "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
        "\n",
        "# Valid: día 6 de octubre\n",
        "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
        "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
        "\n",
        "dd_train = dd.read_parquet(TRAIN_PATH, filters=filters_train)\n",
        "dd_valid = dd.read_parquet(TRAIN_PATH, filters=filters_valid)\n",
        "\n",
        "# Quitar las columnas monstruosas si existen\n",
        "existing_big_cols_train = [c for c in ignore_big_cols if c in dd_train.columns]\n",
        "existing_big_cols_valid = [c for c in ignore_big_cols if c in dd_valid.columns]\n",
        "\n",
        "dd_train = dd_train.drop(columns=existing_big_cols_train)\n",
        "dd_valid = dd_valid.drop(columns=existing_big_cols_valid)\n",
        "\n",
        "# Muestreo de train: AJUSTA ESTO si quieres más datos\n",
        "frac_train = 0.10  # 10% de train; puedes subir a 0.2 si ves que va bien\n",
        "\n",
        "train_sample = dd_train.sample(frac=frac_train, random_state=42).compute()\n",
        "valid_df     = dd_valid.compute()\n",
        "\n",
        "train_sample = reduce_memory(train_sample)\n",
        "valid_df     = reduce_memory(valid_df)\n",
        "\n",
        "print(\"Train sample shape:\", train_sample.shape)\n",
        "print(\"Valid shape:\", valid_df.shape)\n",
        "print(\"Train memory (GB):\", train_sample.memory_usage(deep=True).sum() / (1024**3))\n",
        "print(\"Valid memory (GB):\", valid_df.memory_usage(deep=True).sum() / (1024**3))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:25:08.879459Z",
          "iopub.execute_input": "2025-11-15T14:25:08.87978Z",
          "iopub.status.idle": "2025-11-15T14:31:01.312169Z",
          "shell.execute_reply.started": "2025-11-15T14:25:08.87975Z",
          "shell.execute_reply": "2025-11-15T14:31:01.311301Z"
        },
        "id": "-Li_-OkSGRpH",
        "outputId": "90641ad0-2ada-4096-a14c-0fdebe1b1d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train sample shape: (1729408, 58)\nValid shape: (3306478, 58)\nTrain memory (GB): 2.52042169123888\nValid memory (GB): 4.839879894629121\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Todas las labels auxiliares que SOLO están en train\n",
        "LABEL_COLS = [\n",
        "    \"buyer_d1\",\n",
        "    \"buyer_d7\",\n",
        "    \"buyer_d14\",\n",
        "    \"buyer_d28\",\n",
        "    \"buy_d7\",\n",
        "    \"buy_d14\",\n",
        "    \"buy_d28\",\n",
        "    \"iap_revenue_d7\",   # target principal\n",
        "    \"iap_revenue_d14\",\n",
        "    \"iap_revenue_d28\",\n",
        "    \"registration\",\n",
        "    \"retention_d1_to_d7\",\n",
        "    \"retention_d3_to_d7\",\n",
        "    \"retention_d7_to_d14\",\n",
        "    \"retention_d1\",\n",
        "    \"retention_d3\",\n",
        "    \"retentiond7\",\n",
        "]\n",
        "\n",
        "TARGET_COL = \"iap_revenue_d7\"\n",
        "\n",
        "assert TARGET_COL in train_sample.columns, \"No está iap_revenue_d7 en train\"\n",
        "\n",
        "# y_train / y_valid: solo la target principal\n",
        "y_train = train_sample[TARGET_COL].values\n",
        "y_valid = valid_df[TARGET_COL].values\n",
        "\n",
        "# Columnas que NO queremos como features\n",
        "cols_to_drop_from_X = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
        "\n",
        "# Features = todas las demás\n",
        "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop_from_X]\n",
        "\n",
        "print(\"Número de features:\", len(feature_cols))\n",
        "\n",
        "X_train = train_sample[feature_cols].copy()\n",
        "X_valid = valid_df[feature_cols].copy()\n",
        "\n",
        "# 1) Detectar columnas con listas/dicts y quitarlas\n",
        "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
        "print(\"Columnas con listas/dicts:\", listlike_cols)\n",
        "\n",
        "X_train = X_train.drop(columns=listlike_cols)\n",
        "X_valid = X_valid.drop(columns=listlike_cols)\n",
        "\n",
        "# 2) Volver a calcular numéricas y categóricas\n",
        "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
        "\n",
        "print(\"Numéricas:\", len(num_cols))\n",
        "print(\"Categóricas:\", len(cat_cols))\n",
        "\n",
        "# 3) Preprocesar\n",
        "X_train_prep, X_valid_prep = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n",
        "\n",
        "print(\"X_train_prep shape:\", X_train_prep.shape)\n",
        "print(\"X_valid_prep shape:\", X_valid_prep.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:31:01.314071Z",
          "iopub.execute_input": "2025-11-15T14:31:01.314349Z",
          "iopub.status.idle": "2025-11-15T14:31:21.755462Z",
          "shell.execute_reply.started": "2025-11-15T14:31:01.314327Z",
          "shell.execute_reply": "2025-11-15T14:31:21.754632Z"
        },
        "id": "pNaidivyGRpK",
        "outputId": "906e4395-6204-4ce7-d4cc-224c7ebb87c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Número de features: 39\nColumnas con listas/dicts: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rev_by_adv', 'rwd_prank']\nNuméricas: 10\nCategóricas: 14\nX_train_prep shape: (1729408, 24)\nX_valid_prep shape: (3306478, 24)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_valid_log = np.log1p(y_valid)\n",
        "\n",
        "model = LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=255,\n",
        "    max_depth=-1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=0.0,\n",
        "    verbosity=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train_prep, y_train_log)\n",
        "\n",
        "print(\"Modelo entrenado.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:31:21.75612Z",
          "iopub.execute_input": "2025-11-15T14:31:21.756339Z",
          "iopub.status.idle": "2025-11-15T14:32:42.109623Z",
          "shell.execute_reply.started": "2025-11-15T14:31:21.756321Z",
          "shell.execute_reply": "2025-11-15T14:32:42.108668Z"
        },
        "id": "bFGe1otoGRpM",
        "outputId": "3f485afe-c7d0-4a28-8173-05c861c019bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Modelo entrenado.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción en espacio log\n",
        "valid_pred_log = model.predict(X_valid_prep)\n",
        "\n",
        "# Volver al espacio original\n",
        "valid_pred = np.expm1(valid_pred_log)\n",
        "valid_pred = np.clip(valid_pred, 0, None)\n",
        "\n",
        "msle_model = mean_squared_log_error(y_valid, valid_pred)\n",
        "print(\"MSLE modelo:\", msle_model)\n",
        "\n",
        "zeros_pred = np.zeros_like(y_valid)\n",
        "msle_zeros = mean_squared_log_error(y_valid, zeros_pred)\n",
        "print(\"MSLE baseline (todo 0):\", msle_zeros)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:32:42.110563Z",
          "iopub.execute_input": "2025-11-15T14:32:42.111252Z",
          "iopub.status.idle": "2025-11-15T14:37:03.5407Z",
          "shell.execute_reply.started": "2025-11-15T14:32:42.111201Z",
          "shell.execute_reply": "2025-11-15T14:37:03.53967Z"
        },
        "id": "En06DHuxGRpN",
        "outputId": "e844c3e8-e72f-4627-e800-7f3f2ab5b9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "MSLE modelo: 0.17942944557952253\nMSLE baseline (todo 0): 0.21498893\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Conservamos SOLO lo imprescindible para el test: model, X_train_prep, num_cols, cat_cols\n",
        "to_keep = {\"model\", \"X_train_prep\", \"num_cols\", \"cat_cols\"}\n",
        "\n",
        "for name in list(globals().keys()):\n",
        "    if name.startswith(\"_\"):\n",
        "        continue\n",
        "    if name in to_keep:\n",
        "        continue\n",
        "    # No borramos módulos (dask, pd, np, etc.)\n",
        "    if isinstance(globals()[name], type(os)):\n",
        "        continue\n",
        "    try:\n",
        "        del globals()[name]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:37:03.541748Z",
          "iopub.execute_input": "2025-11-15T14:37:03.542065Z",
          "iopub.status.idle": "2025-11-15T14:37:13.366563Z",
          "shell.execute_reply.started": "2025-11-15T14:37:03.542031Z",
          "shell.execute_reply": "2025-11-15T14:37:13.365689Z"
        },
        "id": "c4gX7fyvGRpP",
        "outputId": "13140b1f-fe56-4921-a3cd-a5b89d93e975"
      },
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "4"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Columnas monstruosas que casi seguro van con listas/mapas/histogramas.\n",
        "# Las quitamos del baseline para RAM y simplicidad.\n",
        "ignore_big_cols = [\n",
        "    \"bundles_ins\",\n",
        "    \"user_bundles\",\n",
        "    \"user_bundles_l28d\",\n",
        "    \"city_hist\",\n",
        "    \"country_hist\",\n",
        "    \"region_hist\",\n",
        "    \"dev_language_hist\",\n",
        "    \"dev_osv_hist\",\n",
        "    \"bcat\",\n",
        "    \"bcat_bottom_taxonomy\",\n",
        "    \"bundles_cat\",\n",
        "    \"bundles_cat_bottom_taxonomy\",\n",
        "    \"first_request_ts_bundle\",\n",
        "    \"first_request_ts_category_bottom_taxonomy\",\n",
        "    \"last_buy_ts_bundle\",\n",
        "    \"last_buy_ts_category\",\n",
        "    \"last_install_ts_bundle\",\n",
        "    \"last_install_ts_category\",\n",
        "    \"advertiser_actions_action_count\",\n",
        "    \"advertiser_actions_action_last_timestamp\",\n",
        "    \"user_actions_bundles_action_count\",\n",
        "    \"user_actions_bundles_action_last_timestamp\",\n",
        "    \"new_bundles\",\n",
        "    \"whale_users_bundle_num_buys_prank\",\n",
        "    \"whale_users_bundle_revenue_prank\",\n",
        "    \"whale_users_bundle_total_num_buys\",\n",
        "    \"whale_users_bundle_total_revenue\",\n",
        "]\n",
        "\n",
        "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type == \"float64\":\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "        elif col_type == \"int64\":\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "    return df\n",
        "\n",
        "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
        "    \"\"\"Detecta columnas que contienen listas o dicts.\"\"\"\n",
        "    if cols is None:\n",
        "        cols = df.columns\n",
        "    listlike = []\n",
        "    for c in cols:\n",
        "        sample_vals = df[c].head(100)\n",
        "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
        "            listlike.append(c)\n",
        "    return listlike\n",
        "\n",
        "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
        "    \"\"\"Preprocesado para train/valid.\"\"\"\n",
        "    X_train = X_train.copy()\n",
        "    X_valid = X_valid.copy()\n",
        "\n",
        "    # Numéricas: NaN -> 0\n",
        "    for c in num_cols:\n",
        "        X_train[c] = X_train[c].fillna(0)\n",
        "        X_valid[c] = X_valid[c].fillna(0)\n",
        "\n",
        "    # Categóricas: strings + categorías fijas basadas en train\n",
        "    for c in cat_cols:\n",
        "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_train[c] = X_train[c].astype(\"category\")\n",
        "\n",
        "        cats = X_train[c].cat.categories\n",
        "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_valid[c] = X_valid[c].astype(\n",
        "            pd.api.types.CategoricalDtype(categories=cats)\n",
        "        )\n",
        "\n",
        "    return X_train, X_valid\n",
        "\n",
        "def preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n",
        "    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n",
        "    X_new = X_new.copy()\n",
        "\n",
        "    for c in num_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].fillna(0)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "            cats = cat_ref_df[c].cat.categories\n",
        "            X_new[c] = X_new[c].astype(\n",
        "                pd.api.types.CategoricalDtype(categories=cats)\n",
        "            )\n",
        "\n",
        "    return X_new"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:37:13.367453Z",
          "iopub.execute_input": "2025-11-15T14:37:13.367734Z",
          "iopub.status.idle": "2025-11-15T14:37:13.380493Z",
          "shell.execute_reply.started": "2025-11-15T14:37:13.367709Z",
          "shell.execute_reply": "2025-11-15T14:37:13.379262Z"
        },
        "id": "2fI9MGGqGRpQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "TEST_PATH = \"/kaggle/input/smadex-challenge-predict-the-revenue/test/test\"\n",
        "\n",
        "dd_test = dd.read_parquet(TEST_PATH)\n",
        "existing_big_cols_test = [c for c in ignore_big_cols if c in dd_test.columns]\n",
        "dd_test = dd_test.drop(columns=existing_big_cols_test)\n",
        "\n",
        "delayed_parts = dd_test.to_delayed()\n",
        "print(\"Número de chunks de test:\", len(delayed_parts))\n",
        "\n",
        "feature_cols = X_train_prep.columns.tolist()\n",
        "\n",
        "pred_dfs = []\n",
        "\n",
        "for i, d in enumerate(delayed_parts):\n",
        "    print(f\"Procesando chunk {i+1}/{len(delayed_parts)}...\")\n",
        "\n",
        "    part_df = d.compute()\n",
        "    part_df = reduce_memory(part_df)\n",
        "\n",
        "    row_ids = part_df[\"row_id\"].values\n",
        "    X_part = part_df[feature_cols].copy()   # ahora SÍ existen todas\n",
        "\n",
        "    X_part_prep = preprocess_new(X_part, num_cols, cat_cols, X_train_prep)\n",
        "\n",
        "    part_pred_log = model.predict(X_part_prep)\n",
        "    part_pred = np.expm1(part_pred_log)\n",
        "    part_pred = np.clip(part_pred, 0, None)\n",
        "\n",
        "    pred_dfs.append(pd.DataFrame({\n",
        "        \"row_id\": row_ids,\n",
        "        \"iap_revenue_d7\": part_pred\n",
        "    }))\n",
        "\n",
        "    del part_df, X_part, X_part_prep, row_ids, part_pred_log, part_pred\n",
        "    gc.collect()\n",
        "\n",
        "submission = pd.concat(pred_dfs, ignore_index=True)\n",
        "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
        "submission.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T14:38:17.340651Z",
          "iopub.execute_input": "2025-11-15T14:38:17.340976Z",
          "iopub.status.idle": "2025-11-15T14:59:01.835885Z",
          "shell.execute_reply.started": "2025-11-15T14:38:17.340952Z",
          "shell.execute_reply": "2025-11-15T14:59:01.835011Z"
        },
        "id": "jpX7nzdvGRpR",
        "outputId": "e9137be5-36e2-4e0f-da24-3bcc9499426a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Número de chunks de test: 96\nProcesando chunk 1/96...\nProcesando chunk 2/96...\nProcesando chunk 3/96...\nProcesando chunk 4/96...\nProcesando chunk 5/96...\nProcesando chunk 6/96...\nProcesando chunk 7/96...\nProcesando chunk 8/96...\nProcesando chunk 9/96...\nProcesando chunk 10/96...\nProcesando chunk 11/96...\nProcesando chunk 12/96...\nProcesando chunk 13/96...\nProcesando chunk 14/96...\nProcesando chunk 15/96...\nProcesando chunk 16/96...\nProcesando chunk 17/96...\nProcesando chunk 18/96...\nProcesando chunk 19/96...\nProcesando chunk 20/96...\nProcesando chunk 21/96...\nProcesando chunk 22/96...\nProcesando chunk 23/96...\nProcesando chunk 24/96...\nProcesando chunk 25/96...\nProcesando chunk 26/96...\nProcesando chunk 27/96...\nProcesando chunk 28/96...\nProcesando chunk 29/96...\nProcesando chunk 30/96...\nProcesando chunk 31/96...\nProcesando chunk 32/96...\nProcesando chunk 33/96...\nProcesando chunk 34/96...\nProcesando chunk 35/96...\nProcesando chunk 36/96...\nProcesando chunk 37/96...\nProcesando chunk 38/96...\nProcesando chunk 39/96...\nProcesando chunk 40/96...\nProcesando chunk 41/96...\nProcesando chunk 42/96...\nProcesando chunk 43/96...\nProcesando chunk 44/96...\nProcesando chunk 45/96...\nProcesando chunk 46/96...\nProcesando chunk 47/96...\nProcesando chunk 48/96...\nProcesando chunk 49/96...\nProcesando chunk 50/96...\nProcesando chunk 51/96...\nProcesando chunk 52/96...\nProcesando chunk 53/96...\nProcesando chunk 54/96...\nProcesando chunk 55/96...\nProcesando chunk 56/96...\nProcesando chunk 57/96...\nProcesando chunk 58/96...\nProcesando chunk 59/96...\nProcesando chunk 60/96...\nProcesando chunk 61/96...\nProcesando chunk 62/96...\nProcesando chunk 63/96...\nProcesando chunk 64/96...\nProcesando chunk 65/96...\nProcesando chunk 66/96...\nProcesando chunk 67/96...\nProcesando chunk 68/96...\nProcesando chunk 69/96...\nProcesando chunk 70/96...\nProcesando chunk 71/96...\nProcesando chunk 72/96...\nProcesando chunk 73/96...\nProcesando chunk 74/96...\nProcesando chunk 75/96...\nProcesando chunk 76/96...\nProcesando chunk 77/96...\nProcesando chunk 78/96...\nProcesando chunk 79/96...\nProcesando chunk 80/96...\nProcesando chunk 81/96...\nProcesando chunk 82/96...\nProcesando chunk 83/96...\nProcesando chunk 84/96...\nProcesando chunk 85/96...\nProcesando chunk 86/96...\nProcesando chunk 87/96...\nProcesando chunk 88/96...\nProcesando chunk 89/96...\nProcesando chunk 90/96...\nProcesando chunk 91/96...\nProcesando chunk 92/96...\nProcesando chunk 93/96...\nProcesando chunk 94/96...\nProcesando chunk 95/96...\nProcesando chunk 96/96...\n",
          "output_type": "stream"
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                 row_id  iap_revenue_d7\n0  e2f514a9-d922-4a17-bf94-f228bf4cd82f        0.000000\n1  4bfc70d3-d619-410a-9683-4cd759f30f32        0.059406\n2  ad433b66-b41e-4157-a6fd-24cd30701f6a        0.000000\n3  5ed964d6-ddce-42e8-9fad-276eb7f64c2f        0.007885\n4  81b73a45-c395-4d08-a4a3-513873440db3        0.000545",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>iap_revenue_d7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e2f514a9-d922-4a17-bf94-f228bf4cd82f</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4bfc70d3-d619-410a-9683-4cd759f30f32</td>\n      <td>0.059406</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ad433b66-b41e-4157-a6fd-24cd30701f6a</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5ed964d6-ddce-42e8-9fad-276eb7f64c2f</td>\n      <td>0.007885</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>81b73a45-c395-4d08-a4a3-513873440db3</td>\n      <td>0.000545</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission.head())\n",
        "print(submission.shape)\n",
        "\n",
        "print(submission.isna().sum())          # no debería haber NaNs\n",
        "print((submission['iap_revenue_d7'] < 0).sum())  # debería ser 0"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T15:06:21.686553Z",
          "iopub.execute_input": "2025-11-15T15:06:21.687176Z",
          "iopub.status.idle": "2025-11-15T15:06:22.739133Z",
          "shell.execute_reply.started": "2025-11-15T15:06:21.687137Z",
          "shell.execute_reply": "2025-11-15T15:06:22.73815Z"
        },
        "id": "nOarGav4GRpS",
        "outputId": "445a2165-941a-4ea3-c9ce-1bbbfec43137"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                                 row_id  iap_revenue_d7\n0  e2f514a9-d922-4a17-bf94-f228bf4cd82f        0.000000\n1  4bfc70d3-d619-410a-9683-4cd759f30f32        0.059406\n2  ad433b66-b41e-4157-a6fd-24cd30701f6a        0.000000\n3  5ed964d6-ddce-42e8-9fad-276eb7f64c2f        0.007885\n4  81b73a45-c395-4d08-a4a3-513873440db3        0.000545\n(13188409, 2)\nrow_id            0\niap_revenue_d7    0\ndtype: int64\n0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}