{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: single\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "MODEL_VERSION = \"single\"  # Options: \"single\" or \"two_step\"\n",
    "\n",
    "TRAIN_PATH = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "TEST_PATH = '/home/stargix/Desktop/hackathons/datathon/test/test'\n",
    "TARGET_COL = \"iap_revenue_d7\"\n",
    "TRAIN_SAMPLE_FRAC = 0.05  # Adjust for more/less data\n",
    "\n",
    "print(f\"Selected model: {MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc324f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7cb9fb2f8a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837c360",
   "metadata": {},
   "source": [
    "## Load Data (with date filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106182a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 21 out of 144 train files\n",
      "Loading train data...\n"
     ]
    }
   ],
   "source": [
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Obtener lista de archivos parquet\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# Usar un porcentaje de archivos\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# Cargar TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Sample y compute\n",
    "train_df = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=42).compute()\n",
    "print(f\"Train loaded: {train_df.shape}\")\n",
    "\n",
    "# Cargar VALID\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=42).compute()\n",
    "print(f\"Valid loaded: {valid_df.shape}\")\n",
    "\n",
    "# Limpiar memoria\n",
    "del dd_train, dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989707d0",
   "metadata": {},
   "source": [
    "## Prepare Features (grok.ipynb approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacde64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir labels a excluir\n",
    "labels_to_exclude = ['buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28', 'buy_d7', 'buy_d14', 'buy_d28',\n",
    "                     'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28', 'registration',\n",
    "                     'retention_d1_to_d7', 'retention_d3_to_d7', 'retention_d7_to_d14',\n",
    "                     'retention_d1', 'retention_d3', 'retention_d7']\n",
    "\n",
    "# Definir features\n",
    "features = [col for col in train_df.columns if col not in labels_to_exclude + ['row_id', 'datetime']]\n",
    "\n",
    "# Categorical features\n",
    "cat_features = ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory',\n",
    "                'advertiser_bottom_taxonomy_level', 'carrier', 'country', 'region',\n",
    "                'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'weekday']\n",
    "\n",
    "# Solo usar cat_features que existan\n",
    "cat_features_valid = [col for col in cat_features if col in features]\n",
    "\n",
    "print(f\"Total features: {len(features)}\")\n",
    "print(f\"Categorical features: {len(cat_features_valid)}\")\n",
    "\n",
    "# Extraer X, y\n",
    "X_train = train_df[features].copy()\n",
    "y_train = train_df[TARGET_COL].values\n",
    "\n",
    "X_val = valid_df[features].copy()\n",
    "y_val = valid_df[TARGET_COL].values\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Val samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas object a numÃ©ricas (excepto categorical features)\n",
    "numeric_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col not in cat_features_valid:\n",
    "        try:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Rellenar NaN generados por conversiÃ³n fallida\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "\n",
    "# Asegurar que cat_features estÃ©n como category\n",
    "for col in cat_features_valid:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "print(\"âœ“ Data types converted\")\n",
    "print(f\"Numeric features: {len([c for c in X_train.columns if c not in cat_features_valid])}\")\n",
    "print(f\"Categorical features: {len(cat_features_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df971621",
   "metadata": {},
   "source": [
    "## Train LightGBM Model (grok.ipynb approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d354f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target to log space\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "# Crear datasets LGBM\n",
    "train_ds = lgb.Dataset(X_train, label=y_train_log, categorical_feature=cat_features_valid)\n",
    "val_ds = lgb.Dataset(X_val, label=y_val_log, reference=train_ds)\n",
    "\n",
    "print(\"âœ“ LightGBM datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b81936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParÃ¡metros LGBM\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "print(\"Parameters configured:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo\n",
    "print(\"Training model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_ds,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_ds, val_ds],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746f078",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en validaciÃ³n\n",
    "pred_log = model.predict(X_val)\n",
    "pred_val = np.expm1(pred_log).clip(0, None)\n",
    "msle = mean_squared_log_error(y_val, pred_val)\n",
    "\n",
    "msle_baseline = mean_squared_log_error(y_val, np.zeros_like(y_val))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSLE: {msle:.6f}\")\n",
    "print(f\"Baseline (all zeros): {msle_baseline:.6f}\")\n",
    "print(f\"Improvement: {((msle_baseline - msle) / msle_baseline * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nPrediction stats:\")\n",
    "print(f\"  Mean: {pred_val.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(pred_val):.4f}\")\n",
    "print(f\"  Max: {pred_val.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(pred_val > 0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1027c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "lgb.plot_importance(model, max_num_features=20, figsize=(10, 8))\n",
    "print(\"\\nTop 20 most important features shown above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50923b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 21 out of 144 train files\n",
      "Loading train data...\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "\n",
      "Loading validation data...\n",
      "\n",
      "Loading validation data...\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "\n",
      "âœ“ Data loaded successfully\n",
      "\n",
      "âœ“ Data loaded successfully\n",
      "Total memory: ~0.44 GB\n",
      "Total memory: ~0.44 GB\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Obtener lista de archivos parquet\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# ðŸ”¥ REDUCIR MÃS: Solo 5-10% de archivos\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))  # CambiÃ© a 5%\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# ðŸ”¥ DROPEAR COLUMNAS ANTES DE COMPUTE\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"row_id\", \"datetime\"]\n",
    "\n",
    "# Cargar TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Dropear columnas pesadas ANTES de compute\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# ðŸ”¥ SAMPLE EN DASK (no en pandas)\n",
    "train_sample = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=42).compute()\n",
    "train_sample = reduce_memory(train_sample)\n",
    "\n",
    "print(f\"Train loaded: {train_sample.shape}, Memory: {train_sample.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Limpiar memoria\n",
    "del dd_train\n",
    "gc.collect()\n",
    "\n",
    "# Cargar VALID (despuÃ©s de liberar train)\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,  # Mismos archivos\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_valid.columns]\n",
    "dd_valid = dd_valid.drop(columns=existing_cols)\n",
    "\n",
    "# ðŸ”¥ SAMPLE MENOS EN VALID (solo necesitas evaluar, no entrenar)\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=42).compute()\n",
    "valid_df = reduce_memory(valid_df)\n",
    "\n",
    "print(f\"Valid loaded: {valid_df.shape}, Memory: {valid_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully\")\n",
    "print(f\"Total memory: ~{(train_sample.memory_usage(deep=True).sum() + valid_df.memory_usage(deep=True).sum()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ace372",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Processing 96 test chunks...\n",
      "  Chunk 10/96...\n",
      "  Chunk 10/96...\n",
      "  Chunk 20/96...\n",
      "  Chunk 20/96...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     part_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(part_pred, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m MODEL_VERSION \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo_step\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     buyer_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_part_prep\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     47\u001b[0m     revenue_pred_log \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_part_prep)\n\u001b[1;32m     48\u001b[0m     revenue_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpm1(revenue_pred_log)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/sklearn.py:1627\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_proba\u001b[39m(\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1617\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1625\u001b[0m ):\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1627\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[1;32m   1638\u001b[0m         _log_warning(\n\u001b[1;32m   1639\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot compute class probabilities or labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning raw scores instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1642\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/sklearn.py:1144\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m predict_params \u001b[38;5;241m=\u001b[39m _choose_param_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m, predict_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m   1142\u001b[0m predict_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_n_jobs(predict_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Booster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredict_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:4767\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4765\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4766\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:1204\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(\n\u001b[1;32m   1198\u001b[0m         csc\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1199\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[1;32m   1200\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m   1201\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[1;32m   1202\u001b[0m     )\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1204\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[1;32m   1211\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_pyarrow_table(\n\u001b[1;32m   1212\u001b[0m         table\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1213\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[1;32m   1214\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m   1215\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[1;32m   1216\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:1361\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:1308\u001b[0m, in \u001b[0;36m_InnerPredictor.__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1306\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1307\u001b[0m _safe_call(\n\u001b[0;32m-> 1308\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m )\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Generating test predictions...\")\n",
    "\n",
    "dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow')\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "\n",
    "print(f\"Processing {len(delayed_parts)} test chunks...\")\n",
    "\n",
    "pred_dfs = []\n",
    "\n",
    "for i, d in enumerate(delayed_parts):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Chunk {i+1}/{len(delayed_parts)}...\")\n",
    "    \n",
    "    part_df = d.compute()\n",
    "    row_ids = part_df[\"row_id\"].values\n",
    "    \n",
    "    # Obtener solo las features que tenemos\n",
    "    X_part = part_df[[c for c in features if c in part_df.columns]].copy()\n",
    "    \n",
    "    # AÃ±adir columnas faltantes con valores por defecto\n",
    "    for col in features:\n",
    "        if col not in X_part.columns:\n",
    "            X_part[col] = 0\n",
    "    \n",
    "    # Reordenar columnas\n",
    "    X_part = X_part[features]\n",
    "    \n",
    "    # Preprocessing similar al train\n",
    "    numeric_cols = X_part.select_dtypes(include=['object']).columns\n",
    "    for col in numeric_cols:\n",
    "        if col not in cat_features_valid:\n",
    "            try:\n",
    "                X_part[col] = pd.to_numeric(X_part[col], errors='coerce')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    X_part = X_part.fillna(0)\n",
    "    \n",
    "    for col in cat_features_valid:\n",
    "        if col in X_part.columns:\n",
    "            X_part[col] = X_part[col].astype('category')\n",
    "    \n",
    "    # Predecir\n",
    "    part_pred_log = model.predict(X_part)\n",
    "    part_pred = np.expm1(part_pred_log).clip(0, None)\n",
    "    \n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"iap_revenue_d7\": part_pred\n",
    "    }))\n",
    "    \n",
    "    del part_df, X_part, row_ids, part_pred\n",
    "    gc.collect()\n",
    "\n",
    "# Combine and save\n",
    "submission = pd.concat(pred_dfs, ignore_index=True)\n",
    "output_file = \"submission_grok.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Submission saved: {output_file}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Sample:\\n{submission.head()}\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"\\nValidation checks:\")\n",
    "print(f\"  NaN values: {submission.isna().sum().sum()}\")\n",
    "print(f\"  Negative values: {(submission['iap_revenue_d7'] < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8486e1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook uses the **grok.ipynb approach**:\n",
    "- LightGBM native API (`lgb.train`) instead of sklearn wrapper\n",
    "- Log-transform target (`np.log1p`)\n",
    "- Explicit categorical feature handling\n",
    "- Early stopping with 100 rounds\n",
    "- Date-based train/validation split (Oct 1-5 for train, Oct 6 for validation)\n",
    "\n",
    "**To adjust data size**: Change `TRAIN_SAMPLE_FRAC` in the first cell (default 0.10 = 10%)\n",
    "\n",
    "Then re-run all cells!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
