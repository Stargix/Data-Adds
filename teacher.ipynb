{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120867,"databundleVersionId":14453560,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport dask.dataframe as dd\nfrom pandas.api.types import is_numeric_dtype\n\n# ===========================\n# Configuración general\n# ===========================\nRANDOM_STATE = 42\nTARGET_COL   = \"iap_revenue_d7\"\n\nTRAIN_PATH = \"/kaggle/input/smadex-challenge-predict-the-revenue/train/train\"\nTEST_PATH  = \"/kaggle/input/smadex-challenge-predict-the-revenue/test/test\"\n\npd.set_option(\"display.max_columns\", 200)\n\n# ===========================\n# Funciones auxiliares\n# ===========================\ndef reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n    df = df.copy()\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type == \"float64\":\n            df[col] = df[col].astype(\"float32\")\n        elif col_type == \"int64\":\n            df[col] = df[col].astype(\"int32\")\n    return df\n\ndef preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n    \"\"\"Preprocesado para train/valid.\"\"\"\n    X_train = X_train.copy()\n    X_valid = X_valid.copy()\n    \n    # Numéricas: NaN -> 0\n    for c in num_cols:\n        X_train[c] = X_train[c].fillna(0)\n        X_valid[c] = X_valid[c].fillna(0)\n    \n    # Categóricas: strings + categorías fijas basadas en TRAIN\n    for c in cat_cols:\n        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n        X_train[c] = X_train[c].astype(\"category\")\n        \n        cats = X_train[c].cat.categories\n        \n        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n        X_valid[c] = X_valid[c].astype(\n            pd.api.types.CategoricalDtype(categories=cats)\n        )\n    \n    return X_train, X_valid\n\ndef preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n    X_new = X_new.copy()\n    \n    for c in num_cols:\n        if c in X_new.columns:\n            X_new[c] = X_new[c].fillna(0)\n    \n    for c in cat_cols:\n        if c in X_new.columns:\n            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n            cats = cat_ref_df[c].cat.categories\n            X_new[c] = X_new[c].astype(\n                pd.api.types.CategoricalDtype(categories=cats)\n            )\n    \n    return X_new\n\n# ===========================\n# 0) Definir tus features\n# ===========================\nBASE_FEATURE_COLS = [\n    \"advertiser_bundle\",\n    \"advertiser_subcategory\",\n    \"country\",\n    \"release_msrp\",\n    \"dev_model\",\n    \"advertiser_category\",\n    \"dev_osv\",\n    \"release_date\",\n    \"hour\",\n    \"dev_make\",\n    \"dev_os\",\n    \"weekday\",\n]\n\n# Columnas que REALMENTE vamos a leer del parquet\n# (features + target + opcionales row_id/datetime)\nNEEDED_COLS = sorted(set(\n    BASE_FEATURE_COLS\n    + [TARGET_COL, \"row_id\", \"datetime\"]\n))\n\nprint(\"Columnas que se van a leer del parquet:\", NEEDED_COLS)\nprint(\"Total columnas leídas:\", len(NEEDED_COLS))\n\n# ===========================\n# 1) Filtros por fecha (como antes)\n# ===========================\nfilters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n\nfilters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n\n# ===========================\n# 2) Leer train/valid SOLO con esas columnas\n# ===========================\ndd_train = dd.read_parquet(TRAIN_PATH, filters=filters_train, columns=NEEDED_COLS)\ndd_valid = dd.read_parquet(TRAIN_PATH, filters=filters_valid, columns=NEEDED_COLS)\n\nprint(\"dd_train columns:\", list(dd_train.columns))\n\n# ===========================\n# 3) Muestreo + paso a pandas\n# ===========================\nfrac_train = 1\n\ntrain_df = dd_train.sample(frac=frac_train, random_state=RANDOM_STATE).compute()\nvalid_df = dd_valid.compute()\n\ntrain_df = reduce_memory(train_df)\nvalid_df = reduce_memory(valid_df)\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Valid shape:\", valid_df.shape)\nprint(\"Train memory (GB):\", train_df.memory_usage(deep=True).sum() / (1024**3))\nprint(\"Valid memory (GB):\", valid_df.memory_usage(deep=True).sum() / (1024**3))\n\n# ===========================\n# 4) y_train / y_valid (escala ORIGINAL)\n# ===========================\nassert TARGET_COL in train_df.columns, \"Falta iap_revenue_d7 en train_df\"\nassert TARGET_COL in valid_df.columns, \"Falta iap_revenue_d7 en valid_df\"\n\ny_train = train_df[TARGET_COL].astype(\"float32\").values\ny_valid = valid_df[TARGET_COL].astype(\"float32\").values\n\n# ===========================\n# 5) Features (solo las que tú quieres y existen)\n# ===========================\nfeature_cols = [c for c in BASE_FEATURE_COLS if c in train_df.columns]\n\nprint(\"Features usadas realmente:\", feature_cols)\n\nX_train = train_df[feature_cols].copy()\nX_valid = valid_df[feature_cols].copy()\n\nprint(\"X_train shape (antes de prep):\", X_train.shape)\nprint(\"X_valid shape (antes de prep):\", X_valid.shape)\n\n# ===========================\n# 6) Numéricas / categóricas + preprocesado\n# ===========================\nnum_cols = [c for c in X_train.columns if is_numeric_dtype(X_train[c])]\ncat_cols = [c for c in X_train.columns if not is_numeric_dtype(X_train[c])]\n\nprint(\"Numéricas:\", len(num_cols), \"->\", num_cols)\nprint(\"Categóricas:\", len(cat_cols), \"->\", cat_cols)\n\nX_train_prep, X_valid_prep = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n\nprint(\"X_train_prep shape:\", X_train_prep.shape)\nprint(\"X_valid_prep shape:\", X_valid_prep.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:04:24.019482Z","iopub.execute_input":"2025-11-15T19:04:24.020349Z","iopub.status.idle":"2025-11-15T19:05:51.008364Z","shell.execute_reply.started":"2025-11-15T19:04:24.020318Z","shell.execute_reply":"2025-11-15T19:05:51.007400Z"}},"outputs":[{"name":"stdout","text":"Columnas que se van a leer del parquet: ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'country', 'datetime', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'hour', 'iap_revenue_d7', 'release_date', 'release_msrp', 'row_id', 'weekday']\nTotal columnas leídas: 15\ndd_train columns: ['advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', 'country', 'datetime', 'dev_make', 'dev_model', 'dev_os', 'dev_osv', 'hour', 'iap_revenue_d7', 'release_date', 'release_msrp', 'row_id', 'weekday']\nTrain shape: (17294102, 15)\nValid shape: (3306478, 15)\nTrain memory (GB): 3.9395919451490045\nValid memory (GB): 0.7526155570521951\nFeatures usadas realmente: ['advertiser_bundle', 'advertiser_subcategory', 'country', 'release_msrp', 'dev_model', 'advertiser_category', 'dev_osv', 'release_date', 'hour', 'dev_make', 'dev_os', 'weekday']\nX_train shape (antes de prep): (17294102, 12)\nX_valid shape (antes de prep): (3306478, 12)\nNuméricas: 2 -> ['release_msrp', 'weekday']\nCategóricas: 10 -> ['advertiser_bundle', 'advertiser_subcategory', 'country', 'dev_model', 'advertiser_category', 'dev_osv', 'release_date', 'hour', 'dev_make', 'dev_os']\nX_train_prep shape: (17294102, 12)\nX_valid_prep shape: (3306478, 12)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pandas.api.types import CategoricalDtype\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device, \"| Num GPUs:\", torch.cuda.device_count())\n\n# ===========================\n# 1) Preparar X_train_nn / X_valid_nn con códigos categóricos\n# ===========================\nX_train_nn = X_train_prep.copy()\nX_valid_nn = X_valid_prep.copy()\n\n# 1.1 Asegurar dtype 'category' y categorías consistentes\nfor c in cat_cols:\n    if c not in X_train_nn.columns:\n        print(f\"[AVISO] '{c}' no está en X_train_nn, la salto.\")\n        continue\n\n    # TRAIN\n    X_train_nn[c] = (\n        X_train_nn[c]\n        .astype(\"object\")\n        .fillna(\"unknown\")\n        .astype(str)\n        .astype(\"category\")\n    )\n    cats = X_train_nn[c].cat.categories\n\n    # VALID con mismas categorías\n    if c in X_valid_nn.columns:\n        X_valid_nn[c] = (\n            X_valid_nn[c]\n            .astype(\"object\")\n            .fillna(\"unknown\")\n            .astype(str)\n        )\n        X_valid_nn[c] = X_valid_nn[c].astype(\n            CategoricalDtype(categories=cats)\n        )\n    else:\n        print(f\"[AVISO] '{c}' no está en X_valid_nn, la salto en valid.\")\n\n# 1.2 Pasar categóricas a códigos enteros y clamped (sin -1)\nfor c in cat_cols:\n    if c not in X_train_nn.columns:\n        continue\n\n    train_codes = X_train_nn[c].cat.codes.astype(\"int64\")\n    valid_codes = X_valid_nn[c].cat.codes.astype(\"int64\") if c in X_valid_nn.columns else None\n\n    # -1 -> 0 (categoría \"desconocida\")\n    train_codes = train_codes.where(train_codes >= 0, 0)\n    if valid_codes is not None:\n        valid_codes = valid_codes.where(valid_codes >= 0, 0)\n\n    X_train_nn[c] = train_codes\n    if valid_codes is not None:\n        X_valid_nn[c] = valid_codes\n\n# 1.3 Cardinalidades de cada columna categórica\ncat_cardinalities = []\nfor c in cat_cols:\n    if c not in X_train_nn.columns:\n        continue\n\n    max_train = int(X_train_nn[c].max())\n    if c in X_valid_nn.columns:\n        max_valid = int(X_valid_nn[c].max())\n    else:\n        max_valid = max_train\n\n    max_code = max(max_train, max_valid)\n    card = max_code + 1\n    cat_cardinalities.append(card)\n\n    print(f\"[CARD] {c}: max_train={max_train}, max_valid={max_valid}, card={card}\")\n\nprint(\"Num cols numéricas:\", len(num_cols))\nprint(\"Num cols categóricas:\", len(cat_cardinalities))\n\nassert len(cat_cardinalities) == len(cat_cols), \\\n    f\"Descuadre: len(cat_cardinalities)={len(cat_cardinalities)}, len(cat_cols)={len(cat_cols)}\"\n\nfor i, card in enumerate(cat_cardinalities):\n    assert isinstance(card, int) and card > 0, f\"Cardinalidad inválida en índice {i}: {card}\"\n\n# ===========================\n# 2) Numpy arrays para la red\n# ===========================\n# Numéricas\nX_train_num = X_train_nn[num_cols].to_numpy(dtype=\"float32\")\nX_valid_num = X_valid_nn[num_cols].to_numpy(dtype=\"float32\")\n\n# Categóricas (si no hay, dejamos matrices vacías)\nif len(cat_cols) > 0:\n    X_train_cat = X_train_nn[cat_cols].to_numpy(dtype=\"int64\")\n    X_valid_cat = X_valid_nn[cat_cols].to_numpy(dtype=\"int64\")\nelse:\n    X_train_cat = np.zeros((X_train_num.shape[0], 0), dtype=\"int64\")\n    X_valid_cat = np.zeros((X_valid_num.shape[0], 0), dtype=\"int64\")\n\n# Targets en LOG1P (como antes)\ny_train_log = np.log1p(y_train.astype(\"float32\"))\ny_valid_log = np.log1p(y_valid.astype(\"float32\"))\n\nprint(\"Shapes -> X_num:\", X_train_num.shape, \"X_cat:\", X_train_cat.shape)\nprint(\"y_train_log shape:\", y_train_log.shape)\n\n# ===========================\n# 3) Dataset y DataLoader\n# ===========================\nclass TabularDataset(Dataset):\n    def __init__(self, X_num, X_cat, y_log):\n        self.X_num = torch.from_numpy(X_num.astype(\"float32\"))\n        self.X_cat = torch.from_numpy(X_cat.astype(\"int64\"))\n        # y en log, guardado como (N,1) para evitar broadcasting raro\n        self.y     = torch.from_numpy(y_log.astype(\"float32\")).view(-1, 1)\n    \n    def __len__(self):\n        return self.X_num.shape[0]\n    \n    def __getitem__(self, idx):\n        return (\n            self.X_num[idx],\n            self.X_cat[idx],\n            self.y[idx],\n        )\n\ntrain_dataset = TabularDataset(X_train_num, X_train_cat, y_train_log)\nvalid_dataset = TabularDataset(X_valid_num, X_valid_cat, y_valid_log)\n\nBATCH_SIZE = 4096  # ajustable\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=False,\n    num_workers=2\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    drop_last=False,\n    num_workers=2\n)\n\nprint(\"Tamaño train_dataset:\", len(train_dataset))\nprint(\"Tamaño valid_dataset:\", len(valid_dataset))\n\n# ===========================\n# 4) Definición del Teacher\n# ===========================\nclass TeacherNet(nn.Module):\n    def __init__(self, num_numeric, cat_cardinalities, emb_max_dim=64):\n        super().__init__()\n        \n        self.num_numeric = num_numeric\n        self.n_cat = len(cat_cardinalities)\n        \n        # Embeddings para cada columna categórica\n        emb_layers = []\n        emb_dims = []\n        for card in cat_cardinalities:\n            emb_dim = min(emb_max_dim, max(4, card // 2))  # regla simple\n            emb_layers.append(nn.Embedding(card, emb_dim))\n            emb_dims.append(emb_dim)\n        \n        self.emb_layers = nn.ModuleList(emb_layers)\n        self.emb_dims = emb_dims\n        \n        input_dim = num_numeric + sum(emb_dims)\n        \n        hidden_sizes = [512, 512, 256, 128]  # Teacher \"grande\"\n        layers = []\n        in_dim = input_dim\n        \n        for h in hidden_sizes:\n            layers.append(nn.Linear(in_dim, h))\n            layers.append(nn.ReLU())\n            layers.append(nn.BatchNorm1d(h))\n            layers.append(nn.Dropout(0.2))\n            in_dim = h\n        \n        layers.append(nn.Linear(in_dim, 1))  # salida escalar (log revenue)\n        \n        self.mlp = nn.Sequential(*layers)\n    \n    def forward(self, x_num, x_cat):\n        # x_num: (B, num_numeric)\n        # x_cat: (B, n_cat)\n        if self.n_cat > 0:\n            emb_list = []\n            for i, emb in enumerate(self.emb_layers):\n                emb_i = emb(x_cat[:, i])\n                emb_list.append(emb_i)\n            x_emb = torch.cat(emb_list, dim=1)\n            x = torch.cat([x_num, x_emb], dim=1)\n        else:\n            x = x_num\n        out = self.mlp(x)  # (B, 1) en log-space\n        return out\n\nnum_numeric_features = len(num_cols)\nmodel_teacher = TeacherNet(num_numeric_features, cat_cardinalities).to(device)\nprint(model_teacher)\n\n# Test rápido de forward en GPU\nx_num_dbg, x_cat_dbg, y_dbg = next(iter(train_loader))\nx_num_dbg = x_num_dbg.to(device)\nx_cat_dbg = x_cat_dbg.to(device)\n\nwith torch.no_grad():\n    out_dbg = model_teacher(x_num_dbg, x_cat_dbg)\n    print(\"GPU forward OK. out shape:\", out_dbg.shape)\n\n# ===========================\n# 5) Entrenamiento en log-space (MSE pequeño como antes)\n# ===========================\ncriterion = nn.MSELoss()  # MSE sobre log1p(revenue)\noptimizer = torch.optim.AdamW(model_teacher.parameters(), lr=1e-3, weight_decay=1e-4)\n\nEPOCHS = 20\nbest_val_loss = np.inf\npatience = 5\npatience_counter = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    # --------- TRAIN ----------\n    model_teacher.train()\n    train_loss_sum = 0.0\n    n_train = 0\n    \n    for x_num, x_cat, y_log in train_loader:\n        x_num = x_num.to(device)\n        x_cat = x_cat.to(device)\n        y_log = y_log.to(device)\n        \n        optimizer.zero_grad()\n        y_pred_log = model_teacher(x_num, x_cat)\n        \n        loss = criterion(y_pred_log, y_log)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss_sum += loss.item() * y_log.size(0)\n        n_train += y_log.size(0)\n    \n    train_loss = train_loss_sum / max(n_train, 1)\n    \n    # --------- VALID ----------\n    model_teacher.eval()\n    val_loss_sum = 0.0\n    n_val = 0\n    \n    with torch.no_grad():\n        for x_num, x_cat, y_log in valid_loader:\n            x_num = x_num.to(device)\n            x_cat = x_cat.to(device)\n            y_log = y_log.to(device)\n            \n            y_pred_log = model_teacher(x_num, x_cat)\n            loss = criterion(y_pred_log, y_log)\n            \n            val_loss_sum += loss.item() * y_log.size(0)\n            n_val += y_log.size(0)\n    \n    val_loss = val_loss_sum / max(n_val, 1)\n    \n    print(f\"Epoch {epoch:02d} | train MSE_log: {train_loss:.6f} | valid MSE_log: {val_loss:.6f}\")\n    \n    # Early stopping\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model_teacher.state_dict(), \"teacher_best.pth\")\n        print(\"  -> New best model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"  -> No improvement. Patience: {patience_counter}/{patience}\")\n        if patience_counter >= patience:\n            print(\"Early stopping activado.\")\n            break\n\n# Cargar el mejor modelo al final\nmodel_teacher.load_state_dict(torch.load(\"teacher_best.pth\", map_location=device))\nmodel_teacher.eval()\n\n# ===========================\n# 6) Métricas: log-space y escala original\n# ===========================\ndef eval_metrics(model, loader):\n    model.eval()\n    preds_log_list = []\n    targets_log_list = []\n    \n    with torch.no_grad():\n        for x_num, x_cat, y_log in loader:\n            x_num = x_num.to(device)\n            x_cat = x_cat.to(device)\n            y_log = y_log.to(device)\n            \n            y_pred_log = model(x_num, x_cat)\n            preds_log_list.append(y_pred_log.cpu().numpy())\n            targets_log_list.append(y_log.cpu().numpy())\n    \n    preds_log = np.concatenate(preds_log_list, axis=0).ravel()\n    targets_log = np.concatenate(targets_log_list, axis=0).ravel()\n    \n    mse_log = np.mean((preds_log - targets_log) ** 2)\n    \n    preds = np.expm1(preds_log)\n    targets = np.expm1(targets_log)\n    \n    mse_orig = np.mean((preds - targets) ** 2)\n    rmse_orig = np.sqrt(mse_orig)\n    return mse_log, mse_orig, rmse_orig\n\nmse_log_valid, mse_valid_orig, rmse_valid_orig = eval_metrics(model_teacher, valid_loader)\nprint(\"Valid MSE_log         :\", float(mse_log_valid))\nprint(\"Valid MSE  (original) :\", float(mse_valid_orig))\nprint(\"Valid RMSE (original) :\", float(rmse_valid_orig))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T19:05:51.009887Z","iopub.execute_input":"2025-11-15T19:05:51.010522Z","iopub.status.idle":"2025-11-15T19:50:21.847516Z","shell.execute_reply.started":"2025-11-15T19:05:51.010498Z","shell.execute_reply":"2025-11-15T19:50:21.846465Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | Num GPUs: 2\n[CARD] advertiser_bundle: max_train=513, max_valid=513, card=514\n[CARD] advertiser_subcategory: max_train=55, max_valid=55, card=56\n[CARD] country: max_train=238, max_valid=238, card=239\n[CARD] dev_model: max_train=14669, max_valid=14662, card=14670\n[CARD] advertiser_category: max_train=21, max_valid=21, card=22\n[CARD] dev_osv: max_train=238, max_valid=238, card=239\n[CARD] release_date: max_train=196, max_valid=196, card=197\n[CARD] hour: max_train=23, max_valid=23, card=24\n[CARD] dev_make: max_train=957, max_valid=957, card=958\n[CARD] dev_os: max_train=2, max_valid=2, card=3\nNum cols numéricas: 2\nNum cols categóricas: 10\nShapes -> X_num: (17294102, 2) X_cat: (17294102, 10)\ny_train_log shape: (17294102,)\nTamaño train_dataset: 17294102\nTamaño valid_dataset: 3306478\nTeacherNet(\n  (emb_layers): ModuleList(\n    (0): Embedding(514, 64)\n    (1): Embedding(56, 28)\n    (2): Embedding(239, 64)\n    (3): Embedding(14670, 64)\n    (4): Embedding(22, 11)\n    (5): Embedding(239, 64)\n    (6): Embedding(197, 64)\n    (7): Embedding(24, 12)\n    (8): Embedding(958, 64)\n    (9): Embedding(3, 4)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=441, out_features=512, bias=True)\n    (1): ReLU()\n    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=512, out_features=512, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.2, inplace=False)\n    (8): Linear(in_features=512, out_features=256, bias=True)\n    (9): ReLU()\n    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): Dropout(p=0.2, inplace=False)\n    (12): Linear(in_features=256, out_features=128, bias=True)\n    (13): ReLU()\n    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): Dropout(p=0.2, inplace=False)\n    (16): Linear(in_features=128, out_features=1, bias=True)\n  )\n)\nGPU forward OK. out shape: torch.Size([4096, 1])\nEpoch 01 | train MSE_log: 0.179447 | valid MSE_log: 0.179597\n  -> New best model saved.\nEpoch 02 | train MSE_log: 0.172814 | valid MSE_log: 0.177997\n  -> New best model saved.\nEpoch 03 | train MSE_log: 0.171601 | valid MSE_log: 0.177250\n  -> New best model saved.\nEpoch 04 | train MSE_log: 0.170737 | valid MSE_log: 0.176503\n  -> New best model saved.\nEpoch 05 | train MSE_log: 0.170152 | valid MSE_log: 0.176678\n  -> No improvement. Patience: 1/5\nEpoch 06 | train MSE_log: 0.169716 | valid MSE_log: 0.176104\n  -> New best model saved.\nEpoch 07 | train MSE_log: 0.169370 | valid MSE_log: 0.176255\n  -> No improvement. Patience: 1/5\nEpoch 08 | train MSE_log: 0.169020 | valid MSE_log: 0.175985\n  -> New best model saved.\nEpoch 09 | train MSE_log: 0.168694 | valid MSE_log: 0.175829\n  -> New best model saved.\nEpoch 10 | train MSE_log: 0.168471 | valid MSE_log: 0.175623\n  -> New best model saved.\nEpoch 11 | train MSE_log: 0.168230 | valid MSE_log: 0.176352\n  -> No improvement. Patience: 1/5\nEpoch 12 | train MSE_log: 0.167972 | valid MSE_log: 0.176068\n  -> No improvement. Patience: 2/5\nEpoch 13 | train MSE_log: 0.167745 | valid MSE_log: 0.175892\n  -> No improvement. Patience: 3/5\nEpoch 14 | train MSE_log: 0.167547 | valid MSE_log: 0.175873\n  -> No improvement. Patience: 4/5\nEpoch 15 | train MSE_log: 0.167337 | valid MSE_log: 0.176075\n  -> No improvement. Patience: 5/5\nEarly stopping activado.\nValid MSE_log         : 0.17562267184257507\nValid MSE  (original) : 31068.443359375\nValid RMSE (original) : 176.26242065429688\n","output_type":"stream"}],"execution_count":3}]}