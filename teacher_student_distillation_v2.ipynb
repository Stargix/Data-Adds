{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c4fd17",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "TRAIN_PATH = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "TARGET_COL = \"iap_revenue_d7\"\n",
    "TRAIN_SAMPLE_FRAC = 0.10  # Adjust for more/less data\n",
    "\n",
    "# PyTorch settings\n",
    "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 256\n",
    "TEACHER_EPOCHS = 5\n",
    "STUDENT_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "DISTILL_ALPHA = 0.6  # weight for hard loss\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Sample fraction: {TRAIN_SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1861a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9232f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from sklearn.metrics import mean_squared_log_error, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Reproducibility\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)\n",
    "torch.manual_seed(RSEED)\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89432efb",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas problemáticas (listas/dicts) que se ignoran\n",
    "IGNORE_BIG_COLS = [\n",
    "    \"bundles_ins\", \"user_bundles\", \"user_bundles_l28d\",\n",
    "    \"city_hist\", \"country_hist\", \"region_hist\",\n",
    "    \"dev_language_hist\", \"dev_osv_hist\",\n",
    "    \"bcat\", \"bcat_bottom_taxonomy\",\n",
    "    \"bundles_cat\", \"bundles_cat_bottom_taxonomy\",\n",
    "    \"first_request_ts_bundle\", \"first_request_ts_category_bottom_taxonomy\",\n",
    "    \"last_buy_ts_bundle\", \"last_buy_ts_category\",\n",
    "    \"last_install_ts_bundle\", \"last_install_ts_category\",\n",
    "    \"advertiser_actions_action_count\", \"advertiser_actions_action_last_timestamp\",\n",
    "    \"user_actions_bundles_action_count\", \"user_actions_bundles_action_last_timestamp\",\n",
    "    \"new_bundles\",\n",
    "    \"whale_users_bundle_num_buys_prank\", \"whale_users_bundle_revenue_prank\",\n",
    "    \"whale_users_bundle_total_num_buys\", \"whale_users_bundle_total_revenue\",\n",
    "]\n",
    "\n",
    "LABEL_COLS = [\n",
    "    \"buyer_d1\", \"buyer_d7\", \"buyer_d14\", \"buyer_d28\",\n",
    "    \"buy_d7\", \"buy_d14\", \"buy_d28\",\n",
    "    \"iap_revenue_d7\", \"iap_revenue_d14\", \"iap_revenue_d28\",\n",
    "    \"registration\",\n",
    "    \"retention_d1_to_d7\", \"retention_d3_to_d7\", \"retention_d7_to_d14\",\n",
    "    \"retention_d1\", \"retention_d3\", \"retention_d7\",\n",
    "]\n",
    "\n",
    "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Downcast numeric columns to save memory.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        elif col_type == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
    "    \"\"\"Detect columns containing lists or dicts.\"\"\"\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    listlike = []\n",
    "    for c in cols:\n",
    "        sample_vals = df[c].head(100)\n",
    "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "            listlike.append(c)\n",
    "    return listlike\n",
    "\n",
    "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
    "    \"\"\"Preprocess train and validation sets.\"\"\"\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    \n",
    "    # Numeric: fill NaN with 0\n",
    "    for c in num_cols:\n",
    "        X_train[c] = X_train[c].fillna(0)\n",
    "        X_valid[c] = X_valid[c].fillna(0)\n",
    "    \n",
    "    # Categorical: convert to strings and encode as integers\n",
    "    cat_mappings = {}\n",
    "    for c in cat_cols:\n",
    "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "        \n",
    "        # Create mapping\n",
    "        cats = X_train[c].cat.categories\n",
    "        cat_mappings[c] = {cat: i for i, cat in enumerate(cats)}\n",
    "        \n",
    "        # Encode train\n",
    "        X_train[c] = X_train[c].cat.codes\n",
    "        \n",
    "        # Encode valid (handle unseen categories)\n",
    "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_valid[c] = X_valid[c].map(cat_mappings[c]).fillna(-1).astype(np.int32)\n",
    "    \n",
    "    return X_train, X_valid, cat_mappings\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358814e",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f345fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Get list of parquet files\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# Reduce number of files for faster training\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# Columns to drop early\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"row_id\", \"datetime\"]\n",
    "\n",
    "# Load TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Drop heavy columns BEFORE compute\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# Sample in Dask\n",
    "train_sample = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=RSEED).compute()\n",
    "train_sample = reduce_memory(train_sample)\n",
    "\n",
    "print(f\"Train loaded: {train_sample.shape}, Memory: {train_sample.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Clean memory\n",
    "del dd_train\n",
    "gc.collect()\n",
    "\n",
    "# Load VALID\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_valid.columns]\n",
    "dd_valid = dd_valid.drop(columns=existing_cols)\n",
    "\n",
    "# Sample less in validation\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=RSEED).compute()\n",
    "valid_df = reduce_memory(valid_df)\n",
    "\n",
    "print(f\"Valid loaded: {valid_df.shape}, Memory: {valid_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"Total memory: ~{(train_sample.memory_usage(deep=True).sum() + valid_df.memory_usage(deep=True).sum()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045041fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract targets\n",
    "y_train = train_sample[TARGET_COL].values\n",
    "y_valid = valid_df[TARGET_COL].values\n",
    "\n",
    "# Extract buyer labels\n",
    "y_train_buyer = train_sample[\"buyer_d7\"].values\n",
    "y_valid_buyer = valid_df[\"buyer_d7\"].values\n",
    "\n",
    "print(f\"Buyer ratio in train: {y_train_buyer.mean():.4f}\")\n",
    "print(f\"Buyer ratio in valid: {y_valid_buyer.mean():.4f}\")\n",
    "\n",
    "# Target transform: log1p for stability (MSLE)\n",
    "y_train_log = np.log1p(y_train.clip(min=0.0))\n",
    "y_valid_log = np.log1p(y_valid.clip(min=0.0))\n",
    "\n",
    "# Prepare features\n",
    "cols_to_drop = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
    "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop]\n",
    "\n",
    "X_train = train_sample[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "\n",
    "# Detect and remove list-like columns\n",
    "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
    "print(f\"Removing {len(listlike_cols)} list-like columns: {listlike_cols}\")\n",
    "X_train = X_train.drop(columns=listlike_cols)\n",
    "X_valid = X_valid.drop(columns=listlike_cols)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Features: {len(X_train.columns)} ({len(num_cols)} numeric, {len(cat_cols)} categorical)\")\n",
    "\n",
    "# Preprocess\n",
    "X_train_prep, X_valid_prep, cat_mappings = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n",
    "print(f\"Data prepared: X_train {X_train_prep.shape}, X_valid {X_valid_prep.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700f8a3",
   "metadata": {},
   "source": [
    "## PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, y, y_buyer=None):\n",
    "        self.cat = df[cat_cols].values.astype(np.int64) if len(cat_cols) > 0 else None\n",
    "        self.num = df[num_cols].values.astype(np.float32) if len(num_cols) > 0 else None\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.buyer = y_buyer.astype(np.float32) if y_buyer is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        if self.cat is not None:\n",
    "            item['cat'] = torch.tensor(self.cat[idx], dtype=torch.long)\n",
    "        if self.num is not None:\n",
    "            item['num'] = torch.tensor(self.num[idx], dtype=torch.float32)\n",
    "        item['y'] = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        if self.buyer is not None:\n",
    "            item['buyer'] = torch.tensor(self.buyer[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = TabularDataset(X_train_prep, cat_cols, num_cols, y_train_log, y_train_buyer)\n",
    "val_ds = TabularDataset(X_valid_prep, cat_cols, num_cols, y_valid_log, y_valid_buyer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Dataloaders ready. Batches per epoch: train={len(train_loader)}, val={len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb785ede",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_sizes(cat_cols, cat_mappings, max_emb_dim=50):\n",
    "    \"\"\"Calculate embedding sizes for categorical features.\"\"\"\n",
    "    emb_sizes = []\n",
    "    for c in cat_cols:\n",
    "        n_unique = len(cat_mappings[c]) + 1  # +1 for unseen category\n",
    "        emb_dim = min(max(1, n_unique // 10), max_emb_dim)\n",
    "        emb_sizes.append((n_unique, emb_dim))\n",
    "    return emb_sizes\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Larger teacher model with dual heads (regression + classification).\"\"\"\n",
    "    def __init__(self, emb_sizes, num_len):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(categories, dim) for categories, dim in emb_sizes])\n",
    "        emb_dim_sum = sum([dim for _, dim in emb_sizes]) if len(emb_sizes) > 0 else 0\n",
    "        input_dim = emb_dim_sum + (num_len if num_len > 0 else 0)\n",
    "        \n",
    "        # Main network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Regression head (revenue prediction)\n",
    "        self.reg_head = nn.Linear(128, 1)\n",
    "        \n",
    "        # Classification head (buyer prediction)\n",
    "        self.buyer_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        if x_cat is not None and len(self.embs) > 0:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + ([x_num] if x_num is not None else []), dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        \n",
    "        feat = self.net(x)\n",
    "        out_reg = self.reg_head(feat)\n",
    "        out_buyer = self.buyer_head(feat)\n",
    "        \n",
    "        return out_reg.view(-1), out_buyer.view(-1)\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Smaller student model (faster inference).\"\"\"\n",
    "    def __init__(self, emb_sizes, num_len):\n",
    "        super().__init__()\n",
    "        # Reduce embedding dimensions by half\n",
    "        small_embs = [(n, max(1, d // 2)) for n, d in emb_sizes]\n",
    "        self.embs = nn.ModuleList([nn.Embedding(categories, dim) for categories, dim in small_embs])\n",
    "        emb_dim_sum = sum([dim for _, dim in small_embs]) if len(small_embs) > 0 else 0\n",
    "        input_dim = emb_dim_sum + (num_len if num_len > 0 else 0)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_cat, x_num):\n",
    "        if x_cat is not None and len(self.embs) > 0:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + ([x_num] if x_num is not None else []), dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        \n",
    "        out = self.net(x)\n",
    "        return out.view(-1)\n",
    "\n",
    "print(\"Model classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "emb_sizes = get_embedding_sizes(cat_cols, cat_mappings, max_emb_dim=50)\n",
    "print(f\"Embedding sizes (categories, dim): {emb_sizes[:5]}...\")  # Show first 5\n",
    "\n",
    "teacher = TeacherModel(emb_sizes, len(num_cols)).to(DEVICE)\n",
    "student = StudentModel(emb_sizes, len(num_cols)).to(DEVICE)\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
    "student_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTeacher params: {teacher_params:,}\")\n",
    "print(f\"Student params: {student_params:,}\")\n",
    "print(f\"Compression ratio: {teacher_params / student_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45318f67",
   "metadata": {},
   "source": [
    "## Train Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27109caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(model, train_loader, val_loader, epochs=5, lr=1e-3, device='cpu'):\n",
    "    \"\"\"Train teacher model with dual objectives.\"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss_reg = 0.0\n",
    "        train_loss_buyer = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "            x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "            y = batch['y'].to(device)\n",
    "            buyer = batch['buyer'].to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            pred_log1p, pred_buyer = model(x_cat, x_num)\n",
    "            \n",
    "            loss_reg = mse_loss(pred_log1p, y)\n",
    "            loss_buyer = bce_loss(pred_buyer, buyer)\n",
    "            loss = loss_reg + 0.5 * loss_buyer\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            train_loss += loss.item() * len(y)\n",
    "            train_loss_reg += loss_reg.item() * len(y)\n",
    "            train_loss_buyer += loss_buyer.item() * len(y)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_loss_reg /= len(train_loader.dataset)\n",
    "        train_loss_buyer /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "                x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "                y = batch['y'].to(device)\n",
    "                buyer = batch['buyer'].to(device)\n",
    "                \n",
    "                pred_log1p, pred_buyer = model(x_cat, x_num)\n",
    "                loss_reg = mse_loss(pred_log1p, y)\n",
    "                loss_buyer = bce_loss(pred_buyer, buyer)\n",
    "                loss = loss_reg + 0.5 * loss_buyer\n",
    "                \n",
    "                val_loss += loss.item() * len(y)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train - Total: {train_loss:.6f}, Reg: {train_loss_reg:.6f}, Buyer: {train_loss_buyer:.6f}\")\n",
    "        print(f\"  Valid - Total: {val_loss:.6f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'teacher_model_v2.pt')\n",
    "    print(\"\\n✓ Teacher saved to teacher_model_v2.pt\")\n",
    "\n",
    "print(\"Teacher training function defined. Running training...\")\n",
    "train_teacher(teacher, train_loader, val_loader, epochs=TEACHER_EPOCHS, lr=LEARNING_RATE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb1e53",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_with_distillation(student, teacher, train_loader, val_loader, epochs=5, lr=1e-3, alpha=0.6, device='cpu'):\n",
    "    \"\"\"Train student using knowledge distillation from teacher.\n",
    "    \n",
    "    Loss = alpha * L_hard + (1 - alpha) * L_soft\n",
    "    where L_hard = MSE(student, true_label) and L_soft = MSE(student, teacher)\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(student.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    teacher.to(device)\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        student.train()\n",
    "        total_loss = 0.0\n",
    "        hard_loss_sum = 0.0\n",
    "        soft_loss_sum = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "            x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Get teacher predictions (soft targets)\n",
    "            with torch.no_grad():\n",
    "                t_pred_log1p, _ = teacher(x_cat, x_num)\n",
    "            \n",
    "            # Get student predictions\n",
    "            s_pred = student(x_cat, x_num)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss_hard = mse(s_pred, y)\n",
    "            loss_soft = mse(s_pred, t_pred_log1p)\n",
    "            loss = alpha * loss_hard + (1.0 - alpha) * loss_soft\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(y)\n",
    "            hard_loss_sum += loss_hard.item() * len(y)\n",
    "            soft_loss_sum += loss_soft.item() * len(y)\n",
    "        \n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        hard_loss_sum /= len(train_loader.dataset)\n",
    "        soft_loss_sum /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        student.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "                x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                s_pred = student(x_cat, x_num)\n",
    "                loss = mse(s_pred, y)\n",
    "                val_loss += loss.item() * len(y)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train - Total: {total_loss:.6f}, Hard: {hard_loss_sum:.6f}, Soft: {soft_loss_sum:.6f}\")\n",
    "        print(f\"  Valid - MSE: {val_loss:.6f}\")\n",
    "    \n",
    "    torch.save(student.state_dict(), 'student_model_v2.pt')\n",
    "    print(\"\\n✓ Student saved to student_model_v2.pt\")\n",
    "\n",
    "print(\"Distillation training function defined. Running distillation...\")\n",
    "train_student_with_distillation(student, teacher, train_loader, val_loader, \n",
    "                                epochs=STUDENT_EPOCHS, lr=LEARNING_RATE, \n",
    "                                alpha=DISTILL_ALPHA, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35588233",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, loader, device='cpu', return_buyer=False):\n",
    "    \"\"\"Generate predictions from model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    buyers_true = []\n",
    "    buyers_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "            x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Check if model has dual heads (teacher) or single head (student)\n",
    "            out = model(x_cat, x_num)\n",
    "            if isinstance(out, tuple):  # Teacher model\n",
    "                pred, buyer_pred = out\n",
    "                if return_buyer:\n",
    "                    buyers_pred.append(buyer_pred.cpu().numpy())\n",
    "            else:  # Student model\n",
    "                pred = out\n",
    "            \n",
    "            preds.append(pred.cpu().numpy())\n",
    "            trues.append(y.cpu().numpy())\n",
    "            \n",
    "            if 'buyer' in batch:\n",
    "                buyers_true.append(batch['buyer'].numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds).ravel()\n",
    "    trues = np.concatenate(trues).ravel()\n",
    "    buyers_true = np.concatenate(buyers_true).ravel() if buyers_true else None\n",
    "    buyers_pred = np.concatenate(buyers_pred).ravel() if buyers_pred else None\n",
    "    \n",
    "    return preds, trues, buyers_true, buyers_pred\n",
    "\n",
    "def msle_from_log_predictions(pred_log1p, true_log1p):\n",
    "    \"\"\"Calculate MSLE from log-transformed predictions.\"\"\"\n",
    "    pred = np.expm1(pred_log1p)\n",
    "    true = np.expm1(true_log1p)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "    true = np.clip(true, 0, None)\n",
    "    return mean_squared_log_error(true, pred)\n",
    "\n",
    "print(\"Evaluation utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Teacher\n",
    "print(\"=\" * 50)\n",
    "print(\"TEACHER MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "teacher_preds_log, teacher_trues_log, buyers_true, buyers_pred = predict_model(\n",
    "    teacher, val_loader, device=DEVICE, return_buyer=True\n",
    ")\n",
    "\n",
    "teacher_msle = msle_from_log_predictions(teacher_preds_log, teacher_trues_log)\n",
    "print(f\"Teacher MSLE: {teacher_msle:.6f}\")\n",
    "\n",
    "if buyers_pred is not None:\n",
    "    buyer_auc = roc_auc_score(buyers_true, buyers_pred)\n",
    "    print(f\"Teacher Buyer AUC: {buyer_auc:.4f}\")\n",
    "\n",
    "# Evaluate Student\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STUDENT MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "student_preds_log, student_trues_log, _, _ = predict_model(\n",
    "    student, val_loader, device=DEVICE\n",
    ")\n",
    "\n",
    "student_msle = msle_from_log_predictions(student_preds_log, student_trues_log)\n",
    "print(f\"Student MSLE: {student_msle:.6f}\")\n",
    "\n",
    "# Baseline (all zeros)\n",
    "baseline_msle = msle_from_log_predictions(\n",
    "    np.zeros_like(student_trues_log), student_trues_log\n",
    ")\n",
    "print(f\"\\nBaseline (all zeros) MSLE: {baseline_msle:.6f}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Teacher improvement: {((baseline_msle - teacher_msle) / baseline_msle * 100):.2f}%\")\n",
    "print(f\"Student improvement: {((baseline_msle - student_msle) / baseline_msle * 100):.2f}%\")\n",
    "print(f\"Student vs Teacher gap: {((student_msle - teacher_msle) / teacher_msle * 100):.2f}%\")\n",
    "\n",
    "# Distribution stats\n",
    "teacher_preds = np.expm1(teacher_preds_log)\n",
    "student_preds = np.expm1(student_preds_log)\n",
    "\n",
    "print(\"\\nTeacher predictions:\")\n",
    "print(f\"  Mean: {teacher_preds.mean():.4f}, Median: {np.median(teacher_preds):.4f}, Max: {teacher_preds.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(teacher_preds > 0).mean() * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nStudent predictions:\")\n",
    "print(f\"  Mean: {student_preds.mean():.4f}, Median: {np.median(student_preds):.4f}, Max: {student_preds.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(student_preds > 0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4db1f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Este notebook combina:\n",
    "1. **Carga de datos robusta** del `simplified_model_comparison.ipynb` (manejo de parquet con Dask, sampling, preprocesado)\n",
    "2. **Modelos de deep learning** (Teacher-Student distillation en PyTorch)\n",
    "\n",
    "### Ventajas del Student Model:\n",
    "- **~50% menos parámetros** que el teacher\n",
    "- **Inferencia más rápida** para producción\n",
    "- **Aprende del teacher** (soft targets) además de los labels reales\n",
    "\n",
    "### Para producción:\n",
    "- Exportar el student a ONNX: `torch.onnx.export(student, ...)`\n",
    "- Aplicar quantization para reducir tamaño y acelerar más\n",
    "- Usar solo el student model (descartar teacher)\n",
    "\n",
    "### Próximos pasos:\n",
    "- Ajustar hiperparámetros (learning rate, arquitectura, alpha)\n",
    "- Probar diferentes embedding dimensions\n",
    "- Añadir más épocas si hay suficiente memoria/tiempo\n",
    "- Implementar early stopping basado en validation loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
