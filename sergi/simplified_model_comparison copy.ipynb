{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e530c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: SAMPLE_FRAC=0.15, TARGET=iap_revenue_d7\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from time import time\n",
    "from glob import glob\n",
    "import dask.dataframe as dd\n",
    "import gc\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "TEST_PATH = '/home/stargix/Desktop/hackathons/datathon/test/test'\n",
    "\n",
    "# Data config\n",
    "SAMPLE_FRAC = 0.15  # Adjust for more/less data (0.15 = 15%)\n",
    "RANDOM_STATE = 42\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "ID_COL = \"row_id\"\n",
    "OUT_MODEL = \"lgbm_simple.joblib\"\n",
    "\n",
    "# Columnas a ignorar (complejas: listas/dicts)\n",
    "IGNORE_BIG_COLS = [\n",
    "    \"bundles_ins\", \"user_bundles\", \"user_bundles_l28d\",\n",
    "    \"city_hist\", \"country_hist\", \"region_hist\",\n",
    "    \"dev_language_hist\", \"dev_osv_hist\",\n",
    "    \"bcat\", \"bcat_bottom_taxonomy\",\n",
    "    \"bundles_cat\", \"bundles_cat_bottom_taxonomy\",\n",
    "    \"first_request_ts_bundle\", \"first_request_ts_category_bottom_taxonomy\",\n",
    "    \"last_buy_ts_bundle\", \"last_buy_ts_category\",\n",
    "    \"last_install_ts_bundle\", \"last_install_ts_category\",\n",
    "    \"advertiser_actions_action_count\", \"advertiser_actions_action_last_timestamp\",\n",
    "    \"user_actions_bundles_action_count\", \"user_actions_bundles_action_last_timestamp\",\n",
    "    \"new_bundles\",\n",
    "    \"whale_users_bundle_num_buys_prank\", \"whale_users_bundle_revenue_prank\",\n",
    "    \"whale_users_bundle_total_num_buys\", \"whale_users_bundle_total_revenue\",\n",
    "]\n",
    "\n",
    "LABEL_COLS = [\n",
    "    \"buyer_d1\", \"buyer_d7\", \"buyer_d14\", \"buyer_d28\",\n",
    "    \"buy_d7\", \"buy_d14\", \"buy_d28\",\n",
    "    \"iap_revenue_d7\", \"iap_revenue_d14\", \"iap_revenue_d28\",\n",
    "    \"registration\",\n",
    "    \"retention_d1_to_d7\", \"retention_d3_to_d7\", \"retention_d7_to_d14\",\n",
    "    \"retention_d1\", \"retention_d3\", \"retention_d7\",\n",
    "]\n",
    "\n",
    "print(f\"Configuration: SAMPLE_FRAC={SAMPLE_FRAC}, TARGET={TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc324f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ========== UTILITY FUNCTIONS ==========\n",
    "\n",
    "def frequency_encoding(df, col):\n",
    "    \"\"\"Frequency encoding para columnas de alta cardinalidad.\"\"\"\n",
    "    freqs = df[col].value_counts(dropna=False)\n",
    "    return df[col].map(freqs).astype(np.float32)\n",
    "\n",
    "def safe_label_encode(train_ser, valid_ser, test_ser=None):\n",
    "    \"\"\"Label encoding safe para categorÃ­as desconocidas.\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    # manejar nulos como string\n",
    "    train_vals = train_ser.fillna(\"__NA__\").astype(str)\n",
    "    le.fit(train_vals)\n",
    "    def transform(s):\n",
    "        return le.transform(s.fillna(\"__NA__\").astype(str))\n",
    "    return transform(train_ser), transform(valid_ser), transform(test_ser) if test_ser is not None else None, le\n",
    "\n",
    "def detect_listlike_columns(df, sample_size=100):\n",
    "    \"\"\"Detecta columnas con tipos complejos (listas, dicts).\"\"\"\n",
    "    listlike = []\n",
    "    for c in df.columns:\n",
    "        sample_vals = df[c].head(sample_size)\n",
    "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "            listlike.append(c)\n",
    "    return listlike\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"Downcast numeric columns para ahorrar memoria.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        elif col_type == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "print(\"âœ“ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e05f7a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc665935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data with Dask...\n",
      "Using 21 out of 144 files\n",
      "âœ“ Data loaded: (2998607, 57)\n",
      "âœ“ Data loaded: (2998607, 57)\n",
      "  Memory: 2.79 GB\n",
      "  Memory: 2.79 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading training data with Dask...\")\n",
    "\n",
    "# Obtener archivos parquet\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "num_files = max(1, int(len(parquet_files_all) * SAMPLE_FRAC))\n",
    "parquet_files_train = parquet_files_all[:num_files]\n",
    "\n",
    "print(f\"Using {num_files} out of {len(parquet_files_all)} files\")\n",
    "\n",
    "# Leer con Dask y dropear columnas pesadas antes de compute\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"datetime\"]\n",
    "\n",
    "dd_train = dd.read_parquet(parquet_files_train, engine='pyarrow')\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# Compute a pandas\n",
    "df = dd_train.compute()\n",
    "df = reduce_memory(df)\n",
    "\n",
    "print(f\"âœ“ Data loaded: {df.shape}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcc274",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50923b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n",
      "Removing 13 list-like columns\n",
      "Numeric columns: 11\n",
      "Categorical columns: 2\n",
      "Numeric columns: 11\n",
      "Categorical columns: 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m le_cols \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# label encoded\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cat_cols:\n\u001b[0;32m---> 29\u001b[0m     nunique \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nunique \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m     31\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m frequency_encoding(df, c)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/base.py:1067\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[0;34m(self, dropna)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1067\u001b[0m     uniqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropna:\n\u001b[1;32m   1069\u001b[0m         uniqs \u001b[38;5;241m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/series.py:2419\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2417\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/base.py:1029\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7260\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7203\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering...\")\n",
    "\n",
    "# Detectar y remover columnas complejas\n",
    "listlike_cols = detect_listlike_columns(df)\n",
    "print(f\"Removing {len(listlike_cols)} list-like columns\")\n",
    "df = df.drop(columns=listlike_cols, errors='ignore')\n",
    "\n",
    "# Remover target columns y ID\n",
    "cols_to_drop = [ID_COL] + LABEL_COLS\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Numeric columns: fill with 0\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "for c in numeric_cols:\n",
    "    df[c] = df[c].fillna(0).astype(np.float32)\n",
    "\n",
    "# Categorical columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {len(cat_cols)}\")\n",
    "\n",
    "# Strategy: \n",
    "# - High cardinality (>100) -> frequency encoding\n",
    "# - Low cardinality (<=100) -> label encoding\n",
    "fe_cols = []  # frequency encoded\n",
    "le_cols = []  # label encoded\n",
    "\n",
    "for c in cat_cols:\n",
    "    nunique = df[c].nunique(dropna=False)\n",
    "    if nunique > 100:\n",
    "        df[f\"{c}_freq\"] = frequency_encoding(df, c)\n",
    "        fe_cols.append(f\"{c}_freq\")\n",
    "    else:\n",
    "        le_cols.append(c)\n",
    "\n",
    "# Build feature list\n",
    "features = numeric_cols + fe_cols + le_cols\n",
    "\n",
    "print(f\"Total features: {len(features)}\")\n",
    "print(f\"  Numeric: {len(numeric_cols)}\")\n",
    "print(f\"  Frequency encoded: {len(fe_cols)}\")\n",
    "print(f\"  Label encoded: {len(le_cols)}\")\n",
    "\n",
    "# Remove rows with NaN target\n",
    "df = df[~df[TARGET].isna()].reset_index(drop=True)\n",
    "print(f\"âœ“ After removing NaN targets: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 14 list-like columns: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rwd_prank']\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "\n",
      "Applying preprocessing (KNN imputation + StandardScaler + categorical encoding)...\n",
      "  Filling 11 numeric columns with 0 for missing values...\n",
      "  Applying StandardScaler normalization...\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n",
      "âœ“ Memory optimized\n",
      "âœ“ Scaler saved for test predictions\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n",
      "âœ“ Memory optimized\n",
      "âœ“ Scaler saved for test predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Train/Valid Split and Encoding...\")\n",
    "\n",
    "# Split 80/20\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Valid: {valid_df.shape}\")\n",
    "\n",
    "# Label encode small-cardinality categories\n",
    "label_encoders = {}\n",
    "for col in le_cols:\n",
    "    train_vals = train_df[col].fillna(\"__NA__\").astype(str)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_vals)\n",
    "    \n",
    "    train_df[col] = le.transform(train_vals)\n",
    "    valid_df[col] = le.transform(valid_df[col].fillna(\"__NA__\").astype(str))\n",
    "    \n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Target transform (log1p)\n",
    "y_train = np.log1p(train_df[TARGET].values.astype(np.float32))\n",
    "y_valid = np.log1p(valid_df[TARGET].values.astype(np.float32))\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_df[features].astype(np.float32)\n",
    "X_valid = valid_df[features].astype(np.float32)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n",
    "print(f\"âœ“ Ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e51107",
   "metadata": {},
   "source": [
    "## Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f40d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING SINGLE LGBM REGRESSOR\n",
      "==================================================\n",
      "âœ“ Model trained\n",
      "âœ“ Model trained\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING LIGHTGBM REGRESSOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LightGBM params - simple y balanceado\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"max_depth\": -1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# Train with early stopping\n",
    "start = time()\n",
    "model = lgb.train(\n",
    "    params=lgb_params,\n",
    "    train_set=train_data,\n",
    "    num_boost_round=lgb_params[\"n_estimators\"],\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=100\n",
    ")\n",
    "elapsed = time() - start\n",
    "\n",
    "print(f\"\\nâœ“ Training completed in {elapsed:.1f}s\")\n",
    "print(f\"âœ“ Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Predictions\n",
    "pred_valid_log = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "pred_valid = np.expm1(pred_valid_log)\n",
    "pred_valid = np.clip(pred_valid, 0, None)\n",
    "\n",
    "# Evaluation\n",
    "msle = mean_squared_log_error(valid_df[TARGET].values, pred_valid)\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"VALIDATION METRICS\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"MSLE: {msle:.6f}\")\n",
    "\n",
    "# Additional metrics\n",
    "frac_zero_true = (valid_df[TARGET] == 0).mean()\n",
    "frac_zero_pred = (pred_valid == 0).mean()\n",
    "print(f\"True zero fraction: {frac_zero_true:.3f}\")\n",
    "print(f\"Pred zero fraction: {frac_zero_pred:.3f}\")\n",
    "print(f\"Mean prediction: {pred_valid.mean():.4f}\")\n",
    "print(f\"Max prediction: {pred_valid.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c1f0d",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafd9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION RESULTS\n",
      "==================================================\n",
      "Model: SINGLE\n",
      "MSLE: 0.185813\n",
      "Baseline (all zeros): 0.228072\n",
      "Improvement: 18.53%\n",
      "\n",
      "Prediction stats:\n",
      "  Mean: 0.1316\n",
      "  Median: 0.0011\n",
      "  Max: 50.3014\n",
      "  % Non-zero: 60.28%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': model.feature_importance()\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20 = importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['Importance'].values)\n",
    "plt.yticks(range(len(top_20)), top_20['Feature'].values)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved to feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ace372",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Processing 96 test chunks...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_new() got an unexpected keyword argument 'scaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m X_part \u001b[38;5;241m=\u001b[39m X_part[feature_cols_final]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ðŸ”¥ IMPORTANTE: Pasar el scaler para normalizaciÃ³n consistente\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m X_part_prep \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_prep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Predict based on model version\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODEL_VERSION \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: preprocess_new() got an unexpected keyword argument 'scaler'"
     ]
    }
   ],
   "source": [
    "print(\"Saving model and artifacts...\")\n",
    "\n",
    "# Save model + metadata\n",
    "artifacts = {\n",
    "    \"model\": model,\n",
    "    \"features\": features,\n",
    "    \"label_encoders\": label_encoders,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"fe_cols\": fe_cols,\n",
    "    \"le_cols\": le_cols,\n",
    "    \"lgb_params\": lgb_params\n",
    "}\n",
    "\n",
    "joblib.dump(artifacts, OUT_MODEL)\n",
    "print(f\"âœ“ Saved to {OUT_MODEL}\")\n",
    "\n",
    "# Function for batch prediction\n",
    "def predict_batch(df_batch, artifacts_path=OUT_MODEL):\n",
    "    \"\"\"Predict on new batch using saved artifacts.\"\"\"\n",
    "    art = joblib.load(artifacts_path)\n",
    "    mdl = art[\"model\"]\n",
    "    feats = art[\"features\"]\n",
    "    les = art[\"label_encoders\"]\n",
    "    num_cols_saved = art[\"numeric_cols\"]\n",
    "    fe_cols_saved = art[\"fe_cols\"]\n",
    "    le_cols_saved = art[\"le_cols\"]\n",
    "    \n",
    "    df_batch = df_batch.copy()\n",
    "    \n",
    "    # Fill numeric\n",
    "    for c in num_cols_saved:\n",
    "        if c in df_batch.columns:\n",
    "            df_batch[c] = df_batch[c].fillna(0).astype(np.float32)\n",
    "        else:\n",
    "            df_batch[c] = 0.0\n",
    "    \n",
    "    # Frequency encode high-cardinality cols\n",
    "    for c in fe_cols_saved:\n",
    "        orig_c = c.replace(\"_freq\", \"\")\n",
    "        if orig_c in df_batch.columns:\n",
    "            df_batch[c] = frequency_encoding(df_batch, orig_c)\n",
    "        else:\n",
    "            df_batch[c] = 0.0\n",
    "    \n",
    "    # Label encode small-cardinality cols\n",
    "    for col in le_cols_saved:\n",
    "        if col in df_batch.columns:\n",
    "            le = les[col]\n",
    "            df_batch[col] = le.transform(df_batch[col].fillna(\"__NA__\").astype(str)).astype(np.float32)\n",
    "        else:\n",
    "            df_batch[col] = 0.0\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for f in feats:\n",
    "        if f not in df_batch.columns:\n",
    "            df_batch[f] = 0.0\n",
    "    \n",
    "    X = df_batch[feats].astype(np.float32)\n",
    "    pred_log = mdl.predict(X, num_iteration=mdl.best_iteration)\n",
    "    pred = np.expm1(pred_log)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "    return pred\n",
    "\n",
    "print(\"âœ“ Prediction function ready\")\n",
    "print(\"\\nGenerating test predictions...\")\n",
    "\n",
    "# Process test in chunks (memory-efficient)\n",
    "dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow')\n",
    "existing_cols = [c for c in IGNORE_BIG_COLS if c in dd_test.columns]\n",
    "dd_test = dd_test.drop(columns=existing_cols, errors='ignore')\n",
    "\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "print(f\"Processing {len(delayed_parts)} test chunks...\")\n",
    "\n",
    "pred_dfs = []\n",
    "\n",
    "for i, d in enumerate(delayed_parts):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Chunk {i+1}/{len(delayed_parts)}...\")\n",
    "    \n",
    "    part_df = d.compute()\n",
    "    part_df = reduce_memory(part_df)\n",
    "    \n",
    "    row_ids = part_df[ID_COL].values\n",
    "    \n",
    "    # Predict\n",
    "    part_pred = predict_batch(part_df)\n",
    "    \n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        ID_COL: row_ids,\n",
    "        TARGET: part_pred\n",
    "    }))\n",
    "    \n",
    "    del part_df, part_pred\n",
    "    gc.collect()\n",
    "\n",
    "# Combine\n",
    "submission = pd.concat(pred_dfs, ignore_index=True)\n",
    "submission = submission.sort_values(ID_COL).reset_index(drop=True)\n",
    "\n",
    "output_file = \"submission.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"âœ“ Submission saved: {output_file}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Sample:\\n{submission.head(10)}\")\n",
    "\n",
    "# Validation\n",
    "print(f\"\\nValidation checks:\")\n",
    "print(f\"  NaN values: {submission.isna().sum().sum()}\")\n",
    "print(f\"  Negative values: {(submission[TARGET] < 0).sum()}\")\n",
    "print(f\"  Min: {submission[TARGET].min():.4f}, Max: {submission[TARGET].max():.4f}\")\n",
    "print(f\"  Mean: {submission[TARGET].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8486e1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This simplified pipeline:\n",
    "\n",
    "1. **Loads data** efficiently with Dask\n",
    "2. **Removes complex columns** (lists/dicts)\n",
    "3. **Feature Engineering**:\n",
    "   - Numeric: fill with 0\n",
    "   - Categorical high-cardinality (>100): frequency encoding\n",
    "   - Categorical low-cardinality (â‰¤100): label encoding\n",
    "4. **Trains LightGBM** with early stopping on log-transformed target\n",
    "5. **Generates submissions** in chunks (memory-efficient)\n",
    "6. **Saves artifacts** for future inference\n",
    "\n",
    "### Key Parameters to Adjust:\n",
    "- `SAMPLE_FRAC`: Data fraction to use (default 0.15 = 15%)\n",
    "- `lgb_params`: LightGBM hyperparameters\n",
    "- Test threshold for frequency encoding (currently >100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
