{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e530c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: single\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "MODEL_VERSION = \"single\"  # Options: \"single\" or \"two_step\"\n",
    "\n",
    "TRAIN_PATH = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "TEST_PATH = '/home/stargix/Desktop/hackathons/datathon/test/test'\n",
    "TARGET_COL = \"iap_revenue_d7\"\n",
    "TRAIN_SAMPLE_FRAC = 0.10  # Adjust for more/less data\n",
    "\n",
    "print(f\"Selected model: {MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc324f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x76d015742d40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import gc\n",
    "import os\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e05f7a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc665935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded (with advanced preprocessing).\n"
     ]
    }
   ],
   "source": [
    "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
    "    \"\"\"Preprocess train and validation sets with 0-filling and normalization.\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    \n",
    "    # Numeric: fill NaN with 0 for missing values\n",
    "    if num_cols:\n",
    "        print(f\"  Filling {len(num_cols)} numeric columns with 0 for missing values...\")\n",
    "        for c in num_cols:\n",
    "            X_train[c] = X_train[c].fillna(0)\n",
    "            X_valid[c] = X_valid[c].fillna(0)\n",
    "        \n",
    "        # Normalization: StandardScaler for numeric features\n",
    "        print(f\"  Applying StandardScaler normalization...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "        X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    # Categorical: convert to strings and set categories based on train\n",
    "    for c in cat_cols:\n",
    "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "        \n",
    "        cats = X_train[c].cat.categories\n",
    "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_valid[c] = X_valid[c].astype(pd.api.types.CategoricalDtype(categories=cats))\n",
    "    \n",
    "    return X_train, X_valid, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcc274",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50923b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 21 out of 144 train files\n",
      "Loading train data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "\n",
      "Loading validation data...\n",
      "\n",
      "Loading validation data...\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "\n",
      "âœ“ Data loaded successfully\n",
      "Total memory: ~0.44 GB\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Obtener lista de archivos parquet\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# ðŸ”¥ REDUCIR MÃS: Solo 5-10% de archivos\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))  # CambiÃ© a 5%\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# ðŸ”¥ DROPEAR COLUMNAS ANTES DE COMPUTE\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"row_id\", \"datetime\"]\n",
    "\n",
    "# Cargar TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Dropear columnas pesadas ANTES de compute\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# ðŸ”¥ SAMPLE EN DASK (no en pandas)\n",
    "train_sample = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=42).compute()\n",
    "train_sample = reduce_memory(train_sample)\n",
    "\n",
    "print(f\"Train loaded: {train_sample.shape}, Memory: {train_sample.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Limpiar memoria\n",
    "del dd_train\n",
    "gc.collect()\n",
    "\n",
    "# Cargar VALID (despuÃ©s de liberar train)\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,  # Mismos archivos\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_valid.columns]\n",
    "dd_valid = dd_valid.drop(columns=existing_cols)\n",
    "\n",
    "# ðŸ”¥ SAMPLE MENOS EN VALID (solo necesitas evaluar, no entrenar)\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=42).compute()\n",
    "valid_df = reduce_memory(valid_df)\n",
    "\n",
    "print(f\"Valid loaded: {valid_df.shape}, Memory: {valid_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully\")\n",
    "print(f\"Total memory: ~{(train_sample.memory_usage(deep=True).sum() + valid_df.memory_usage(deep=True).sum()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0fb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 14 list-like columns: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rwd_prank']\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "\n",
      "Applying preprocessing (KNN imputation + StandardScaler + categorical encoding)...\n",
      "  Applying KNN imputation to 11 numeric columns...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Preprocess with advanced imputation and normalization\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mApplying preprocessing (KNN imputation + StandardScaler + categorical encoding)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m X_train_prep, X_valid_prep, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_train_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData prepared: X_train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_prep\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, X_valid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_valid_prep\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Memory optimization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mpreprocess_train_valid\u001b[0;34m(X_train, X_valid, num_cols, cat_cols)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Applying KNN imputation to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(num_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m numeric columns...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m     knn_imputer \u001b[38;5;241m=\u001b[39m KNNImputer(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     X_train[num_cols] \u001b[38;5;241m=\u001b[39m \u001b[43mknn_imputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     X_valid[num_cols] \u001b[38;5;241m=\u001b[39m knn_imputer\u001b[38;5;241m.\u001b[39mtransform(X_valid[num_cols])\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Categorical: convert to strings and set categories based on train\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/base.py:894\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    879\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    880\u001b[0m             (\n\u001b[1;32m    881\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    890\u001b[0m         )\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/impute/_knn.py:376\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    368\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    369\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[1;32m    375\u001b[0m )\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# process_chunk modifies X in place. No return value.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2249\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2248\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2249\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2250\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/impute/_knn.py:359\u001b[0m, in \u001b[0;36mKNNImputer.transform.<locals>.process_chunk\u001b[0;34m(dist_chunk, start)\u001b[0m\n\u001b[1;32m    354\u001b[0m     dist_subset \u001b[38;5;241m=\u001b[39m dist_chunk[dist_idx_map[receivers_idx] \u001b[38;5;241m-\u001b[39m start][\n\u001b[1;32m    355\u001b[0m         :, potential_donors_idx\n\u001b[1;32m    356\u001b[0m     ]\n\u001b[1;32m    358\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neighbors, \u001b[38;5;28mlen\u001b[39m(potential_donors_idx))\n\u001b[0;32m--> 359\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_impute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m X[receivers_idx, col] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/impute/_knn.py:188\u001b[0m, in \u001b[0;36mKNNImputer._calc_impute\u001b[0;34m(self, dist_pot_donors, n_neighbors, fit_X_col, mask_fit_X_col)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to impute a single column.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    Imputed values for receiver.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Get donors\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m donors_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_pot_donors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    189\u001b[0m     :, :n_neighbors\n\u001b[1;32m    190\u001b[0m ]\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Get weight matrix from distance matrix\u001b[39;00m\n\u001b[1;32m    193\u001b[0m donors_dist \u001b[38;5;241m=\u001b[39m dist_pot_donors[\n\u001b[1;32m    194\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(donors_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m], donors_idx\n\u001b[1;32m    195\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:962\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    878\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    960\u001b[0m \n\u001b[1;32m    961\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract targets\n",
    "y_train = train_sample[TARGET_COL].values\n",
    "y_valid = valid_df[TARGET_COL].values\n",
    "\n",
    "# Always extract buyer labels (needed for two-step model)\n",
    "y_train_buyer = train_sample[\"buyer_d7\"].values\n",
    "y_valid_buyer = valid_df[\"buyer_d7\"].values\n",
    "\n",
    "if MODEL_VERSION == \"two_step\":\n",
    "    print(f\"Buyer ratio in train: {y_train_buyer.mean():.4f}\")\n",
    "    print(f\"Buyer ratio in valid: {y_valid_buyer.mean():.4f}\")\n",
    "\n",
    "# Prepare features\n",
    "cols_to_drop = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
    "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop]\n",
    "\n",
    "X_train = train_sample[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "\n",
    "# Detect and remove list-like columns\n",
    "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
    "print(f\"Removing {len(listlike_cols)} list-like columns: {listlike_cols}\")\n",
    "X_train = X_train.drop(columns=listlike_cols)\n",
    "X_valid = X_valid.drop(columns=listlike_cols)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Features: {len(X_train.columns)} ({len(num_cols)} numeric, {len(cat_cols)} categorical)\")\n",
    "\n",
    "# Preprocess with advanced imputation and normalization\n",
    "print(\"\\nApplying preprocessing (KNN imputation + StandardScaler + categorical encoding)...\")\n",
    "X_train_prep, X_valid_prep, scaler = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n",
    "print(f\"Data prepared: X_train {X_train_prep.shape}, X_valid {X_valid_prep.shape}\")\n",
    "\n",
    "# Memory optimization\n",
    "X_train_prep = reduce_memory(X_train_prep)\n",
    "X_valid_prep = reduce_memory(X_valid_prep)\n",
    "print(f\"âœ“ Memory optimized\")\n",
    "print(f\"âœ“ Scaler saved for test predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e51107",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f40d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING SINGLE LGBM REGRESSOR\n",
      "==================================================\n",
      "âœ“ Model trained\n",
      "âœ“ Model trained\n"
     ]
    }
   ],
   "source": [
    "MODEL_VERSION == \"single\"\n",
    "\n",
    "if MODEL_VERSION == \"single\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRAINING SINGLE LGBM REGRESSOR\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Transform target to log space\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    y_valid_log = np.log1p(y_valid)\n",
    "    \n",
    "    # Train single regressor\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=255,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        verbosity=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_prep, y_train_log)\n",
    "    print(\"âœ“ Model trained\")\n",
    "    \n",
    "    # Predict\n",
    "    valid_pred_log = model.predict(X_valid_prep)\n",
    "    valid_pred = np.expm1(valid_pred_log)\n",
    "    valid_pred = np.clip(valid_pred, 0, None)\n",
    "    \n",
    "    # Store for submission\n",
    "    models = {\"regressor\": model}\n",
    "    \n",
    "elif MODEL_VERSION == \"two_step\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRAINING TWO-STEP MODEL (CLASSIFIER + REGRESSOR)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Train buyer classifier\n",
    "    print(\"\\n[1/2] Training buyer classifier...\")\n",
    "    buyer_classifier = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=127,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        verbosity=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    buyer_classifier.fit(X_train_prep, y_train_buyer)\n",
    "    print(\"âœ“ Buyer classifier trained\")\n",
    "    \n",
    "    # Get buyer probabilities\n",
    "    buyer_prob_train = buyer_classifier.predict_proba(X_train_prep)[:, 1]\n",
    "    buyer_prob_valid = buyer_classifier.predict_proba(X_valid_prep)[:, 1]\n",
    "    \n",
    "    # Step 2: Train revenue regressor on buyers only\n",
    "    print(\"\\n[2/2] Training revenue regressor (buyers only)...\")\n",
    "    buyer_mask_train = y_train > 0\n",
    "    \n",
    "    X_train_buyers = X_train_prep[buyer_mask_train]\n",
    "    y_train_buyers_log = np.log1p(y_train[buyer_mask_train])\n",
    "    \n",
    "    print(f\"Training on {buyer_mask_train.sum()} buyers out of {len(y_train)} samples\")\n",
    "    \n",
    "    revenue_regressor = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=255,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        verbosity=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    revenue_regressor.fit(X_train_buyers, y_train_buyers_log)\n",
    "    print(\"âœ“ Revenue regressor trained\")\n",
    "    \n",
    "    # Step 3: Combined predictions\n",
    "    valid_revenue_pred_log = revenue_regressor.predict(X_valid_prep)\n",
    "    valid_revenue_pred = np.expm1(valid_revenue_pred_log)\n",
    "    valid_revenue_pred = np.clip(valid_revenue_pred, 0, None)\n",
    "    \n",
    "    # Final prediction: P(buyer) * E[revenue | buyer]\n",
    "    valid_pred = buyer_prob_valid * valid_revenue_pred\n",
    "    \n",
    "    # Store for submission\n",
    "    models = {\n",
    "        \"classifier\": buyer_classifier,\n",
    "        \"regressor\": revenue_regressor\n",
    "    }\n",
    "    \n",
    "    # Additional metrics\n",
    "    buyer_acc = (buyer_classifier.predict(X_valid_prep) == y_valid_buyer).mean()\n",
    "    print(f\"\\nBuyer classifier accuracy: {buyer_acc:.4f}\")\n",
    "    print(f\"Average predicted buyer probability: {buyer_prob_valid.mean():.4f}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid MODEL_VERSION: {MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c1f0d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafd9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION RESULTS\n",
      "==================================================\n",
      "Model: SINGLE\n",
      "MSLE: 0.186644\n",
      "Baseline (all zeros): 0.228072\n",
      "Improvement: 18.16%\n",
      "\n",
      "Prediction stats:\n",
      "  Mean: 0.1340\n",
      "  Median: 0.0010\n",
      "  Max: 57.9766\n",
      "  % Non-zero: 60.10%\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSLE\n",
    "msle_model = mean_squared_log_error(y_valid, valid_pred)\n",
    "msle_baseline = mean_squared_log_error(y_valid, np.zeros_like(y_valid))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_VERSION.upper()}\")\n",
    "print(f\"MSLE: {msle_model:.6f}\")\n",
    "print(f\"Baseline (all zeros): {msle_baseline:.6f}\")\n",
    "print(f\"Improvement: {((msle_baseline - msle_model) / msle_baseline * 100):.2f}%\")\n",
    "\n",
    "# Distribution stats\n",
    "print(f\"\\nPrediction stats:\")\n",
    "print(f\"  Mean: {valid_pred.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(valid_pred):.4f}\")\n",
    "print(f\"  Max: {valid_pred.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(valid_pred > 0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbd426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRID SEARCH - SINGLE LGBM REGRESSOR\n",
      "============================================================\n",
      "\n",
      "Total combinations: 729\n",
      "Estimated time: ~1458 minutes (aprox 2 min/model)\n",
      "\n",
      "[1/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.180962 | Time: 6.3s\n",
      "\n",
      "[2/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.180962 | Time: 6.3s\n",
      "\n",
      "[2/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.181132 | Time: 6.0s\n",
      "\n",
      "[3/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.181132 | Time: 6.0s\n",
      "\n",
      "[3/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.181049 | Time: 5.9s\n",
      "\n",
      "[4/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181049 | Time: 5.9s\n",
      "\n",
      "[4/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.180962 | Time: 5.4s\n",
      "\n",
      "[5/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.180962 | Time: 5.4s\n",
      "\n",
      "[5/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.181132 | Time: 5.8s\n",
      "\n",
      "[6/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.181132 | Time: 5.8s\n",
      "\n",
      "[6/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.181049 | Time: 5.9s\n",
      "\n",
      "[7/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181049 | Time: 5.9s\n",
      "\n",
      "[7/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.180962 | Time: 5.5s\n",
      "\n",
      "[8/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.180962 | Time: 5.5s\n",
      "\n",
      "[8/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.181132 | Time: 5.8s\n",
      "\n",
      "[9/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.181132 | Time: 5.8s\n",
      "\n",
      "[9/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.181049 | Time: 6.0s\n",
      "\n",
      "[10/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181049 | Time: 6.0s\n",
      "\n",
      "[10/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181122 | Time: 4.7s\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP 5 BEST MODELS\n",
      "============================================================\n",
      " n_estimators  learning_rate  num_leaves  max_depth  subsample  colsample_bytree     msle  train_time\n",
      "          400           0.03         127         -1        0.7               0.7 0.180962    6.338896\n",
      "          400           0.03         127         -1        0.8               0.7 0.180962    5.373410\n",
      "          400           0.03         127         -1        0.9               0.7 0.180962    5.520281\n",
      "          400           0.03         127         -1        0.7               0.9 0.181049    5.876275\n",
      "          400           0.03         127         -1        0.8               0.9 0.181049    5.937019\n",
      "\n",
      "============================================================\n",
      "BEST MODEL PARAMETERS\n",
      "============================================================\n",
      "  n_estimators: 400.0\n",
      "  learning_rate: 0.03\n",
      "  num_leaves: 127.0\n",
      "  max_depth: -1.0\n",
      "  subsample: 0.7\n",
      "  colsample_bytree: 0.7\n",
      "  msle: 0.1809620956646173\n",
      "  train_time: 6.338896\n",
      "\n",
      "============================================================\n",
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "============================================================\n",
      "  MSLE: 0.181122 | Time: 4.7s\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP 5 BEST MODELS\n",
      "============================================================\n",
      " n_estimators  learning_rate  num_leaves  max_depth  subsample  colsample_bytree     msle  train_time\n",
      "          400           0.03         127         -1        0.7               0.7 0.180962    6.338896\n",
      "          400           0.03         127         -1        0.8               0.7 0.180962    5.373410\n",
      "          400           0.03         127         -1        0.9               0.7 0.180962    5.520281\n",
      "          400           0.03         127         -1        0.7               0.9 0.181049    5.876275\n",
      "          400           0.03         127         -1        0.8               0.9 0.181049    5.937019\n",
      "\n",
      "============================================================\n",
      "BEST MODEL PARAMETERS\n",
      "============================================================\n",
      "  n_estimators: 400.0\n",
      "  learning_rate: 0.03\n",
      "  num_leaves: 127.0\n",
      "  max_depth: -1.0\n",
      "  subsample: 0.7\n",
      "  colsample_bytree: 0.7\n",
      "  msle: 0.1809620956646173\n",
      "  train_time: 6.338896\n",
      "\n",
      "============================================================\n",
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "============================================================\n",
      "\n",
      "âœ“ Final Model MSLE: 0.180962\n",
      "  Baseline MSLE: 0.228072\n",
      "  Improvement: 20.66%\n",
      "\n",
      "âœ“ Results saved to: grid_search_results.csv\n",
      "\n",
      "âœ“ Final Model MSLE: 0.180962\n",
      "  Baseline MSLE: 0.228072\n",
      "  Improvement: 20.66%\n",
      "\n",
      "âœ“ Results saved to: grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "## Grid Search for Single Model\n",
    "\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "# Verificar que estamos en modo single\n",
    "if MODEL_VERSION != \"single\":\n",
    "    print(\"âš ï¸ Grid search only works with MODEL_VERSION='single'\")\n",
    "    print(\"Please change MODEL_VERSION in the first cell and re-run from the beginning\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GRID SEARCH - SINGLE LGBM REGRESSOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [400, 600, 800],\n",
    "        'learning_rate': [0.03, 0.05, 0.07],\n",
    "        'num_leaves': [127, 255, 511],\n",
    "        'max_depth': [-1, 10, 15],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    combinations = list(itertools.product(*values))\n",
    "    \n",
    "    print(f\"\\nTotal combinations: {len(combinations)}\")\n",
    "    print(f\"Estimated time: ~{len(combinations) * 2} minutes (aprox 2 min/model)\\n\")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Transform target once\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    \n",
    "    # Grid search\n",
    "    for i, params in enumerate(combinations[:10], 1):  # Limit to first 10 for testing\n",
    "        param_dict = dict(zip(keys, params))\n",
    "        \n",
    "        # ðŸ”¥ FIX: Convertir parÃ¡metros int explÃ­citamente\n",
    "        param_dict['n_estimators'] = int(param_dict['n_estimators'])\n",
    "        param_dict['num_leaves'] = int(param_dict['num_leaves'])\n",
    "        param_dict['max_depth'] = int(param_dict['max_depth'])\n",
    "        \n",
    "        print(f\"[{i}/{min(10, len(combinations))}] Testing: {param_dict}\")\n",
    "        \n",
    "        # Train model\n",
    "        model = LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=0.0,\n",
    "            verbosity=-1,\n",
    "            random_state=42,\n",
    "            **param_dict\n",
    "        )\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        model.fit(X_train_prep, y_train_log)\n",
    "        train_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Predict\n",
    "        valid_pred_log = model.predict(X_valid_prep)\n",
    "        valid_pred = np.expm1(valid_pred_log)\n",
    "        valid_pred = np.clip(valid_pred, 0, None)\n",
    "        \n",
    "        # Evaluate\n",
    "        msle = mean_squared_log_error(y_valid, valid_pred)\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            **param_dict,\n",
    "            'msle': msle,\n",
    "            'train_time': train_time\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  MSLE: {msle:.6f} | Time: {train_time:.1f}s\\n\")\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('msle')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 5 BEST MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results_df.head(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BEST MODEL PARAMETERS\")\n",
    "    print(\"=\" * 60)\n",
    "    best_params = results_df.iloc[0].to_dict()\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Train final model with best params\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    best_model_params = {k: v for k, v in best_params.items() \n",
    "                         if k not in ['msle', 'train_time']}\n",
    "    \n",
    "    # ðŸ”¥ FIX: Asegurar que los parÃ¡metros finales tambiÃ©n son int\n",
    "    best_model_params['n_estimators'] = int(best_model_params['n_estimators'])\n",
    "    best_model_params['num_leaves'] = int(best_model_params['num_leaves'])\n",
    "    best_model_params['max_depth'] = int(best_model_params['max_depth'])\n",
    "    \n",
    "    final_model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        verbosity=-1,\n",
    "        random_state=42,\n",
    "        **best_model_params\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_prep, y_train_log)\n",
    "    \n",
    "    # Final predictions\n",
    "    valid_pred_log = final_model.predict(X_valid_prep)\n",
    "    valid_pred = np.expm1(valid_pred_log)\n",
    "    valid_pred = np.clip(valid_pred, 0, None)\n",
    "    \n",
    "    msle_final = mean_squared_log_error(y_valid, valid_pred)\n",
    "    msle_baseline = mean_squared_log_error(y_valid, np.zeros_like(y_valid))\n",
    "    \n",
    "    print(f\"\\nâœ“ Final Model MSLE: {msle_final:.6f}\")\n",
    "    print(f\"  Baseline MSLE: {msle_baseline:.6f}\")\n",
    "    print(f\"  Improvement: {((msle_baseline - msle_final) / msle_baseline * 100):.2f}%\")\n",
    "    \n",
    "    # Update models dict for submission\n",
    "    models = {\"regressor\": final_model}\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"grid_search_results.csv\", index=False)\n",
    "    print(f\"\\nâœ“ Results saved to: grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ace372",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Processing 96 test chunks...\n",
      "  Chunk 10/96...\n",
      "  Chunk 10/96...\n",
      "  Chunk 20/96...\n",
      "  Chunk 20/96...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     part_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(part_pred, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m MODEL_VERSION \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo_step\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     buyer_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_part_prep\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     47\u001b[0m     revenue_pred_log \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_part_prep)\n\u001b[1;32m     48\u001b[0m     revenue_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpm1(revenue_pred_log)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/sklearn.py:1627\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_proba\u001b[39m(\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1617\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1625\u001b[0m ):\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1627\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[1;32m   1638\u001b[0m         _log_warning(\n\u001b[1;32m   1639\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot compute class probabilities or labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning raw scores instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1642\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/sklearn.py:1144\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m predict_params \u001b[38;5;241m=\u001b[39m _choose_param_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m, predict_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m   1142\u001b[0m predict_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_n_jobs(predict_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Booster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredict_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:4767\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4765\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4766\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:1204\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(\n\u001b[1;32m   1198\u001b[0m         csc\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1199\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[1;32m   1200\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m   1201\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[1;32m   1202\u001b[0m     )\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1204\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[1;32m   1211\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_pyarrow_table(\n\u001b[1;32m   1212\u001b[0m         table\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1213\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[1;32m   1214\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m   1215\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[1;32m   1216\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:1361\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/lightgbm/basic.py:1308\u001b[0m, in \u001b[0;36m_InnerPredictor.__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1306\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1307\u001b[0m _safe_call(\n\u001b[0;32m-> 1308\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m )\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Generating test predictions...\")\n",
    "\n",
    "dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow')\n",
    "existing_big_cols_test = [c for c in IGNORE_BIG_COLS if c in dd_test.columns]\n",
    "dd_test = dd_test.drop(columns=existing_big_cols_test)\n",
    "\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "print(f\"Processing {len(delayed_parts)} test chunks...\")\n",
    "\n",
    "pred_dfs = []\n",
    "feature_cols_final = X_train_prep.columns.tolist()\n",
    "\n",
    "for i, d in enumerate(delayed_parts):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Chunk {i+1}/{len(delayed_parts)}...\")\n",
    "    \n",
    "    part_df = d.compute()\n",
    "    part_df = reduce_memory(part_df)\n",
    "    \n",
    "    row_ids = part_df[\"row_id\"].values\n",
    "    \n",
    "    # ðŸ”¥ IMPORTANTE: Solo seleccionar columnas que existen en test\n",
    "    available_cols = [c for c in feature_cols_final if c in part_df.columns]\n",
    "    X_part = part_df[available_cols].copy()\n",
    "    \n",
    "    # ðŸ”¥ AÃ±adir columnas faltantes con valores por defecto\n",
    "    for col in feature_cols_final:\n",
    "        if col not in X_part.columns:\n",
    "            if col in num_cols:\n",
    "                X_part[col] = 0  # Numeric â†’ 0\n",
    "            else:\n",
    "                X_part[col] = \"unknown\"  # Categorical â†’ unknown\n",
    "    \n",
    "    # Reordenar columnas para que coincidan con el train\n",
    "    X_part = X_part[feature_cols_final]\n",
    "    \n",
    "    # ðŸ”¥ IMPORTANTE: Pasar el scaler para normalizaciÃ³n consistente\n",
    "    X_part_prep = preprocess_new(X_part, num_cols, cat_cols, X_train_prep, scaler=scaler)\n",
    "    \n",
    "    # Predict based on model version\n",
    "    if MODEL_VERSION == \"single\":\n",
    "        part_pred_log = models[\"regressor\"].predict(X_part_prep)\n",
    "        part_pred = np.expm1(part_pred_log)\n",
    "        part_pred = np.clip(part_pred, 0, None)\n",
    "        \n",
    "    elif MODEL_VERSION == \"two_step\":\n",
    "        buyer_prob = models[\"classifier\"].predict_proba(X_part_prep)[:, 1]\n",
    "        revenue_pred_log = models[\"regressor\"].predict(X_part_prep)\n",
    "        revenue_pred = np.expm1(revenue_pred_log)\n",
    "        revenue_pred = np.clip(revenue_pred, 0, None)\n",
    "        part_pred = buyer_prob * revenue_pred\n",
    "    \n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"iap_revenue_d7\": part_pred\n",
    "    }))\n",
    "    \n",
    "    del part_df, X_part, X_part_prep, row_ids, part_pred\n",
    "    gc.collect()\n",
    "\n",
    "# Combine and save\n",
    "submission = pd.concat(pred_dfs, ignore_index=True)\n",
    "output_file = f\"submission_{MODEL_VERSION}.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Submission saved: {output_file}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Sample:\\n{submission.head()}\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"\\nValidation checks:\")\n",
    "print(f\"  NaN values: {submission.isna().sum().sum()}\")\n",
    "print(f\"  Negative values: {(submission['iap_revenue_d7'] < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8486e1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**To switch models**: Change `MODEL_VERSION` in the first code cell to:\n",
    "- `\"single\"` - Single LightGBM regressor (direct approach)\n",
    "- `\"two_step\"` - Classifier + Regressor (probabilistic approach)\n",
    "\n",
    "**To adjust data size**: Change `TRAIN_SAMPLE_FRAC` (default 0.10 = 10%)\n",
    "\n",
    "Then re-run all cells!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
