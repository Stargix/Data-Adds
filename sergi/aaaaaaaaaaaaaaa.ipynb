{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fe2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from glob import glob\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tqdm\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "test_path = '/home/stargix/Desktop/hackathons/datathon/test/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9854149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir columnas necesarias\n",
    "required_columns = [\n",
    "    'row_id', 'datetime',\n",
    "    'buyer_d7', 'iap_revenue_d7',\n",
    "    'advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', \n",
    "    'advertiser_bottom_taxonomy_level',\n",
    "    'country', 'region',\n",
    "    'dev_make', 'dev_model', 'dev_os', 'dev_osv',\n",
    "    'carrier',\n",
    "    'hour', 'weekday', 'weekend_ratio', 'hour_ratio',\n",
    "    'release_date', 'release_msrp',\n",
    "    'avg_act_days', 'avg_daily_sessions', 'avg_days_ins', 'avg_duration',\n",
    "    'weeks_since_first_seen', 'wifi_ratio',\n",
    "    'retentiond7',\n",
    "    'city_hist', 'country_hist', 'region_hist', 'dev_language_hist', 'dev_osv_hist',\n",
    "    'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk',\n",
    "    'iap_revenue_usd_bundle', 'iap_revenue_usd_category',\n",
    "    'num_buys_bundle', 'num_buys_category',\n",
    "    'last_buy', 'last_ins',\n",
    "    'bcat', 'bcat_bottom_taxonomy',\n",
    "    'bundles_cat', 'bundles_cat_bottom_taxonomy', \n",
    "    'bundles_ins',\n",
    "    'new_bundles', 'user_bundles', 'user_bundles_l28d',\n",
    "    'advertiser_actions_action_count', 'advertiser_actions_action_last_timestamp',\n",
    "    'user_actions_bundles_action_count', 'user_actions_bundles_action_last_timestamp',\n",
    "    'last_advertiser_action',\n",
    "    'first_request_ts', 'first_request_ts_bundle', \n",
    "    'first_request_ts_category_bottom_taxonomy',\n",
    "    'rwd_prank',\n",
    "    'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank'\n",
    "]\n",
    "\n",
    "# Cargar datos (10% de los archivos)\n",
    "parquet_files_train = glob(os.path.join(train_path, '**/part-*.parquet'), recursive=True)\n",
    "num_files_train = max(1, int(len(parquet_files_train) * 0.1))\n",
    "parquet_files_train = parquet_files_train[:num_files_train]\n",
    "\n",
    "try:\n",
    "    train_ddf = dd.read_parquet(parquet_files_train, engine='pyarrow', columns=required_columns)\n",
    "    print(f\"‚úì Train cargado con {num_files_train} archivos\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Cargando todas las columnas: {e}\")\n",
    "    train_ddf = dd.read_parquet(parquet_files_train, engine='pyarrow')\n",
    "\n",
    "# Computar a Pandas\n",
    "train_df = train_ddf.compute(scheduler='synchronous')\n",
    "print(f\"‚úì Train shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar columnas con listas - SUMAR todos los valores\n",
    "import ast\n",
    "\n",
    "columns_to_sum = [\n",
    "    'iap_revenue_usd_bundle',\n",
    "    'num_buys_bundle',\n",
    "    'rwd_prank',\n",
    "    'whale_users_bundle_num_buys_prank',\n",
    "    'whale_users_bundle_revenue_prank'\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESANDO COLUMNAS CON LISTAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def sum_values(x):\n",
    "    \"\"\"Suma todos los valores num√©ricos de la lista\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return 0\n",
    "    try:\n",
    "        # Si es string, convertir a lista\n",
    "        if isinstance(x, str):\n",
    "            x = ast.literal_eval(x)\n",
    "        \n",
    "        # Si es lista de tuplas, sumar el segundo valor de cada tupla\n",
    "        if isinstance(x, list) and len(x) > 0:\n",
    "            total = sum([item[1] for item in x if isinstance(item, tuple) and len(item) > 1])\n",
    "            return total\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for col in columns_to_sum:\n",
    "    if col in train_df.columns:\n",
    "        print(f\"\\nüìä Procesando {col}...\")\n",
    "        \n",
    "        # Aplicar transformaci√≥n\n",
    "        train_df[col] = train_df[col].apply(sum_values)\n",
    "        \n",
    "        # Verificar resultado\n",
    "        print(f\"  ‚úì Convertido a num√©rico\")\n",
    "        print(f\"  Tipo nuevo: {train_df[col].dtype}\")\n",
    "        print(f\"  Valores ejemplo: {train_df[col].head(3).values}\")\n",
    "        print(f\"  Stats: min={train_df[col].min():.4f}, max={train_df[col].max():.4f}, mean={train_df[col].mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {col} - NO ENCONTRADA\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì PREPROCESAMIENTO COMPLETADO\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir features categ√≥ricas\n",
    "cat_features = [\n",
    "    'advertiser_bundle', 'advertiser_category', 'advertiser_subcategory',\n",
    "    'country', 'region', 'dev_make', 'dev_model', 'dev_os', 'dev_osv'\n",
    "]\n",
    "\n",
    "labels_to_exclude = [\n",
    "    'buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28',\n",
    "    'buy_d7', 'buy_d14', 'buy_d28',\n",
    "    'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28',\n",
    "    'registration', 'retention_d1_to_d7', 'retention_d3_to_d7',\n",
    "    'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retention_d7',\n",
    "    'row_id', 'datetime',\n",
    "    'advertiser_actions_action_last_timestamp',\n",
    "    'user_actions_bundles_action_last_timestamp',\n",
    "    'first_request_ts', 'first_request_ts_bundle',\n",
    "    'first_request_ts_category_bottom_taxonomy'\n",
    "]\n",
    "\n",
    "cat_features = [c for c in cat_features if c in train_df.columns]\n",
    "\n",
    "# Label Encoding para categor√≠as\n",
    "label_encoders = {}\n",
    "for col in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col].astype(str).fillna(\"__NA__\"))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Features num√©ricas\n",
    "numeric_features = [\n",
    "    c for c in train_df.columns\n",
    "    if c not in labels_to_exclude and c not in cat_features\n",
    "    and train_df[c].dtype in ['int64', 'int32', 'int16', 'int8', 'float32', 'float64']\n",
    "]\n",
    "\n",
    "# Features finales\n",
    "features = numeric_features + cat_features\n",
    "print(f\"Total features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe699601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temporal - validaci√≥n solo del d√≠a 2025-10-06\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'].astype(str))\n",
    "\n",
    "# Usar solo el d√≠a 2025-10-06 para validaci√≥n\n",
    "val_date = pd.Timestamp('2025-10-06', tz='UTC')\n",
    "val_mask = train_df['datetime'].dt.date == val_date.date()\n",
    "\n",
    "X_train = train_df[~val_mask][features]\n",
    "X_val = train_df[val_mask][features]\n",
    "\n",
    "# Target\n",
    "y_train = train_df[~val_mask]['iap_revenue_d7']\n",
    "y_val = train_df[val_mask]['iap_revenue_d7']\n",
    "\n",
    "# Transform con log1p\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(f\"Fecha m√°xima dataset: {train_df['datetime'].max()}\")\n",
    "print(f\"Fecha validaci√≥n: {val_date.date()}\")\n",
    "print(f\"Train: {len(X_train):,} samples ({train_df[~val_mask]['datetime'].min()} a {train_df[~val_mask]['datetime'].max()})\")\n",
    "print(f\"Val: {len(X_val):,} samples ({train_df[val_mask]['datetime'].min()} a {train_df[val_mask]['datetime'].max()})\")\n",
    "print(f\"Val es solo d√≠a 2025-10-06: {train_df[val_mask]['datetime'].dt.date.nunique() == 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e779d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar datos\n",
    "for col in X_train.select_dtypes(include=['object']).columns:\n",
    "    if col not in cat_features:\n",
    "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "\n",
    "for col in cat_features:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "print(\"‚úì Datos preprocesados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAR√ÅMETROS OPTIMIZADOS\n",
    "params_optimized = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 127,\n",
    "    'max_depth': -1,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENANDO MODELO OPTIMIZADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Par√°metros:\")\n",
    "print(f\"  Learning rate: {params_optimized['learning_rate']}\")\n",
    "print(f\"  Num leaves: {params_optimized['num_leaves']}\")\n",
    "print(f\"  Max depth: {params_optimized['max_depth']}\")\n",
    "print(f\"  Min data in leaf: {params_optimized['min_data_in_leaf']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo\n",
    "cat_features_valid = [col for col in cat_features if col in features]\n",
    "\n",
    "train_ds = lgb.Dataset(X_train, label=y_train_log, categorical_feature=cat_features_valid)\n",
    "val_ds = lgb.Dataset(X_val, label=y_val_log, reference=train_ds)\n",
    "\n",
    "model = lgb.train(\n",
    "    params_optimized,\n",
    "    train_ds,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_ds, val_ds],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Modelo entrenado ({model.best_iteration} iteraciones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en validaci√≥n\n",
    "pred_log = model.predict(X_val)\n",
    "pred = np.expm1(pred_log).clip(0, None)\n",
    "\n",
    "msle = mean_squared_log_error(y_val, pred)\n",
    "rmse = mean_squared_error(y_val, pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTADOS EN VALIDACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"MSLE: {msle:.6f}\")\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"Revenue promedio predicho: ${pred.mean():.2f}\")\n",
    "print(f\"Revenue promedio real: ${y_val.mean():.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# GRID SEARCH PARA OPTIMIZAR HIPERPAR√ÅMETROS\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRID SEARCH - OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir grid de par√°metros\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'max_depth': [-1, 10, 15],\n",
    "    'min_data_in_leaf': [20, 50, 100],\n",
    "    'feature_fraction': [0.7, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Base params\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "# Preparar datasets\n",
    "cat_features_valid = [col for col in cat_features if col in features]\n",
    "train_ds = lgb.Dataset(X_train, label=y_train_log, categorical_feature=cat_features_valid)\n",
    "val_ds = lgb.Dataset(X_val, label=y_val_log, reference=train_ds)\n",
    "\n",
    "# Grid search\n",
    "results = []\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "print(f\"\\nTotal combinaciones a probar: {len(grid)}\")\n",
    "print(\"Probando combinaciones (esto puede tardar)...\\n\")\n",
    "\n",
    "for i, params in enumerate(grid[:20], 1):  # Limitar a 20 combinaciones para no tardar mucho\n",
    "    print(f\"[{i}/20] Probando: lr={params['learning_rate']}, leaves={params['num_leaves']}, depth={params['max_depth']}\")\n",
    "    \n",
    "    # Combinar params\n",
    "    current_params = {**base_params, **params}\n",
    "    \n",
    "    # Entrenar\n",
    "    start_time = time.time()\n",
    "    model_temp = lgb.train(\n",
    "        current_params,\n",
    "        train_ds,\n",
    "        num_boost_round=500,  # Reducido para grid search\n",
    "        valid_sets=[val_ds],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Evaluar\n",
    "    pred_log = model_temp.predict(X_val)\n",
    "    pred = np.expm1(pred_log).clip(0, None)\n",
    "    msle = mean_squared_log_error(y_val, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    \n",
    "    # Guardar resultado\n",
    "    results.append({\n",
    "        **params,\n",
    "        'msle': msle,\n",
    "        'rmse': rmse,\n",
    "        'best_iteration': model_temp.best_iteration,\n",
    "        'time': elapsed\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì MSLE: {msle:.6f} | RMSE: {rmse:.2f} | Iters: {model_temp.best_iteration} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Actualizar mejor\n",
    "    if msle < best_score:\n",
    "        best_score = msle\n",
    "        best_params = params\n",
    "        print(f\"  üéØ NUEVO MEJOR SCORE!\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTADOS GRID SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('msle')\n",
    "print(\"\\nTop 5 mejores combinaciones:\")\n",
    "print(results_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MEJORES PAR√ÅMETROS ENCONTRADOS:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nMejor MSLE: {best_score:.6f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Entrenar modelo final con mejores params\n",
    "print(\"\\nEntrenando modelo final con mejores par√°metros...\")\n",
    "final_params = {**base_params, **best_params}\n",
    "\n",
    "model_final = lgb.train(\n",
    "    final_params,\n",
    "    train_ds,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_ds, val_ds],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Modelo final entrenado ({model_final.best_iteration} iteraciones)\")\n",
    "\n",
    "# Evaluar modelo final\n",
    "pred_log = model_final.predict(X_val)\n",
    "pred = np.expm1(pred_log).clip(0, None)\n",
    "msle_final = mean_squared_log_error(y_val, pred)\n",
    "rmse_final = np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTADOS FINALES CON MEJORES PAR√ÅMETROS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"MSLE: {msle_final:.6f}\")\n",
    "print(f\"RMSE: ${rmse_final:.2f}\")\n",
    "print(f\"Revenue promedio predicho: ${pred.mean():.2f}\")\n",
    "print(f\"Revenue promedio real: ${y_val.mean():.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Guardar modelo final\n",
    "model = model_final\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407432a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "lgb.plot_importance(model, max_num_features=30, ax=ax, title='Top 30 Features - Modelo Optimizado')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Paths\n",
    "TEST_PATH = \"/home/stargix/Desktop/hackathons/datathon/test/test\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERANDO PREDICCIONES EN TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Cargar test con las columnas necesarias\n",
    "try:\n",
    "    dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow', columns=['row_id'] + features)\n",
    "    print(\"‚úì Test cargado con columnas espec√≠ficas\")\n",
    "except:\n",
    "    dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow')\n",
    "    print(\"‚úì Test cargado con todas las columnas\")\n",
    "\n",
    "# 2. Procesar por chunks\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "print(f\"N√∫mero de chunks: {len(delayed_parts)}\\n\")\n",
    "\n",
    "pred_dfs = []\n",
    "\n",
    "for i, delayed_part in enumerate(delayed_parts, 1):\n",
    "    print(f\"[{i}/{len(delayed_parts)}] Procesando chunk...\")\n",
    "    \n",
    "    # Computar chunk\n",
    "    part_df = delayed_part.compute()\n",
    "    \n",
    "    # Guardar row_ids\n",
    "    row_ids = part_df[\"row_id\"].values\n",
    "    \n",
    "    # Seleccionar y preparar features\n",
    "    X_part = part_df[[col for col in features if col in part_df.columns]].copy()\n",
    "    \n",
    "    # Asegurar que tenga todas las features del entrenamiento\n",
    "    for col in features:\n",
    "        if col not in X_part.columns:\n",
    "            X_part[col] = 0\n",
    "    \n",
    "    # Reordenar columnas como en entrenamiento\n",
    "    X_part = X_part[features]\n",
    "    \n",
    "    # Preprocesar igual que en train\n",
    "    # Columnas con listas (sumar valores)\n",
    "    columns_to_sum = [\n",
    "        'iap_revenue_usd_bundle', 'num_buys_bundle', 'rwd_prank',\n",
    "        'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank'\n",
    "    ]\n",
    "    \n",
    "    for col in columns_to_sum:\n",
    "        if col in X_part.columns:\n",
    "            X_part[col] = X_part[col].apply(lambda x: sum([item[1] for item in x if isinstance(item, tuple) and len(item) > 1]) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Label encoding para categ√≥ricas\n",
    "    for col in cat_features:\n",
    "        if col in X_part.columns:\n",
    "            # Usar el mismo encoder del train\n",
    "            X_part[col] = X_part[col].astype(str).fillna(\"__NA__\")\n",
    "            # Transformar con encoder existente, usar -1 para desconocidos\n",
    "            X_part[col] = X_part[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
    "            X_part[col] = X_part[col].astype('category')\n",
    "    \n",
    "    # Convertir num√©ricas\n",
    "    for col in numeric_features:\n",
    "        if col in X_part.columns:\n",
    "            X_part[col] = pd.to_numeric(X_part[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    X_part = X_part.fillna(0)\n",
    "    \n",
    "    # Predecir\n",
    "    pred_log = model.predict(X_part, num_iteration=model.best_iteration)\n",
    "    pred = np.expm1(pred_log).clip(0, None)\n",
    "    \n",
    "    # Guardar predicciones\n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"iap_revenue_d7\": pred\n",
    "    }))\n",
    "    \n",
    "    print(f\"  ‚úì {len(row_ids):,} predicciones | Avg revenue: ${pred.mean():.2f}\\n\")\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    del part_df, X_part, row_ids, pred_log, pred\n",
    "    gc.collect()\n",
    "\n",
    "# 3. Concatenar y guardar\n",
    "print(\"=\" * 60)\n",
    "print(\"FINALIZANDO SUBMISSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "submission = pd.concat(pred_dfs, ignore_index=True)\n",
    "\n",
    "# Validaciones\n",
    "print(f\"Total filas: {len(submission):,}\")\n",
    "print(f\"Row IDs √∫nicos: {submission['row_id'].nunique():,}\")\n",
    "print(f\"Duplicados: {submission['row_id'].duplicated().sum()}\")\n",
    "print(f\"NaNs: {submission['iap_revenue_d7'].isna().sum()}\")\n",
    "print(f\"\\nEstad√≠sticas predicciones:\")\n",
    "print(submission['iap_revenue_d7'].describe())\n",
    "\n",
    "# Guardar\n",
    "submission_path = '/home/stargix/Desktop/hackathons/datathon/submission_optimized.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Guardado en: {submission_path}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45266533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear submission\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': all_row_ids,\n",
    "    'iap_revenue_d7': all_predictions\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUBMISSION FINAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total filas: {len(submission):,}\")\n",
    "print(f\"Row IDs √∫nicos: {submission['row_id'].nunique():,}\")\n",
    "print(f\"Duplicados: {submission['row_id'].duplicated().sum()}\")\n",
    "print(f\"NaN: {submission['iap_revenue_d7'].isna().sum()}\")\n",
    "print(f\"\\nEstad√≠sticas:\")\n",
    "print(submission['iap_revenue_d7'].describe())\n",
    "\n",
    "# Guardar\n",
    "submission_path = '/home/stargix/Desktop/hackathons/datathon/submission_optimized.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úì Guardado en: {submission_path}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
