{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e530c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: SAMPLE_FRAC=0.05, TARGET=iap_revenue_d7\n",
      "âš ï¸  Using 5.0% of data to avoid memory issues\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from time import time\n",
    "from glob import glob\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import gc\n",
    "\n",
    "# ConfiguraciÃ³n de Dask para evitar warnings\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "TEST_PATH = '/home/stargix/Desktop/hackathons/datathon/test/test'\n",
    "\n",
    "# Data config\n",
    "SAMPLE_FRAC = 0.05  # ðŸ”¥ REDUCIDO a 5% para evitar memory issues\n",
    "RANDOM_STATE = 42\n",
    "TARGET = \"iap_revenue_d7\"\n",
    "ID_COL = \"row_id\"\n",
    "OUT_MODEL = \"lgbm_simple.joblib\"\n",
    "\n",
    "# Columnas a ignorar (complejas: listas/dicts)\n",
    "IGNORE_BIG_COLS = [\n",
    "    \"bundles_ins\", \"user_bundles\", \"user_bundles_l28d\",\n",
    "    \"city_hist\", \"country_hist\", \"region_hist\",\n",
    "    \"dev_language_hist\", \"dev_osv_hist\",\n",
    "    \"bcat\", \"bcat_bottom_taxonomy\",\n",
    "    \"bundles_cat\", \"bundles_cat_bottom_taxonomy\",\n",
    "    \"first_request_ts_bundle\", \"first_request_ts_category_bottom_taxonomy\",\n",
    "    \"last_buy_ts_bundle\", \"last_buy_ts_category\",\n",
    "    \"last_install_ts_bundle\", \"last_install_ts_category\",\n",
    "    \"advertiser_actions_action_count\", \"advertiser_actions_action_last_timestamp\",\n",
    "    \"user_actions_bundles_action_count\", \"user_actions_bundles_action_last_timestamp\",\n",
    "    \"new_bundles\",\n",
    "    \"whale_users_bundle_num_buys_prank\", \"whale_users_bundle_revenue_prank\",\n",
    "    \"whale_users_bundle_total_num_buys\", \"whale_users_bundle_total_revenue\",\n",
    "]\n",
    "\n",
    "LABEL_COLS = [\n",
    "    \"buyer_d1\", \"buyer_d7\", \"buyer_d14\", \"buyer_d28\",\n",
    "    \"buy_d7\", \"buy_d14\", \"buy_d28\",\n",
    "    \"iap_revenue_d7\", \"iap_revenue_d14\", \"iap_revenue_d28\",\n",
    "    \"registration\",\n",
    "    \"retention_d1_to_d7\", \"retention_d3_to_d7\", \"retention_d7_to_d14\",\n",
    "    \"retention_d1\", \"retention_d3\", \"retention_d7\",\n",
    "]\n",
    "\n",
    "print(f\"Configuration: SAMPLE_FRAC={SAMPLE_FRAC}, TARGET={TARGET}\")\n",
    "print(f\"âš ï¸  Using {SAMPLE_FRAC*100}% of data to avoid memory issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc324f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ========== UTILITY FUNCTIONS ==========\n",
    "\n",
    "def frequency_encoding(df, col):\n",
    "    \"\"\"Frequency encoding para columnas de alta cardinalidad.\"\"\"\n",
    "    freqs = df[col].value_counts(dropna=False)\n",
    "    return df[col].map(freqs).astype(np.float32)\n",
    "\n",
    "def safe_label_encode(train_ser, valid_ser, test_ser=None):\n",
    "    \"\"\"Label encoding safe para categorÃ­as desconocidas.\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    # manejar nulos como string\n",
    "    train_vals = train_ser.fillna(\"__NA__\").astype(str)\n",
    "    le.fit(train_vals)\n",
    "    def transform(s):\n",
    "        return le.transform(s.fillna(\"__NA__\").astype(str))\n",
    "    return transform(train_ser), transform(valid_ser), transform(test_ser) if test_ser is not None else None, le\n",
    "\n",
    "def detect_listlike_columns(df, sample_size=100):\n",
    "    \"\"\"Detecta columnas con tipos complejos (listas, dicts).\"\"\"\n",
    "    listlike = []\n",
    "    for c in df.columns:\n",
    "        sample_vals = df[c].head(sample_size)\n",
    "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "            listlike.append(c)\n",
    "    return listlike\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"Downcast numeric columns para ahorrar memoria.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        elif col_type == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "print(\"âœ“ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e05f7a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc665935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data with Dask...\n",
      "============================================================\n",
      "ðŸ“‚ Using 14 out of 144 files\n",
      "ðŸ”„ Reading parquet files...\n",
      "ðŸŽ² Sampling 5.0% of rows...\n",
      "ðŸ’¾ Computing to pandas...\n",
      "\n",
      "âœ“ Data loaded: (105213, 57)\n",
      "\n",
      "âœ“ Data loaded: (105213, 57)\n",
      "  Memory: 0.16 GB\n",
      "  Memory freed\n",
      "  Memory: 0.16 GB\n",
      "  Memory freed\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training data with Dask...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Obtener archivos parquet\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "num_files = max(1, int(len(parquet_files_all) * 0.1))  # Solo 10% de archivos\n",
    "parquet_files_train = parquet_files_all[:num_files]\n",
    "\n",
    "print(f\"ðŸ“‚ Using {num_files} out of {len(parquet_files_all)} files\")\n",
    "\n",
    "# Columnas a dropear ANTES de compute\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"datetime\"]\n",
    "\n",
    "# Leer con Dask\n",
    "print(\"ðŸ”„ Reading parquet files...\")\n",
    "dd_train = dd.read_parquet(parquet_files_train, engine='pyarrow')\n",
    "\n",
    "# Dropear columnas pesadas\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# ðŸ”¥ SAMPLE EN DASK antes de compute (crucial para memoria)\n",
    "print(f\"ðŸŽ² Sampling {SAMPLE_FRAC*100}% of rows...\")\n",
    "dd_train = dd_train.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "\n",
    "# Compute a pandas\n",
    "print(\"ðŸ’¾ Computing to pandas...\")\n",
    "df = dd_train.compute()\n",
    "\n",
    "# Reducir memoria\n",
    "df = reduce_memory(df)\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded: {df.shape}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Limpiar\n",
    "del dd_train\n",
    "gc.collect()\n",
    "print(f\"  Memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcc274",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50923b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£  Detecting complex columns...\n",
      "   Removing 14 list-like columns\n",
      "   Examples: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr']\n",
      "\n",
      "2ï¸âƒ£  Removing target and ID columns...\n",
      "\n",
      "2ï¸âƒ£  Removing target and ID columns...\n",
      "   Dropped 17 columns\n",
      "\n",
      "3ï¸âƒ£  Processing numeric columns...\n",
      "   Found 11 numeric columns\n",
      "\n",
      "4ï¸âƒ£  Processing categorical columns...\n",
      "   Found 15 categorical columns\n",
      "   Safe categorical columns: 15\n",
      "   âš ï¸  Error processing rev_by_adv: unhashable type: 'list'\n",
      "\n",
      "âœ“ Features summary:\n",
      "  Numeric: 11\n",
      "  Frequency encoded: 8\n",
      "  Label encoded: 6\n",
      "  Total: 25\n",
      "\n",
      "5ï¸âƒ£  Removing NaN targets...\n",
      "   Dropped 17 columns\n",
      "\n",
      "3ï¸âƒ£  Processing numeric columns...\n",
      "   Found 11 numeric columns\n",
      "\n",
      "4ï¸âƒ£  Processing categorical columns...\n",
      "   Found 15 categorical columns\n",
      "   Safe categorical columns: 15\n",
      "   âš ï¸  Error processing rev_by_adv: unhashable type: 'list'\n",
      "\n",
      "âœ“ Features summary:\n",
      "  Numeric: 11\n",
      "  Frequency encoded: 8\n",
      "  Label encoded: 6\n",
      "  Total: 25\n",
      "\n",
      "5ï¸âƒ£  Removing NaN targets...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'iap_revenue_d7'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'iap_revenue_d7'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5ï¸âƒ£  Removing NaN targets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[0;32m---> 77\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTARGET\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna()]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m removed \u001b[38;5;241m=\u001b[39m before \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Removed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows with NaN target\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'iap_revenue_d7'"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ðŸ”¥ PASO 1: Detectar y remover columnas complejas (listas, dicts, etc)\n",
    "print(\"\\n1ï¸âƒ£  Detecting complex columns...\")\n",
    "listlike_cols = detect_listlike_columns(df)\n",
    "print(f\"   Removing {len(listlike_cols)} list-like columns\")\n",
    "if listlike_cols:\n",
    "    print(f\"   Examples: {listlike_cols[:5]}\")\n",
    "df = df.drop(columns=listlike_cols, errors='ignore')\n",
    "\n",
    "# ðŸ”¥ PASO 2: Remover target columns y ID (EXCEPTO el TARGET que usaremos)\n",
    "print(\"\\n2ï¸âƒ£  Removing target and ID columns...\")\n",
    "cols_to_drop = [ID_COL] + [c for c in LABEL_COLS if c != TARGET]  # ðŸ”¥ KEEP TARGET!\n",
    "cols_to_drop_actual = [c for c in cols_to_drop if c in df.columns]\n",
    "df = df.drop(columns=cols_to_drop_actual, errors='ignore')\n",
    "print(f\"   Dropped {len(cols_to_drop_actual)} columns\")\n",
    "print(f\"   âœ“ Kept TARGET column: {TARGET}\")\n",
    "\n",
    "# ðŸ”¥ PASO 3: Procesar columnas numÃ©ricas\n",
    "print(\"\\n3ï¸âƒ£  Processing numeric columns...\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"   Found {len(numeric_cols)} numeric columns\")\n",
    "for c in numeric_cols:\n",
    "    if c != TARGET:  # No modificar target aÃºn\n",
    "        df[c] = df[c].fillna(0).astype(np.float32)\n",
    "\n",
    "# ðŸ”¥ PASO 4: Procesar columnas categÃ³ricas\n",
    "print(\"\\n4ï¸âƒ£  Processing categorical columns...\")\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"   Found {len(cat_cols)} categorical columns\")\n",
    "\n",
    "# Filtrar cat_cols: excluir columnas que contengan listas/dicts\n",
    "cat_cols_safe = []\n",
    "for c in cat_cols:\n",
    "    try:\n",
    "        # Verificar si realmente contiene valores simples\n",
    "        sample = df[c].head(10)\n",
    "        is_safe = all(isinstance(v, (str, type(None), float, int)) or pd.isna(v) for v in sample)\n",
    "        if is_safe:\n",
    "            cat_cols_safe.append(c)\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Excluding {c} (contains complex types)\")\n",
    "    except:\n",
    "        print(f\"   âš ï¸  Excluding {c} (error checking type)\")\n",
    "\n",
    "cat_cols = cat_cols_safe\n",
    "print(f\"   Safe categorical columns: {len(cat_cols)}\")\n",
    "\n",
    "# Strategy: \n",
    "# - High cardinality (>100) -> frequency encoding\n",
    "# - Low cardinality (<=100) -> label encoding\n",
    "fe_cols = []  # frequency encoded\n",
    "le_cols = []  # label encoded\n",
    "\n",
    "for c in cat_cols:\n",
    "    try:\n",
    "        nunique = df[c].nunique(dropna=False)\n",
    "        if nunique > 100:\n",
    "            df[f\"{c}_freq\"] = frequency_encoding(df, c)\n",
    "            fe_cols.append(f\"{c}_freq\")\n",
    "        else:\n",
    "            le_cols.append(c)\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error processing {c}: {str(e)[:50]}\")\n",
    "\n",
    "# Build feature list (numeric_cols includes TARGET, we'll remove it)\n",
    "numeric_features = [c for c in numeric_cols if c != TARGET]\n",
    "features = numeric_features + fe_cols + le_cols\n",
    "\n",
    "print(f\"\\nâœ“ Features summary:\")\n",
    "print(f\"  Numeric: {len(numeric_features)}\")\n",
    "print(f\"  Frequency encoded: {len(fe_cols)}\")\n",
    "print(f\"  Label encoded: {len(le_cols)}\")\n",
    "print(f\"  Total: {len(features)}\")\n",
    "\n",
    "# ðŸ”¥ PASO 5: Remover rows con target NaN\n",
    "print(\"\\n5ï¸âƒ£  Removing NaN targets...\")\n",
    "before = len(df)\n",
    "df = df[~df[TARGET].isna()].reset_index(drop=True)\n",
    "removed = before - len(df)\n",
    "print(f\"   Removed {removed} rows with NaN target\")\n",
    "print(f\"   Final shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 14 list-like columns: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rwd_prank']\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "\n",
      "Applying preprocessing (KNN imputation + StandardScaler + categorical encoding)...\n",
      "  Filling 11 numeric columns with 0 for missing values...\n",
      "  Applying StandardScaler normalization...\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n",
      "âœ“ Memory optimized\n",
      "âœ“ Scaler saved for test predictions\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n",
      "âœ“ Memory optimized\n",
      "âœ“ Scaler saved for test predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Train/Valid Split and Encoding...\")\n",
    "\n",
    "# Split 80/20\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Valid: {valid_df.shape}\")\n",
    "\n",
    "# Label encode small-cardinality categories\n",
    "label_encoders = {}\n",
    "for col in le_cols:\n",
    "    train_vals = train_df[col].fillna(\"__NA__\").astype(str)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_vals)\n",
    "    \n",
    "    train_df[col] = le.transform(train_vals)\n",
    "    valid_df[col] = le.transform(valid_df[col].fillna(\"__NA__\").astype(str))\n",
    "    \n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Target transform (log1p)\n",
    "y_train = np.log1p(train_df[TARGET].values.astype(np.float32))\n",
    "y_valid = np.log1p(valid_df[TARGET].values.astype(np.float32))\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_df[features].astype(np.float32)\n",
    "X_valid = valid_df[features].astype(np.float32)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n",
    "print(f\"âœ“ Ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e51107",
   "metadata": {},
   "source": [
    "## Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f40d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING SINGLE LGBM REGRESSOR\n",
      "==================================================\n",
      "âœ“ Model trained\n",
      "âœ“ Model trained\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING LIGHTGBM REGRESSOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LightGBM params - simple y balanceado\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"max_depth\": -1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# Train with early stopping\n",
    "start = time()\n",
    "model = lgb.train(\n",
    "    params=lgb_params,\n",
    "    train_set=train_data,\n",
    "    num_boost_round=lgb_params[\"n_estimators\"],\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]\n",
    ")\n",
    "elapsed = time() - start\n",
    "\n",
    "print(f\"\\nâœ“ Training completed in {elapsed:.1f}s\")\n",
    "print(f\"âœ“ Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Predictions\n",
    "pred_valid_log = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "pred_valid = np.expm1(pred_valid_log)\n",
    "pred_valid = np.clip(pred_valid, 0, None)\n",
    "\n",
    "# Evaluation\n",
    "y_valid_original = valid_df[TARGET].values\n",
    "msle = mean_squared_log_error(y_valid_original, pred_valid)\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"VALIDATION METRICS\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"MSLE: {msle:.6f}\")\n",
    "\n",
    "# Additional metrics\n",
    "frac_zero_true = (y_valid_original == 0).mean()\n",
    "frac_zero_pred = (pred_valid == 0).mean()\n",
    "print(f\"True zero fraction: {frac_zero_true:.3f}\")\n",
    "print(f\"Pred zero fraction: {frac_zero_pred:.3f}\")\n",
    "print(f\"Mean prediction: {pred_valid.mean():.4f}\")\n",
    "print(f\"Max prediction: {pred_valid.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c1f0d",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafd9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION RESULTS\n",
      "==================================================\n",
      "Model: SINGLE\n",
      "MSLE: 0.185813\n",
      "Baseline (all zeros): 0.228072\n",
      "Improvement: 18.53%\n",
      "\n",
      "Prediction stats:\n",
      "  Mean: 0.1316\n",
      "  Median: 0.0011\n",
      "  Max: 50.3014\n",
      "  % Non-zero: 60.28%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': model.feature_importance()\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20 = importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['Importance'].values)\n",
    "plt.yticks(range(len(top_20)), top_20['Feature'].values)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved to feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbd426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRID SEARCH - SINGLE LGBM REGRESSOR\n",
      "============================================================\n",
      "\n",
      "Total combinations: 729\n",
      "Estimated time: ~1458 minutes (aprox 2 min/model)\n",
      "\n",
      "[1/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181637 | Time: 4.2s\n",
      "\n",
      "[2/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.181637 | Time: 4.2s\n",
      "\n",
      "[2/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.180772 | Time: 4.2s\n",
      "\n",
      "[3/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.180772 | Time: 4.2s\n",
      "\n",
      "[3/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.180503 | Time: 4.7s\n",
      "\n",
      "[4/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.180503 | Time: 4.7s\n",
      "\n",
      "[4/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181637 | Time: 4.4s\n",
      "\n",
      "[5/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.181637 | Time: 4.4s\n",
      "\n",
      "[5/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.180772 | Time: 4.4s\n",
      "\n",
      "[6/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.180772 | Time: 4.4s\n",
      "\n",
      "[6/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.180503 | Time: 4.4s\n",
      "\n",
      "[7/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.180503 | Time: 4.4s\n",
      "\n",
      "[7/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181637 | Time: 4.2s\n",
      "\n",
      "[8/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.181637 | Time: 4.2s\n",
      "\n",
      "[8/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.180772 | Time: 4.4s\n",
      "\n",
      "[9/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.180772 | Time: 4.4s\n",
      "\n",
      "[9/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.180503 | Time: 4.2s\n",
      "\n",
      "[10/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.180503 | Time: 4.2s\n",
      "\n",
      "[10/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.181262 | Time: 3.4s\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP 5 BEST MODELS\n",
      "============================================================\n",
      " n_estimators  learning_rate  num_leaves  max_depth  subsample  colsample_bytree     msle  train_time\n",
      "          400           0.03         127         -1        0.7               0.9 0.180503    4.723041\n",
      "          400           0.03         127         -1        0.8               0.9 0.180503    4.352753\n",
      "          400           0.03         127         -1        0.9               0.9 0.180503    4.151641\n",
      "          400           0.03         127         -1        0.8               0.8 0.180772    4.379467\n",
      "          400           0.03         127         -1        0.9               0.8 0.180772    4.425844\n",
      "\n",
      "============================================================\n",
      "BEST MODEL PARAMETERS\n",
      "============================================================\n",
      "  n_estimators: 400.0\n",
      "  learning_rate: 0.03\n",
      "  num_leaves: 127.0\n",
      "  max_depth: -1.0\n",
      "  subsample: 0.7\n",
      "  colsample_bytree: 0.9\n",
      "  msle: 0.1805025997461251\n",
      "  train_time: 4.723041\n",
      "\n",
      "============================================================\n",
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "============================================================\n",
      "  MSLE: 0.181262 | Time: 3.4s\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP 5 BEST MODELS\n",
      "============================================================\n",
      " n_estimators  learning_rate  num_leaves  max_depth  subsample  colsample_bytree     msle  train_time\n",
      "          400           0.03         127         -1        0.7               0.9 0.180503    4.723041\n",
      "          400           0.03         127         -1        0.8               0.9 0.180503    4.352753\n",
      "          400           0.03         127         -1        0.9               0.9 0.180503    4.151641\n",
      "          400           0.03         127         -1        0.8               0.8 0.180772    4.379467\n",
      "          400           0.03         127         -1        0.9               0.8 0.180772    4.425844\n",
      "\n",
      "============================================================\n",
      "BEST MODEL PARAMETERS\n",
      "============================================================\n",
      "  n_estimators: 400.0\n",
      "  learning_rate: 0.03\n",
      "  num_leaves: 127.0\n",
      "  max_depth: -1.0\n",
      "  subsample: 0.7\n",
      "  colsample_bytree: 0.9\n",
      "  msle: 0.1805025997461251\n",
      "  train_time: 4.723041\n",
      "\n",
      "============================================================\n",
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "============================================================\n",
      "\n",
      "âœ“ Final Model MSLE: 0.180503\n",
      "  Baseline MSLE: 0.228072\n",
      "  Improvement: 20.86%\n",
      "\n",
      "âœ“ Results saved to: grid_search_results.csv\n",
      "\n",
      "âœ“ Final Model MSLE: 0.180503\n",
      "  Baseline MSLE: 0.228072\n",
      "  Improvement: 20.86%\n",
      "\n",
      "âœ“ Results saved to: grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model and artifacts...\")\n",
    "\n",
    "# Save model + metadata\n",
    "artifacts = {\n",
    "    \"model\": model,\n",
    "    \"features\": features,\n",
    "    \"label_encoders\": label_encoders,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"fe_cols\": fe_cols,\n",
    "    \"le_cols\": le_cols,\n",
    "    \"lgb_params\": lgb_params\n",
    "}\n",
    "\n",
    "joblib.dump(artifacts, OUT_MODEL)\n",
    "print(f\"âœ“ Saved to {OUT_MODEL}\")\n",
    "\n",
    "# Function for batch prediction\n",
    "def predict_batch(df_batch, artifacts_path=OUT_MODEL):\n",
    "    \"\"\"Predict on new batch using saved artifacts.\"\"\"\n",
    "    art = joblib.load(artifacts_path)\n",
    "    mdl = art[\"model\"]\n",
    "    feats = art[\"features\"]\n",
    "    les = art[\"label_encoders\"]\n",
    "    num_cols_saved = art[\"numeric_cols\"]\n",
    "    fe_cols_saved = art[\"fe_cols\"]\n",
    "    le_cols_saved = art[\"le_cols\"]\n",
    "    \n",
    "    df_batch = df_batch.copy()\n",
    "    \n",
    "    # Fill numeric\n",
    "    for c in num_cols_saved:\n",
    "        if c in df_batch.columns:\n",
    "            df_batch[c] = df_batch[c].fillna(0).astype(np.float32)\n",
    "        else:\n",
    "            df_batch[c] = 0.0\n",
    "    \n",
    "    # Frequency encode high-cardinality cols\n",
    "    for c in fe_cols_saved:\n",
    "        orig_c = c.replace(\"_freq\", \"\")\n",
    "        if orig_c in df_batch.columns:\n",
    "            df_batch[c] = frequency_encoding(df_batch, orig_c)\n",
    "        else:\n",
    "            df_batch[c] = 0.0\n",
    "    \n",
    "    # Label encode small-cardinality cols\n",
    "    for col in le_cols_saved:\n",
    "        if col in df_batch.columns:\n",
    "            le = les[col]\n",
    "            df_batch[col] = le.transform(df_batch[col].fillna(\"__NA__\").astype(str)).astype(np.float32)\n",
    "        else:\n",
    "            df_batch[col] = 0.0\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for f in feats:\n",
    "        if f not in df_batch.columns:\n",
    "            df_batch[f] = 0.0\n",
    "    \n",
    "    X = df_batch[feats].astype(np.float32)\n",
    "    pred_log = mdl.predict(X, num_iteration=mdl.best_iteration)\n",
    "    pred = np.expm1(pred_log)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "    return pred\n",
    "\n",
    "print(\"âœ“ Prediction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ace372",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Processing 96 test chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Processing 96 test chunks...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_new() got an unexpected keyword argument 'scaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m X_part \u001b[38;5;241m=\u001b[39m X_part[feature_cols_final]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ðŸ”¥ IMPORTANTE: Pasar el scaler para normalizaciÃ³n consistente\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m X_part_prep \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_prep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Predict based on model version\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODEL_VERSION \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: preprocess_new() got an unexpected keyword argument 'scaler'"
     ]
    }
   ],
   "source": [
    "print(\"Generating test predictions...\")\n",
    "\n",
    "# Process test in chunks (memory-efficient)\n",
    "dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow')\n",
    "existing_cols = [c for c in IGNORE_BIG_COLS if c in dd_test.columns]\n",
    "dd_test = dd_test.drop(columns=existing_cols, errors='ignore')\n",
    "\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "print(f\"Processing {len(delayed_parts)} test chunks...\")\n",
    "\n",
    "pred_dfs = []\n",
    "\n",
    "for i, d in enumerate(delayed_parts):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Chunk {i+1}/{len(delayed_parts)}...\")\n",
    "    \n",
    "    part_df = d.compute()\n",
    "    part_df = reduce_memory(part_df)\n",
    "    \n",
    "    row_ids = part_df[ID_COL].values\n",
    "    \n",
    "    # Predict\n",
    "    part_pred = predict_batch(part_df)\n",
    "    \n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        ID_COL: row_ids,\n",
    "        TARGET: part_pred\n",
    "    }))\n",
    "    \n",
    "    del part_df, part_pred\n",
    "    gc.collect()\n",
    "\n",
    "# Combine\n",
    "submission = pd.concat(pred_dfs, ignore_index=True)\n",
    "submission = submission.sort_values(ID_COL).reset_index(drop=True)\n",
    "\n",
    "output_file = \"submission.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"âœ“ Submission saved: {output_file}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Sample:\\n{submission.head(10)}\")\n",
    "\n",
    "# Validation\n",
    "print(f\"\\nValidation checks:\")\n",
    "print(f\"  NaN values: {submission.isna().sum().sum()}\")\n",
    "print(f\"  Negative values: {(submission[TARGET] < 0).sum()}\")\n",
    "print(f\"  Min: {submission[TARGET].min():.4f}, Max: {submission[TARGET].max():.4f}\")\n",
    "print(f\"  Mean: {submission[TARGET].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8486e1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This simplified pipeline:\n",
    "\n",
    "1. **Loads data** efficiently with Dask\n",
    "2. **Removes complex columns** (lists/dicts)\n",
    "3. **Feature Engineering**:\n",
    "   - Numeric: fill with 0\n",
    "   - Categorical high-cardinality (>100): frequency encoding\n",
    "   - Categorical low-cardinality (â‰¤100): label encoding\n",
    "4. **Trains LightGBM** with early stopping on log-transformed target\n",
    "5. **Generates submissions** in chunks (memory-efficient)\n",
    "6. **Saves artifacts** for future inference\n",
    "\n",
    "### Key Parameters to Adjust:\n",
    "- `SAMPLE_FRAC`: Data fraction to use (default 0.15 = 15%)\n",
    "- `lgb_params`: LightGBM hyperparameters\n",
    "- Test threshold for frequency encoding (currently >100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
