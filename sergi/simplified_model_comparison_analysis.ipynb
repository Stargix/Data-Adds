{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e530c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: single\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "MODEL_VERSION = \"single\"  # Options: \"single\" or \"two_step\"\n",
    "\n",
    "TRAIN_PATH = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "TEST_PATH = '/home/stargix/Desktop/hackathons/datathon/test/test'\n",
    "TARGET_COL = \"iap_revenue_d7\"\n",
    "TRAIN_SAMPLE_FRAC = 0.10  # Adjust for more/less data\n",
    "\n",
    "print(f\"Selected model: {MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc324f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7683f2bbed10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import gc\n",
    "import os\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e05f7a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc665935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Columnas problem√°ticas (listas/dicts) que se ignoran\n",
    "IGNORE_BIG_COLS = [\n",
    "    \"bundles_ins\", \"user_bundles\", \"user_bundles_l28d\",\n",
    "    \"city_hist\", \"country_hist\", \"region_hist\",\n",
    "    \"dev_language_hist\", \"dev_osv_hist\",\n",
    "    \"bcat\", \"bcat_bottom_taxonomy\",\n",
    "    \"bundles_cat\", \"bundles_cat_bottom_taxonomy\",\n",
    "    \"first_request_ts_bundle\", \"first_request_ts_category_bottom_taxonomy\",\n",
    "    \"last_buy_ts_bundle\", \"last_buy_ts_category\",\n",
    "    \"last_install_ts_bundle\", \"last_install_ts_category\",\n",
    "    \"advertiser_actions_action_count\", \"advertiser_actions_action_last_timestamp\",\n",
    "    \"user_actions_bundles_action_count\", \"user_actions_bundles_action_last_timestamp\",\n",
    "    \"new_bundles\",\n",
    "    \"whale_users_bundle_num_buys_prank\", \"whale_users_bundle_revenue_prank\",\n",
    "    \"whale_users_bundle_total_num_buys\", \"whale_users_bundle_total_revenue\",\n",
    "]\n",
    "\n",
    "LABEL_COLS = [\n",
    "    \"buyer_d1\", \"buyer_d7\", \"buyer_d14\", \"buyer_d28\",\n",
    "    \"buy_d7\", \"buy_d14\", \"buy_d28\",\n",
    "    \"iap_revenue_d7\", \"iap_revenue_d14\", \"iap_revenue_d28\",\n",
    "    \"registration\",\n",
    "    \"retention_d1_to_d7\", \"retention_d3_to_d7\", \"retention_d7_to_d14\",\n",
    "    \"retention_d1\", \"retention_d3\", \"retention_d7\",\n",
    "]\n",
    "\n",
    "# Variables cr√≠ticas seg√∫n EDA (features m√°s importantes)\n",
    "TIER1_FEATURES = [\n",
    "    \"iap_revenue_usd_bundle\", \"iap_revenue_usd_category\",\n",
    "    \"num_buys_bundle\", \"num_buys_category\",\n",
    "    \"advertiser_bundle\", \"advertiser_category\",\n",
    "    \"country\", \"cpm\", \"ctr\", \"avg_daily_sessions\"\n",
    "]\n",
    "\n",
    "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Downcast numeric columns to save memory.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        elif col_type == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
    "    \"\"\"Detect columns containing lists or dicts.\"\"\"\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    listlike = []\n",
    "    for c in cols:\n",
    "        sample_vals = df[c].head(100)\n",
    "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "            listlike.append(c)\n",
    "    return listlike\n",
    "\n",
    "def create_engineered_features(df):\n",
    "    \"\"\"Feature engineering basado en EDA.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Helper para verificar que columna es num√©rica\n",
    "    def is_numeric_col(df, col):\n",
    "        return col in df.columns and pd.api.types.is_numeric_dtype(df[col])\n",
    "    \n",
    "    # 1. Flags para missing (informaci√≥n valiosa: \"usuario nuevo\")\n",
    "    if is_numeric_col(df, \"iap_revenue_usd_bundle\"):\n",
    "        df[\"has_previous_revenue\"] = (df[\"iap_revenue_usd_bundle\"].fillna(0) > 0).astype(int)\n",
    "    \n",
    "    if is_numeric_col(df, \"num_buys_bundle\"):\n",
    "        df[\"has_previous_buys\"] = (df[\"num_buys_bundle\"].fillna(0) > 0).astype(int)\n",
    "    \n",
    "    # 2. Ratios (muy predictivos seg√∫n EDA)\n",
    "    if is_numeric_col(df, \"iap_revenue_usd_bundle\") and is_numeric_col(df, \"num_buys_bundle\"):\n",
    "        df[\"revenue_per_buy\"] = df[\"iap_revenue_usd_bundle\"] / (df[\"num_buys_bundle\"] + 1)\n",
    "    \n",
    "    if is_numeric_col(df, \"avg_daily_sessions\") and is_numeric_col(df, \"avg_duration\"):\n",
    "        df[\"session_intensity\"] = df[\"avg_daily_sessions\"] * df[\"avg_duration\"]\n",
    "    \n",
    "    # 3. Features temporales (importantes seg√∫n EDA)\n",
    "    if is_numeric_col(df, \"weekday\"):\n",
    "        df[\"is_weekend\"] = df[\"weekday\"].isin([5, 6]).astype(int)\n",
    "    \n",
    "    if is_numeric_col(df, \"hour\"):\n",
    "        df[\"is_peak_hour\"] = df[\"hour\"].isin([18, 19, 20, 21, 22]).astype(int)\n",
    "        df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "        df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "    \n",
    "    # 4. Bucketing de variables continuas\n",
    "    if is_numeric_col(df, \"release_msrp\"):\n",
    "        try:\n",
    "            df[\"device_price_tier\"] = pd.cut(df[\"release_msrp\"], \n",
    "                                              bins=[0, 200, 500, 1000, 10000], \n",
    "                                              labels=[0, 1, 2, 3]).astype(float)\n",
    "        except:\n",
    "            # Si falla el binning, crear feature simple\n",
    "            df[\"device_price_tier\"] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_low_variance_features(X_train, X_valid, threshold=0.01):\n",
    "    \"\"\"Eliminar features con baja varianza (casi constantes).\"\"\"\n",
    "    low_var_cols = []\n",
    "    for col in X_train.select_dtypes(include=[np.number]).columns:\n",
    "        unique_ratio = X_train[col].nunique() / len(X_train)\n",
    "        if unique_ratio < threshold:\n",
    "            low_var_cols.append(col)\n",
    "    \n",
    "    if low_var_cols:\n",
    "        print(f\"Removing {len(low_var_cols)} low variance features: {low_var_cols[:5]}...\")\n",
    "        X_train = X_train.drop(columns=low_var_cols)\n",
    "        X_valid = X_valid.drop(columns=[c for c in low_var_cols if c in X_valid.columns])\n",
    "    \n",
    "    return X_train, X_valid\n",
    "\n",
    "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols, y_train=None):\n",
    "    \"\"\"Preprocess train and validation sets with improved strategy.\"\"\"\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    \n",
    "    # Feature engineering primero\n",
    "    X_train = create_engineered_features(X_train)\n",
    "    X_valid = create_engineered_features(X_valid)\n",
    "    \n",
    "    # Actualizar listas de columnas despu√©s de feature engineering\n",
    "    new_num_cols = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\n",
    "    new_cat_cols = [c for c in X_train.columns if c not in new_num_cols]\n",
    "    \n",
    "    # Numeric: estrategia mejorada de imputaci√≥n\n",
    "    for c in new_num_cols:\n",
    "        # Para features de usuario (historial), -1 indica \"no data\" vs 0 \"no activity\"\n",
    "        is_user_feature = any(x in c for x in [\"revenue\", \"buys\", \"sessions\", \"days\", \"avg_\"])\n",
    "        \n",
    "        if is_user_feature:\n",
    "            X_train[c] = X_train[c].fillna(-1)\n",
    "            X_valid[c] = X_valid[c].fillna(-1)\n",
    "        else:\n",
    "            # Para otras features, usar mediana\n",
    "            median_val = X_train[c].median()\n",
    "            X_train[c] = X_train[c].fillna(median_val)\n",
    "            X_valid[c] = X_valid[c].fillna(median_val)\n",
    "    \n",
    "    # Categorical: frequency encoding para alta cardinalidad\n",
    "    freq_encoding_cols = []\n",
    "    for c in new_cat_cols:\n",
    "        try:\n",
    "            # Convertir a string primero para evitar problemas de tipos\n",
    "            X_train[c] = X_train[c].astype(str)\n",
    "            X_valid[c] = X_valid[c].astype(str)\n",
    "            \n",
    "            n_unique = X_train[c].nunique()\n",
    "            \n",
    "            # Alta cardinalidad (>100 valores) ‚Üí frequency encoding\n",
    "            if n_unique > 100:\n",
    "                freq_encoding_cols.append(c)\n",
    "                freq_map = X_train[c].value_counts(normalize=True).to_dict()\n",
    "                X_train[c + \"_freq\"] = X_train[c].map(freq_map).fillna(0).astype(np.float32)\n",
    "                X_valid[c + \"_freq\"] = X_valid[c].map(freq_map).fillna(0).astype(np.float32)\n",
    "                # Eliminar original\n",
    "                X_train = X_train.drop(columns=[c])\n",
    "                X_valid = X_valid.drop(columns=[c])\n",
    "            else:\n",
    "                # Baja cardinalidad ‚Üí categorical encoding normal\n",
    "                X_train[c] = X_train[c].fillna(\"MISSING\")\n",
    "                X_train[c] = X_train[c].astype(\"category\")\n",
    "                \n",
    "                cats = X_train[c].cat.categories\n",
    "                X_valid[c] = X_valid[c].fillna(\"MISSING\")\n",
    "                X_valid[c] = X_valid[c].astype(pd.api.types.CategoricalDtype(categories=cats))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process categorical column {c}: {e}\")\n",
    "            # Si falla, eliminar la columna\n",
    "            if c in X_train.columns:\n",
    "                X_train = X_train.drop(columns=[c])\n",
    "            if c in X_valid.columns:\n",
    "                X_valid = X_valid.drop(columns=[c])\n",
    "    \n",
    "    if freq_encoding_cols:\n",
    "        print(f\"Applied frequency encoding to {len(freq_encoding_cols)} high-cardinality features\")\n",
    "    \n",
    "    # Eliminar features de baja varianza\n",
    "    X_train, X_valid = remove_low_variance_features(X_train, X_valid)\n",
    "    \n",
    "    return X_train, X_valid\n",
    "\n",
    "def preprocess_new(X_new, num_cols, cat_cols, cat_ref_df, freq_encoding_info=None):\n",
    "    \"\"\"Preprocess test data using train statistics.\"\"\"\n",
    "    X_new = X_new.copy()\n",
    "    \n",
    "    # Feature engineering\n",
    "    X_new = create_engineered_features(X_new)\n",
    "    \n",
    "    # Actualizar listas despu√©s de feature engineering\n",
    "    new_num_cols = [c for c in X_new.columns if pd.api.types.is_numeric_dtype(X_new[c])]\n",
    "    new_cat_cols = [c for c in X_new.columns if c not in new_num_cols]\n",
    "    \n",
    "    # Numeric imputation\n",
    "    for c in new_num_cols:\n",
    "        if c in X_new.columns:\n",
    "            is_user_feature = any(x in c for x in [\"revenue\", \"buys\", \"sessions\", \"days\", \"avg_\"])\n",
    "            if is_user_feature:\n",
    "                X_new[c] = X_new[c].fillna(-1)\n",
    "            else:\n",
    "                # Usar mediana del train si est√° disponible\n",
    "                if c in cat_ref_df.columns and pd.api.types.is_numeric_dtype(cat_ref_df[c]):\n",
    "                    median_val = cat_ref_df[c].median()\n",
    "                else:\n",
    "                    median_val = 0\n",
    "                X_new[c] = X_new[c].fillna(median_val)\n",
    "    \n",
    "    # Categorical\n",
    "    for c in new_cat_cols:\n",
    "        if c in X_new.columns:\n",
    "            try:\n",
    "                X_new[c] = X_new[c].astype(str).fillna(\"MISSING\")\n",
    "                \n",
    "                if c in cat_ref_df.columns and hasattr(cat_ref_df[c], 'cat'):\n",
    "                    cats = cat_ref_df[c].cat.categories\n",
    "                    X_new[c] = X_new[c].astype(pd.api.types.CategoricalDtype(categories=cats))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process test column {c}: {e}\")\n",
    "                # Si falla, eliminar\n",
    "                if c in X_new.columns:\n",
    "                    X_new = X_new.drop(columns=[c])\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcc274",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50923b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 21 out of 144 train files\n",
      "Loading train data...\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "\n",
      "Loading validation data...\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "\n",
      "‚úì Data loaded successfully\n",
      "Total memory: ~0.44 GB\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Obtener lista de archivos parquet\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# üî• REDUCIR M√ÅS: Solo 5-10% de archivos\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))  # Cambi√© a 5%\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# üî• DROPEAR COLUMNAS ANTES DE COMPUTE\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"row_id\", \"datetime\"]\n",
    "\n",
    "# Cargar TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Dropear columnas pesadas ANTES de compute\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# üî• SAMPLE EN DASK (no en pandas)\n",
    "train_sample = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=42).compute()\n",
    "train_sample = reduce_memory(train_sample)\n",
    "\n",
    "print(f\"Train loaded: {train_sample.shape}, Memory: {train_sample.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Limpiar memoria\n",
    "del dd_train\n",
    "gc.collect()\n",
    "\n",
    "# Cargar VALID (despu√©s de liberar train)\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,  # Mismos archivos\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_valid.columns]\n",
    "dd_valid = dd_valid.drop(columns=existing_cols)\n",
    "\n",
    "# üî• SAMPLE MENOS EN VALID (solo necesitas evaluar, no entrenar)\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=42).compute()\n",
    "valid_df = reduce_memory(valid_df)\n",
    "\n",
    "print(f\"Valid loaded: {valid_df.shape}, Memory: {valid_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úì Data loaded successfully\")\n",
    "print(f\"Total memory: ~{(train_sample.memory_usage(deep=True).sum() + valid_df.memory_usage(deep=True).sum()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0fb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 14 list-like columns: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rwd_prank']\n",
      "Features before preprocessing: 26 (11 numeric, 15 categorical)\n",
      "Applied frequency encoding to 9 high-cardinality features\n",
      "Removing 18 low variance features: ['retentiond7', 'release_msrp', 'weekday', 'avg_act_days', 'avg_days_ins']...\n",
      "Features after preprocessing: 10 (4 numeric, 6 categorical)\n",
      "Data prepared: X_train (271487, 10), X_valid (28373, 10)\n"
     ]
    }
   ],
   "source": [
    "# Extract targets\n",
    "y_train = train_sample[TARGET_COL].values\n",
    "y_valid = valid_df[TARGET_COL].values\n",
    "\n",
    "# Always extract buyer labels (needed for two-step model)\n",
    "y_train_buyer = train_sample[\"buyer_d7\"].values\n",
    "y_valid_buyer = valid_df[\"buyer_d7\"].values\n",
    "\n",
    "if MODEL_VERSION == \"two_step\":\n",
    "    print(f\"Buyer ratio in train: {y_train_buyer.mean():.4f}\")\n",
    "    print(f\"Buyer ratio in valid: {y_valid_buyer.mean():.4f}\")\n",
    "\n",
    "# Prepare features\n",
    "cols_to_drop = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
    "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop]\n",
    "\n",
    "X_train = train_sample[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "\n",
    "# Detect and remove list-like columns\n",
    "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
    "print(f\"Removing {len(listlike_cols)} list-like columns: {listlike_cols}\")\n",
    "X_train = X_train.drop(columns=listlike_cols)\n",
    "X_valid = X_valid.drop(columns=listlike_cols)\n",
    "\n",
    "# Identify numeric and categorical columns BEFORE preprocessing\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Features before preprocessing: {len(X_train.columns)} ({len(num_cols)} numeric, {len(cat_cols)} categorical)\")\n",
    "\n",
    "# Preprocess with improved strategy\n",
    "X_train_prep, X_valid_prep = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols, y_train)\n",
    "\n",
    "# Actualizar num_cols y cat_cols despu√©s del preprocessing\n",
    "final_num_cols = X_train_prep.select_dtypes(include=[np.number]).columns.tolist()\n",
    "final_cat_cols = [c for c in X_train_prep.columns if c not in final_num_cols]\n",
    "\n",
    "print(f\"Features after preprocessing: {len(X_train_prep.columns)} ({len(final_num_cols)} numeric, {len(final_cat_cols)} categorical)\")\n",
    "print(f\"Data prepared: X_train {X_train_prep.shape}, X_valid {X_valid_prep.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e51107",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829f40d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING SINGLE LGBM REGRESSOR\n",
      "==================================================\n",
      "‚úì Model trained\n",
      "\n",
      "üî• Top 20 Most Important Features:\n",
      "                         feature  importance\n",
      "                   weekend_ratio       23492\n",
      "                      wifi_ratio       22193\n",
      "                        last_ins       12300\n",
      "                        last_buy        9256\n",
      "                            hour        6444\n",
      "          advertiser_subcategory        5726\n",
      "advertiser_bottom_taxonomy_level        5429\n",
      "                          dev_os        2126\n",
      "             advertiser_category        1949\n",
      "          last_advertiser_action         473\n"
     ]
    }
   ],
   "source": [
    "MODEL_VERSION == \"single\"\n",
    "\n",
    "if MODEL_VERSION == \"single\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRAINING SINGLE LGBM REGRESSOR\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Transform target to log space\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    y_valid_log = np.log1p(y_valid)\n",
    "    \n",
    "    # Train single regressor with optimized params (based on EDA recommendations)\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        metric=\"rmse\",\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=127,  # Reducido para evitar overfitting\n",
    "        max_depth=10,    # Limitado seg√∫n EDA\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,   # L1 regularization (ayuda con features irrelevantes)\n",
    "        reg_lambda=0.1,  # L2 regularization\n",
    "        min_child_samples=20,  # Evitar hojas con muy pocos samples\n",
    "        verbosity=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_prep, y_train_log,\n",
    "        eval_set=[(X_valid_prep, y_valid_log)],\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    print(\"‚úì Model trained\")\n",
    "    \n",
    "    # Predict\n",
    "    valid_pred_log = model.predict(X_valid_prep)\n",
    "    valid_pred = np.expm1(valid_pred_log)\n",
    "    valid_pred = np.clip(valid_pred, 0, None)\n",
    "    \n",
    "    # Store for submission\n",
    "    models = {\"regressor\": model}\n",
    "    \n",
    "    # Feature importance (top 20)\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X_train_prep.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüî• Top 20 Most Important Features:\")\n",
    "    print(importance.head(20).to_string(index=False))\n",
    "    \n",
    "elif MODEL_VERSION == \"two_step\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TRAINING TWO-STEP MODEL (CLASSIFIER + REGRESSOR)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Train buyer classifier\n",
    "    print(\"\\n[1/2] Training buyer classifier...\")\n",
    "    buyer_classifier = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=127,\n",
    "        max_depth=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        min_child_samples=20,\n",
    "        verbosity=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    buyer_classifier.fit(X_train_prep, y_train_buyer)\n",
    "    print(\"‚úì Buyer classifier trained\")\n",
    "    \n",
    "    # Get buyer probabilities\n",
    "    buyer_prob_train = buyer_classifier.predict_proba(X_train_prep)[:, 1]\n",
    "    buyer_prob_valid = buyer_classifier.predict_proba(X_valid_prep)[:, 1]\n",
    "    \n",
    "    # Step 2: Train revenue regressor on buyers only\n",
    "    print(\"\\n[2/2] Training revenue regressor (buyers only)...\")\n",
    "    buyer_mask_train = y_train > 0\n",
    "    \n",
    "    X_train_buyers = X_train_prep[buyer_mask_train]\n",
    "    y_train_buyers_log = np.log1p(y_train[buyer_mask_train])\n",
    "    \n",
    "    print(f\"Training on {buyer_mask_train.sum()} buyers out of {len(y_train)} samples\")\n",
    "    \n",
    "    revenue_regressor = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        metric=\"rmse\",\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=127,\n",
    "        max_depth=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        min_child_samples=20,\n",
    "        verbosity=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    revenue_regressor.fit(X_train_buyers, y_train_buyers_log)\n",
    "    print(\"‚úì Revenue regressor trained\")\n",
    "    \n",
    "    # Step 3: Combined predictions\n",
    "    valid_revenue_pred_log = revenue_regressor.predict(X_valid_prep)\n",
    "    valid_revenue_pred = np.expm1(valid_revenue_pred_log)\n",
    "    valid_revenue_pred = np.clip(valid_revenue_pred, 0, None)\n",
    "    \n",
    "    # Final prediction: P(buyer) * E[revenue | buyer]\n",
    "    valid_pred = buyer_prob_valid * valid_revenue_pred\n",
    "    \n",
    "    # Store for submission\n",
    "    models = {\n",
    "        \"classifier\": buyer_classifier,\n",
    "        \"regressor\": revenue_regressor\n",
    "    }\n",
    "    \n",
    "    # Additional metrics\n",
    "    buyer_acc = (buyer_classifier.predict(X_valid_prep) == y_valid_buyer).mean()\n",
    "    print(f\"\\nBuyer classifier accuracy: {buyer_acc:.4f}\")\n",
    "    print(f\"Average predicted buyer probability: {buyer_prob_valid.mean():.4f}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid MODEL_VERSION: {MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c1f0d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fafd9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION RESULTS\n",
      "==================================================\n",
      "Model: SINGLE\n",
      "MSLE: 0.204424\n",
      "Baseline (all zeros): 0.228072\n",
      "Improvement: 10.37%\n",
      "\n",
      "Prediction stats:\n",
      "  Mean: 0.0960\n",
      "  Median: 0.0084\n",
      "  Max: 65.5665\n",
      "  % Non-zero: 69.90%\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSLE\n",
    "msle_model = mean_squared_log_error(y_valid, valid_pred)\n",
    "msle_baseline = mean_squared_log_error(y_valid, np.zeros_like(y_valid))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_VERSION.upper()}\")\n",
    "print(f\"MSLE: {msle_model:.6f}\")\n",
    "print(f\"Baseline (all zeros): {msle_baseline:.6f}\")\n",
    "print(f\"Improvement: {((msle_baseline - msle_model) / msle_baseline * 100):.2f}%\")\n",
    "\n",
    "# Distribution stats\n",
    "print(f\"\\nPrediction stats:\")\n",
    "print(f\"  Mean: {valid_pred.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(valid_pred):.4f}\")\n",
    "print(f\"  Max: {valid_pred.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(valid_pred > 0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddbd426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRID SEARCH - SINGLE LGBM REGRESSOR\n",
      "============================================================\n",
      "\n",
      "Total combinations: 729\n",
      "Estimated time: ~1458 minutes (aprox 2 min/model)\n",
      "\n",
      "[1/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.202018 | Time: 2.5s\n",
      "\n",
      "[2/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.202734 | Time: 2.3s\n",
      "\n",
      "[3/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.7, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.203288 | Time: 2.5s\n",
      "\n",
      "[4/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.202018 | Time: 2.4s\n",
      "\n",
      "[5/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.202734 | Time: 2.2s\n",
      "\n",
      "[6/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.203288 | Time: 2.2s\n",
      "\n",
      "[7/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.202018 | Time: 2.3s\n",
      "\n",
      "[8/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.8}\n",
      "  MSLE: 0.202734 | Time: 2.1s\n",
      "\n",
      "[9/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': -1, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "  MSLE: 0.203288 | Time: 2.3s\n",
      "\n",
      "[10/10] Testing: {'n_estimators': 400, 'learning_rate': 0.03, 'num_leaves': 127, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "  MSLE: 0.201730 | Time: 2.2s\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP 5 BEST MODELS\n",
      "============================================================\n",
      " n_estimators  learning_rate  num_leaves  max_depth  subsample  colsample_bytree     msle  train_time\n",
      "          400           0.03         127         10        0.7               0.7 0.201730    2.163592\n",
      "          400           0.03         127         -1        0.7               0.7 0.202018    2.541156\n",
      "          400           0.03         127         -1        0.9               0.7 0.202018    2.338029\n",
      "          400           0.03         127         -1        0.8               0.7 0.202018    2.432946\n",
      "          400           0.03         127         -1        0.9               0.8 0.202734    2.149901\n",
      "\n",
      "============================================================\n",
      "BEST MODEL PARAMETERS\n",
      "============================================================\n",
      "  n_estimators: 400.0\n",
      "  learning_rate: 0.03\n",
      "  num_leaves: 127.0\n",
      "  max_depth: 10.0\n",
      "  subsample: 0.7\n",
      "  colsample_bytree: 0.7\n",
      "  msle: 0.20172958342563213\n",
      "  train_time: 2.163592\n",
      "\n",
      "============================================================\n",
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "============================================================\n",
      "\n",
      "‚úì Final Model MSLE: 0.201730\n",
      "  Baseline MSLE: 0.228072\n",
      "  Improvement: 11.55%\n",
      "\n",
      "‚úì Results saved to: grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "## Grid Search for Single Model\n",
    "\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "# Verificar que estamos en modo single\n",
    "if MODEL_VERSION != \"single\":\n",
    "    print(\"‚ö†Ô∏è Grid search only works with MODEL_VERSION='single'\")\n",
    "    print(\"Please change MODEL_VERSION in the first cell and re-run from the beginning\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GRID SEARCH - SINGLE LGBM REGRESSOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [400, 600, 800],\n",
    "        'learning_rate': [0.03, 0.05, 0.07],\n",
    "        'num_leaves': [127, 255, 511],\n",
    "        'max_depth': [-1, 10, 15],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    combinations = list(itertools.product(*values))\n",
    "    \n",
    "    print(f\"\\nTotal combinations: {len(combinations)}\")\n",
    "    print(f\"Estimated time: ~{len(combinations) * 2} minutes (aprox 2 min/model)\\n\")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Transform target once\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    \n",
    "    # Grid search\n",
    "    for i, params in enumerate(combinations[:10], 1):  # Limit to first 10 for testing\n",
    "        param_dict = dict(zip(keys, params))\n",
    "        \n",
    "        # üî• FIX: Convertir par√°metros int expl√≠citamente\n",
    "        param_dict['n_estimators'] = int(param_dict['n_estimators'])\n",
    "        param_dict['num_leaves'] = int(param_dict['num_leaves'])\n",
    "        param_dict['max_depth'] = int(param_dict['max_depth'])\n",
    "        \n",
    "        print(f\"[{i}/{min(10, len(combinations))}] Testing: {param_dict}\")\n",
    "        \n",
    "        # Train model\n",
    "        model = LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=0.0,\n",
    "            verbosity=-1,\n",
    "            random_state=42,\n",
    "            **param_dict\n",
    "        )\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        model.fit(X_train_prep, y_train_log)\n",
    "        train_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Predict\n",
    "        valid_pred_log = model.predict(X_valid_prep)\n",
    "        valid_pred = np.expm1(valid_pred_log)\n",
    "        valid_pred = np.clip(valid_pred, 0, None)\n",
    "        \n",
    "        # Evaluate\n",
    "        msle = mean_squared_log_error(y_valid, valid_pred)\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            **param_dict,\n",
    "            'msle': msle,\n",
    "            'train_time': train_time\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  MSLE: {msle:.6f} | Time: {train_time:.1f}s\\n\")\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('msle')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 5 BEST MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results_df.head(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BEST MODEL PARAMETERS\")\n",
    "    print(\"=\" * 60)\n",
    "    best_params = results_df.iloc[0].to_dict()\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Train final model with best params\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    best_model_params = {k: v for k, v in best_params.items() \n",
    "                         if k not in ['msle', 'train_time']}\n",
    "    \n",
    "    # üî• FIX: Asegurar que los par√°metros finales tambi√©n son int\n",
    "    best_model_params['n_estimators'] = int(best_model_params['n_estimators'])\n",
    "    best_model_params['num_leaves'] = int(best_model_params['num_leaves'])\n",
    "    best_model_params['max_depth'] = int(best_model_params['max_depth'])\n",
    "    \n",
    "    final_model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        verbosity=-1,\n",
    "        random_state=42,\n",
    "        **best_model_params\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_prep, y_train_log)\n",
    "    \n",
    "    # Final predictions\n",
    "    valid_pred_log = final_model.predict(X_valid_prep)\n",
    "    valid_pred = np.expm1(valid_pred_log)\n",
    "    valid_pred = np.clip(valid_pred, 0, None)\n",
    "    \n",
    "    msle_final = mean_squared_log_error(y_valid, valid_pred)\n",
    "    msle_baseline = mean_squared_log_error(y_valid, np.zeros_like(y_valid))\n",
    "    \n",
    "    print(f\"\\n‚úì Final Model MSLE: {msle_final:.6f}\")\n",
    "    print(f\"  Baseline MSLE: {msle_baseline:.6f}\")\n",
    "    print(f\"  Improvement: {((msle_baseline - msle_final) / msle_baseline * 100):.2f}%\")\n",
    "    \n",
    "    # Update models dict for submission\n",
    "    models = {\"regressor\": final_model}\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"grid_search_results.csv\", index=False)\n",
    "    print(f\"\\n‚úì Results saved to: grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ace372",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Processing 96 test chunks...\n",
      "  Chunk 10/96...\n",
      "  Chunk 20/96...\n",
      "  Chunk 30/96...\n",
      "  Chunk 40/96...\n",
      "  Chunk 50/96...\n",
      "  Chunk 60/96...\n",
      "  Chunk 70/96...\n",
      "  Chunk 80/96...\n",
      "  Chunk 90/96...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating test predictions...\")\n",
    "\n",
    "dd_test = dd.read_parquet(TEST_PATH, engine='pyarrow')\n",
    "existing_big_cols_test = [c for c in IGNORE_BIG_COLS if c in dd_test.columns]\n",
    "dd_test = dd_test.drop(columns=existing_big_cols_test)\n",
    "\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "print(f\"Processing {len(delayed_parts)} test chunks...\")\n",
    "\n",
    "pred_dfs = []\n",
    "feature_cols_final = X_train_prep.columns.tolist()\n",
    "\n",
    "for i, d in enumerate(delayed_parts):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Chunk {i+1}/{len(delayed_parts)}...\")\n",
    "    \n",
    "    part_df = d.compute()\n",
    "    part_df = reduce_memory(part_df)\n",
    "    \n",
    "    row_ids = part_df[\"row_id\"].values\n",
    "    \n",
    "    # üî• IMPORTANTE: Solo seleccionar columnas que existen en test\n",
    "    available_cols = [c for c in feature_cols if c in part_df.columns]\n",
    "    X_part = part_df[available_cols].copy()\n",
    "    \n",
    "    # üî• A√±adir columnas faltantes con valores por defecto\n",
    "    for col in feature_cols:\n",
    "        if col not in X_part.columns:\n",
    "            if col in num_cols:\n",
    "                X_part[col] = 0  # Numeric ‚Üí 0\n",
    "            else:\n",
    "                X_part[col] = \"MISSING\"  # Categorical ‚Üí MISSING\n",
    "    \n",
    "    # Reordenar columnas para que coincidan con el train original\n",
    "    X_part = X_part[feature_cols]\n",
    "    \n",
    "    # Aplicar el mismo preprocessing (con frequency encoding, feature engineering, etc.)\n",
    "    X_part_prep = preprocess_new(X_part, num_cols, cat_cols, X_train_prep)\n",
    "    \n",
    "    # Asegurar que tiene exactamente las mismas columnas que train\n",
    "    missing_in_test = set(feature_cols_final) - set(X_part_prep.columns)\n",
    "    if missing_in_test:\n",
    "        for col in missing_in_test:\n",
    "            if col in final_num_cols:\n",
    "                X_part_prep[col] = 0\n",
    "            else:\n",
    "                X_part_prep[col] = \"MISSING\"\n",
    "    \n",
    "    # Reordenar para match exacto\n",
    "    X_part_prep = X_part_prep[feature_cols_final]\n",
    "    \n",
    "    # Predict based on model version\n",
    "    if MODEL_VERSION == \"single\":\n",
    "        part_pred_log = models[\"regressor\"].predict(X_part_prep)\n",
    "        part_pred = np.expm1(part_pred_log)\n",
    "        part_pred = np.clip(part_pred, 0, None)\n",
    "        \n",
    "    elif MODEL_VERSION == \"two_step\":\n",
    "        buyer_prob = models[\"classifier\"].predict_proba(X_part_prep)[:, 1]\n",
    "        revenue_pred_log = models[\"regressor\"].predict(X_part_prep)\n",
    "        revenue_pred = np.expm1(revenue_pred_log)\n",
    "        revenue_pred = np.clip(revenue_pred, 0, None)\n",
    "        part_pred = buyer_prob * revenue_pred\n",
    "    \n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"iap_revenue_d7\": part_pred\n",
    "    }))\n",
    "    \n",
    "    del part_df, X_part, X_part_prep, row_ids, part_pred\n",
    "    gc.collect()\n",
    "\n",
    "# Combine and save\n",
    "submission = pd.concat(pred_dfs, ignore_index=True)\n",
    "output_file = f\"submission_{MODEL_VERSION}.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Submission saved: {output_file}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Sample:\\n{submission.head()}\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"\\nValidation checks:\")\n",
    "print(f\"  NaN values: {submission.isna().sum().sum()}\")\n",
    "print(f\"  Negative values: {(submission['iap_revenue_d7'] < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8486e1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**To switch models**: Change `MODEL_VERSION` in the first code cell to:\n",
    "- `\"single\"` - Single LightGBM regressor (direct approach)\n",
    "- `\"two_step\"` - Classifier + Regressor (probabilistic approach)\n",
    "\n",
    "**To adjust data size**: Change `TRAIN_SAMPLE_FRAC` (default 0.10 = 10%)\n",
    "\n",
    "Then re-run all cells!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
