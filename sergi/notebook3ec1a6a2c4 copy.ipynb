{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:25:06.525606Z",
          "iopub.status.busy": "2025-11-15T14:25:06.525188Z",
          "iopub.status.idle": "2025-11-15T14:25:08.864745Z",
          "shell.execute_reply": "2025-11-15T14:25:08.863775Z",
          "shell.execute_reply.started": "2025-11-15T14:25:06.525576Z"
        },
        "id": "vtZioGhSGRpE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import dask\n",
        "import dask.dataframe as dd\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "dask.config.set({\"dataframe.convert-string\": False})\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import gc\n",
        "\n",
        "TRAIN_PATH = \"./train/train\"\n",
        "TEST_PATH  = \"./test/test\"\n",
        "\n",
        "TARGET_COL = \"iap_revenue_d7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:25:08.866067Z",
          "iopub.status.busy": "2025-11-15T14:25:08.865594Z",
          "iopub.status.idle": "2025-11-15T14:25:08.87851Z",
          "shell.execute_reply": "2025-11-15T14:25:08.87755Z",
          "shell.execute_reply.started": "2025-11-15T14:25:08.866047Z"
        },
        "id": "yxYY_yRtGRpF",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Columnas monstruosas que casi seguro van con listas/mapas/histogramas.\n",
        "# Las quitamos del baseline para RAM y simplicidad.\n",
        "ignore_big_cols = [\n",
        "    \"bundles_ins\",\n",
        "    \"user_bundles\",\n",
        "    \"user_bundles_l28d\",\n",
        "    \"city_hist\",\n",
        "    \"country_hist\",\n",
        "    \"region_hist\",\n",
        "    \"dev_language_hist\",\n",
        "    \"dev_osv_hist\",\n",
        "    \"bcat\",\n",
        "    \"bcat_bottom_taxonomy\",\n",
        "    \"bundles_cat\",\n",
        "    \"bundles_cat_bottom_taxonomy\",\n",
        "    \"first_request_ts_bundle\",\n",
        "    \"first_request_ts_category_bottom_taxonomy\",\n",
        "    \"last_buy_ts_bundle\",\n",
        "    \"last_buy_ts_category\",\n",
        "    \"last_install_ts_bundle\",\n",
        "    \"last_install_ts_category\",\n",
        "    \"advertiser_actions_action_count\",\n",
        "    \"advertiser_actions_action_last_timestamp\",\n",
        "    \"user_actions_bundles_action_count\",\n",
        "    \"user_actions_bundles_action_last_timestamp\",\n",
        "    \"new_bundles\",\n",
        "    \"whale_users_bundle_num_buys_prank\",\n",
        "    \"whale_users_bundle_revenue_prank\",\n",
        "    \"whale_users_bundle_total_num_buys\",\n",
        "    \"whale_users_bundle_total_revenue\",\n",
        "]\n",
        "\n",
        "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type == \"float64\":\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "        elif col_type == \"int64\":\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "    return df\n",
        "\n",
        "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
        "    \"\"\"Detecta columnas que contienen listas o dicts.\"\"\"\n",
        "    if cols is None:\n",
        "        cols = df.columns\n",
        "    listlike = []\n",
        "    for c in cols:\n",
        "        sample_vals = df[c].head(100)\n",
        "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
        "            listlike.append(c)\n",
        "    return listlike\n",
        "\n",
        "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
        "    \"\"\"Preprocesado para train/valid.\"\"\"\n",
        "    X_train = X_train.copy()\n",
        "    X_valid = X_valid.copy()\n",
        "\n",
        "    # Numéricas: NaN -> 0\n",
        "    for c in num_cols:\n",
        "        X_train[c] = X_train[c].fillna(0)\n",
        "        X_valid[c] = X_valid[c].fillna(0)\n",
        "\n",
        "    # Categóricas: strings + categorías fijas basadas en train\n",
        "    for c in cat_cols:\n",
        "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_train[c] = X_train[c].astype(\"category\")\n",
        "\n",
        "        cats = X_train[c].cat.categories\n",
        "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_valid[c] = X_valid[c].astype(\n",
        "            pd.api.types.CategoricalDtype(categories=cats)\n",
        "        )\n",
        "\n",
        "    return X_train, X_valid\n",
        "\n",
        "def preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n",
        "    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n",
        "    X_new = X_new.copy()\n",
        "\n",
        "    for c in num_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].fillna(0)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "            cats = cat_ref_df[c].cat.categories\n",
        "            X_new[c] = X_new[c].astype(\n",
        "                pd.api.types.CategoricalDtype(categories=cats)\n",
        "            )\n",
        "\n",
        "    return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:25:08.87978Z",
          "iopub.status.busy": "2025-11-15T14:25:08.879459Z",
          "iopub.status.idle": "2025-11-15T14:31:01.312169Z",
          "shell.execute_reply": "2025-11-15T14:31:01.311301Z",
          "shell.execute_reply.started": "2025-11-15T14:25:08.87975Z"
        },
        "id": "-Li_-OkSGRpH",
        "outputId": "90641ad0-2ada-4096-a14c-0fdebe1b1d04",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train: 1–5 de octubre\n",
        "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
        "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
        "\n",
        "# Valid: día 6 de octubre\n",
        "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
        "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
        "\n",
        "dd_train = dd.read_parquet(TRAIN_PATH, filters=filters_train)\n",
        "dd_valid = dd.read_parquet(TRAIN_PATH, filters=filters_valid)\n",
        "\n",
        "# Quitar las columnas monstruosas si existen\n",
        "existing_big_cols_train = [c for c in ignore_big_cols if c in dd_train.columns]\n",
        "existing_big_cols_valid = [c for c in ignore_big_cols if c in dd_valid.columns]\n",
        "\n",
        "dd_train = dd_train.drop(columns=existing_big_cols_train)\n",
        "dd_valid = dd_valid.drop(columns=existing_big_cols_valid)\n",
        "\n",
        "# Muestreo de train: AJUSTA ESTO si quieres más datos\n",
        "frac_train = 0.01  # 10% de train; puedes subir a 0.2 si ves que va bien\n",
        "\n",
        "train_sample = dd_train.sample(frac=frac_train, random_state=42).compute()\n",
        "valid_df     = dd_valid.compute()\n",
        "\n",
        "train_sample = reduce_memory(train_sample)\n",
        "valid_df     = reduce_memory(valid_df)\n",
        "\n",
        "print(\"Train sample shape:\", train_sample.shape)\n",
        "print(\"Valid shape:\", valid_df.shape)\n",
        "print(\"Train memory (GB):\", train_sample.memory_usage(deep=True).sum() / (1024**3))\n",
        "print(\"Valid memory (GB):\", valid_df.memory_usage(deep=True).sum() / (1024**3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:31:01.314349Z",
          "iopub.status.busy": "2025-11-15T14:31:01.314071Z",
          "iopub.status.idle": "2025-11-15T14:31:21.755462Z",
          "shell.execute_reply": "2025-11-15T14:31:21.754632Z",
          "shell.execute_reply.started": "2025-11-15T14:31:01.314327Z"
        },
        "id": "pNaidivyGRpK",
        "outputId": "906e4395-6204-4ce7-d4cc-224c7ebb87c6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de features: 39\n",
            "Columnas con listas/dicts: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rev_by_adv', 'rwd_prank']\n",
            "Numéricas: 10\n",
            "Categóricas: 14\n",
            "X_train_prep shape: (1729408, 24)\n",
            "X_valid_prep shape: (3306478, 24)\n"
          ]
        }
      ],
      "source": [
        "# Todas las labels auxiliares que SOLO están en train\n",
        "LABEL_COLS = [\n",
        "    \"buyer_d1\",\n",
        "    \"buyer_d7\",\n",
        "    \"buyer_d14\",\n",
        "    \"buyer_d28\",\n",
        "    \"buy_d7\",\n",
        "    \"buy_d14\",\n",
        "    \"buy_d28\",\n",
        "    \"iap_revenue_d7\",   # target principal\n",
        "    \"iap_revenue_d14\",\n",
        "    \"iap_revenue_d28\",\n",
        "    \"registration\",\n",
        "    \"retention_d1_to_d7\",\n",
        "    \"retention_d3_to_d7\",\n",
        "    \"retention_d7_to_d14\",\n",
        "    \"retention_d1\",\n",
        "    \"retention_d3\",\n",
        "    \"retentiond7\",\n",
        "]\n",
        "\n",
        "TARGET_COL = \"iap_revenue_d7\"\n",
        "\n",
        "assert TARGET_COL in train_sample.columns, \"No está iap_revenue_d7 en train\"\n",
        "\n",
        "# y_train / y_valid: solo la target principal\n",
        "y_train = train_sample[TARGET_COL].values\n",
        "y_valid = valid_df[TARGET_COL].values\n",
        "\n",
        "# Columnas que NO queremos como features\n",
        "cols_to_drop_from_X = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
        "\n",
        "# Features = todas las demás\n",
        "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop_from_X]\n",
        "\n",
        "print(\"Número de features:\", len(feature_cols))\n",
        "\n",
        "X_train = train_sample[feature_cols].copy()\n",
        "X_valid = valid_df[feature_cols].copy()\n",
        "\n",
        "# 1) Detectar columnas con listas/dicts y quitarlas\n",
        "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
        "print(\"Columnas con listas/dicts:\", listlike_cols)\n",
        "\n",
        "X_train = X_train.drop(columns=listlike_cols)\n",
        "X_valid = X_valid.drop(columns=listlike_cols)\n",
        "\n",
        "# 2) Volver a calcular numéricas y categóricas\n",
        "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
        "\n",
        "print(\"Numéricas:\", len(num_cols))\n",
        "print(\"Categóricas:\", len(cat_cols))\n",
        "\n",
        "# 3) Preprocesar\n",
        "X_train_prep, X_valid_prep = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n",
        "\n",
        "print(\"X_train_prep shape:\", X_train_prep.shape)\n",
        "print(\"X_valid_prep shape:\", X_valid_prep.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:31:21.756339Z",
          "iopub.status.busy": "2025-11-15T14:31:21.75612Z",
          "iopub.status.idle": "2025-11-15T14:32:42.109623Z",
          "shell.execute_reply": "2025-11-15T14:32:42.108668Z",
          "shell.execute_reply.started": "2025-11-15T14:31:21.756321Z"
        },
        "id": "bFGe1otoGRpM",
        "outputId": "3f485afe-c7d0-4a28-8173-05c861c019bc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo entrenado.\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_valid_log = np.log1p(y_valid)\n",
        "\n",
        "model = LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=255,\n",
        "    max_depth=-1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=0.0,\n",
        "    verbosity=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train_prep, y_train_log)\n",
        "\n",
        "print(\"Modelo entrenado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:32:42.111252Z",
          "iopub.status.busy": "2025-11-15T14:32:42.110563Z",
          "iopub.status.idle": "2025-11-15T14:37:03.5407Z",
          "shell.execute_reply": "2025-11-15T14:37:03.53967Z",
          "shell.execute_reply.started": "2025-11-15T14:32:42.111201Z"
        },
        "id": "En06DHuxGRpN",
        "outputId": "e844c3e8-e72f-4627-e800-7f3f2ab5b9c2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSLE modelo: 0.17942944557952253\n",
            "MSLE baseline (todo 0): 0.21498893\n"
          ]
        }
      ],
      "source": [
        "# Predicción en espacio log\n",
        "valid_pred_log = model.predict(X_valid_prep)\n",
        "\n",
        "# Volver al espacio original\n",
        "valid_pred = np.expm1(valid_pred_log)\n",
        "valid_pred = np.clip(valid_pred, 0, None)\n",
        "\n",
        "msle_model = mean_squared_log_error(y_valid, valid_pred)\n",
        "print(\"MSLE modelo:\", msle_model)\n",
        "\n",
        "zeros_pred = np.zeros_like(y_valid)\n",
        "msle_zeros = mean_squared_log_error(y_valid, zeros_pred)\n",
        "print(\"MSLE baseline (todo 0):\", msle_zeros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:37:03.542065Z",
          "iopub.status.busy": "2025-11-15T14:37:03.541748Z",
          "iopub.status.idle": "2025-11-15T14:37:13.366563Z",
          "shell.execute_reply": "2025-11-15T14:37:13.365689Z",
          "shell.execute_reply.started": "2025-11-15T14:37:03.542031Z"
        },
        "id": "c4gX7fyvGRpP",
        "outputId": "13140b1f-fe56-4921-a3cd-a5b89d93e975",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conservamos SOLO lo imprescindible para el test: model, X_train_prep, num_cols, cat_cols\n",
        "to_keep = {\"model\", \"X_train_prep\", \"num_cols\", \"cat_cols\"}\n",
        "\n",
        "for name in list(globals().keys()):\n",
        "    if name.startswith(\"_\"):\n",
        "        continue\n",
        "    if name in to_keep:\n",
        "        continue\n",
        "    # No borramos módulos (dask, pd, np, etc.)\n",
        "    if isinstance(globals()[name], type(os)):\n",
        "        continue\n",
        "    try:\n",
        "        del globals()[name]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:37:13.367734Z",
          "iopub.status.busy": "2025-11-15T14:37:13.367453Z",
          "iopub.status.idle": "2025-11-15T14:37:13.380493Z",
          "shell.execute_reply": "2025-11-15T14:37:13.379262Z",
          "shell.execute_reply.started": "2025-11-15T14:37:13.367709Z"
        },
        "id": "2fI9MGGqGRpQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Columnas monstruosas que casi seguro van con listas/mapas/histogramas.\n",
        "# Las quitamos del baseline para RAM y simplicidad.\n",
        "ignore_big_cols = [\n",
        "    \"bundles_ins\",\n",
        "    \"user_bundles\",\n",
        "    \"user_bundles_l28d\",\n",
        "    \"city_hist\",\n",
        "    \"country_hist\",\n",
        "    \"region_hist\",\n",
        "    \"dev_language_hist\",\n",
        "    \"dev_osv_hist\",\n",
        "    \"bcat\",\n",
        "    \"bcat_bottom_taxonomy\",\n",
        "    \"bundles_cat\",\n",
        "    \"bundles_cat_bottom_taxonomy\",\n",
        "    \"first_request_ts_bundle\",\n",
        "    \"first_request_ts_category_bottom_taxonomy\",\n",
        "    \"last_buy_ts_bundle\",\n",
        "    \"last_buy_ts_category\",\n",
        "    \"last_install_ts_bundle\",\n",
        "    \"last_install_ts_category\",\n",
        "    \"advertiser_actions_action_count\",\n",
        "    \"advertiser_actions_action_last_timestamp\",\n",
        "    \"user_actions_bundles_action_count\",\n",
        "    \"user_actions_bundles_action_last_timestamp\",\n",
        "    \"new_bundles\",\n",
        "    \"whale_users_bundle_num_buys_prank\",\n",
        "    \"whale_users_bundle_revenue_prank\",\n",
        "    \"whale_users_bundle_total_num_buys\",\n",
        "    \"whale_users_bundle_total_revenue\",\n",
        "]\n",
        "\n",
        "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type == \"float64\":\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "        elif col_type == \"int64\":\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "    return df\n",
        "\n",
        "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
        "    \"\"\"Detecta columnas que contienen listas o dicts.\"\"\"\n",
        "    if cols is None:\n",
        "        cols = df.columns\n",
        "    listlike = []\n",
        "    for c in cols:\n",
        "        sample_vals = df[c].head(100)\n",
        "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
        "            listlike.append(c)\n",
        "    return listlike\n",
        "\n",
        "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
        "    \"\"\"Preprocesado para train/valid.\"\"\"\n",
        "    X_train = X_train.copy()\n",
        "    X_valid = X_valid.copy()\n",
        "\n",
        "    # Numéricas: NaN -> 0\n",
        "    for c in num_cols:\n",
        "        X_train[c] = X_train[c].fillna(0)\n",
        "        X_valid[c] = X_valid[c].fillna(0)\n",
        "\n",
        "    # Categóricas: strings + categorías fijas basadas en train\n",
        "    for c in cat_cols:\n",
        "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_train[c] = X_train[c].astype(\"category\")\n",
        "\n",
        "        cats = X_train[c].cat.categories\n",
        "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "        X_valid[c] = X_valid[c].astype(\n",
        "            pd.api.types.CategoricalDtype(categories=cats)\n",
        "        )\n",
        "\n",
        "    return X_train, X_valid\n",
        "\n",
        "def preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n",
        "    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n",
        "    X_new = X_new.copy()\n",
        "\n",
        "    for c in num_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].fillna(0)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if c in X_new.columns:\n",
        "            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
        "            cats = cat_ref_df[c].cat.categories\n",
        "            X_new[c] = X_new[c].astype(\n",
        "                pd.api.types.CategoricalDtype(categories=cats)\n",
        "            )\n",
        "\n",
        "    return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T14:38:17.340976Z",
          "iopub.status.busy": "2025-11-15T14:38:17.340651Z",
          "iopub.status.idle": "2025-11-15T14:59:01.835885Z",
          "shell.execute_reply": "2025-11-15T14:59:01.835011Z",
          "shell.execute_reply.started": "2025-11-15T14:38:17.340952Z"
        },
        "id": "jpX7nzdvGRpR",
        "outputId": "e9137be5-36e2-4e0f-da24-3bcc9499426a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de chunks de test: 96\n",
            "Procesando chunk 1/96...\n",
            "Procesando chunk 2/96...\n",
            "Procesando chunk 3/96...\n",
            "Procesando chunk 4/96...\n",
            "Procesando chunk 5/96...\n",
            "Procesando chunk 6/96...\n",
            "Procesando chunk 7/96...\n",
            "Procesando chunk 8/96...\n",
            "Procesando chunk 9/96...\n",
            "Procesando chunk 10/96...\n",
            "Procesando chunk 11/96...\n",
            "Procesando chunk 12/96...\n",
            "Procesando chunk 13/96...\n",
            "Procesando chunk 14/96...\n",
            "Procesando chunk 15/96...\n",
            "Procesando chunk 16/96...\n",
            "Procesando chunk 17/96...\n",
            "Procesando chunk 18/96...\n",
            "Procesando chunk 19/96...\n",
            "Procesando chunk 20/96...\n",
            "Procesando chunk 21/96...\n",
            "Procesando chunk 22/96...\n",
            "Procesando chunk 23/96...\n",
            "Procesando chunk 24/96...\n",
            "Procesando chunk 25/96...\n",
            "Procesando chunk 26/96...\n",
            "Procesando chunk 27/96...\n",
            "Procesando chunk 28/96...\n",
            "Procesando chunk 29/96...\n",
            "Procesando chunk 30/96...\n",
            "Procesando chunk 31/96...\n",
            "Procesando chunk 32/96...\n",
            "Procesando chunk 33/96...\n",
            "Procesando chunk 34/96...\n",
            "Procesando chunk 35/96...\n",
            "Procesando chunk 36/96...\n",
            "Procesando chunk 37/96...\n",
            "Procesando chunk 38/96...\n",
            "Procesando chunk 39/96...\n",
            "Procesando chunk 40/96...\n",
            "Procesando chunk 41/96...\n",
            "Procesando chunk 42/96...\n",
            "Procesando chunk 43/96...\n",
            "Procesando chunk 44/96...\n",
            "Procesando chunk 45/96...\n",
            "Procesando chunk 46/96...\n",
            "Procesando chunk 47/96...\n",
            "Procesando chunk 48/96...\n",
            "Procesando chunk 49/96...\n",
            "Procesando chunk 50/96...\n",
            "Procesando chunk 51/96...\n",
            "Procesando chunk 52/96...\n",
            "Procesando chunk 53/96...\n",
            "Procesando chunk 54/96...\n",
            "Procesando chunk 55/96...\n",
            "Procesando chunk 56/96...\n",
            "Procesando chunk 57/96...\n",
            "Procesando chunk 58/96...\n",
            "Procesando chunk 59/96...\n",
            "Procesando chunk 60/96...\n",
            "Procesando chunk 61/96...\n",
            "Procesando chunk 62/96...\n",
            "Procesando chunk 63/96...\n",
            "Procesando chunk 64/96...\n",
            "Procesando chunk 65/96...\n",
            "Procesando chunk 66/96...\n",
            "Procesando chunk 67/96...\n",
            "Procesando chunk 68/96...\n",
            "Procesando chunk 69/96...\n",
            "Procesando chunk 70/96...\n",
            "Procesando chunk 71/96...\n",
            "Procesando chunk 72/96...\n",
            "Procesando chunk 73/96...\n",
            "Procesando chunk 74/96...\n",
            "Procesando chunk 75/96...\n",
            "Procesando chunk 76/96...\n",
            "Procesando chunk 77/96...\n",
            "Procesando chunk 78/96...\n",
            "Procesando chunk 79/96...\n",
            "Procesando chunk 80/96...\n",
            "Procesando chunk 81/96...\n",
            "Procesando chunk 82/96...\n",
            "Procesando chunk 83/96...\n",
            "Procesando chunk 84/96...\n",
            "Procesando chunk 85/96...\n",
            "Procesando chunk 86/96...\n",
            "Procesando chunk 87/96...\n",
            "Procesando chunk 88/96...\n",
            "Procesando chunk 89/96...\n",
            "Procesando chunk 90/96...\n",
            "Procesando chunk 91/96...\n",
            "Procesando chunk 92/96...\n",
            "Procesando chunk 93/96...\n",
            "Procesando chunk 94/96...\n",
            "Procesando chunk 95/96...\n",
            "Procesando chunk 96/96...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>iap_revenue_d7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>e2f514a9-d922-4a17-bf94-f228bf4cd82f</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4bfc70d3-d619-410a-9683-4cd759f30f32</td>\n",
              "      <td>0.059406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ad433b66-b41e-4157-a6fd-24cd30701f6a</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5ed964d6-ddce-42e8-9fad-276eb7f64c2f</td>\n",
              "      <td>0.007885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81b73a45-c395-4d08-a4a3-513873440db3</td>\n",
              "      <td>0.000545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 row_id  iap_revenue_d7\n",
              "0  e2f514a9-d922-4a17-bf94-f228bf4cd82f        0.000000\n",
              "1  4bfc70d3-d619-410a-9683-4cd759f30f32        0.059406\n",
              "2  ad433b66-b41e-4157-a6fd-24cd30701f6a        0.000000\n",
              "3  5ed964d6-ddce-42e8-9fad-276eb7f64c2f        0.007885\n",
              "4  81b73a45-c395-4d08-a4a3-513873440db3        0.000545"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "TEST_PATH = \"/kaggle/input/smadex-challenge-predict-the-revenue/test/test\"\n",
        "\n",
        "dd_test = dd.read_parquet(TEST_PATH)\n",
        "existing_big_cols_test = [c for c in ignore_big_cols if c in dd_test.columns]\n",
        "dd_test = dd_test.drop(columns=existing_big_cols_test)\n",
        "\n",
        "delayed_parts = dd_test.to_delayed()\n",
        "print(\"Número de chunks de test:\", len(delayed_parts))\n",
        "\n",
        "feature_cols = X_train_prep.columns.tolist()\n",
        "\n",
        "pred_dfs = []\n",
        "\n",
        "for i, d in enumerate(delayed_parts):\n",
        "    print(f\"Procesando chunk {i+1}/{len(delayed_parts)}...\")\n",
        "\n",
        "    part_df = d.compute()\n",
        "    part_df = reduce_memory(part_df)\n",
        "\n",
        "    row_ids = part_df[\"row_id\"].values\n",
        "    X_part = part_df[feature_cols].copy()   # ahora SÍ existen todas\n",
        "\n",
        "    X_part_prep = preprocess_new(X_part, num_cols, cat_cols, X_train_prep)\n",
        "\n",
        "    part_pred_log = model.predict(X_part_prep)\n",
        "    part_pred = np.expm1(part_pred_log)\n",
        "    part_pred = np.clip(part_pred, 0, None)\n",
        "\n",
        "    pred_dfs.append(pd.DataFrame({\n",
        "        \"row_id\": row_ids,\n",
        "        \"iap_revenue_d7\": part_pred\n",
        "    }))\n",
        "\n",
        "    del part_df, X_part, X_part_prep, row_ids, part_pred_log, part_pred\n",
        "    gc.collect()\n",
        "\n",
        "submission = pd.concat(pred_dfs, ignore_index=True)\n",
        "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T15:06:21.687176Z",
          "iopub.status.busy": "2025-11-15T15:06:21.686553Z",
          "iopub.status.idle": "2025-11-15T15:06:22.739133Z",
          "shell.execute_reply": "2025-11-15T15:06:22.73815Z",
          "shell.execute_reply.started": "2025-11-15T15:06:21.687137Z"
        },
        "id": "nOarGav4GRpS",
        "outputId": "445a2165-941a-4ea3-c9ce-1bbbfec43137",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 row_id  iap_revenue_d7\n",
            "0  e2f514a9-d922-4a17-bf94-f228bf4cd82f        0.000000\n",
            "1  4bfc70d3-d619-410a-9683-4cd759f30f32        0.059406\n",
            "2  ad433b66-b41e-4157-a6fd-24cd30701f6a        0.000000\n",
            "3  5ed964d6-ddce-42e8-9fad-276eb7f64c2f        0.007885\n",
            "4  81b73a45-c395-4d08-a4a3-513873440db3        0.000545\n",
            "(13188409, 2)\n",
            "row_id            0\n",
            "iap_revenue_d7    0\n",
            "dtype: int64\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(submission.head())\n",
        "print(submission.shape)\n",
        "\n",
        "print(submission.isna().sum())          # no debería haber NaNs\n",
        "print((submission['iap_revenue_d7'] < 0).sum())  # debería ser 0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook3ec1a6a2c4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 14453560,
          "sourceId": 120867,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
