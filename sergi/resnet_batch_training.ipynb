{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f943d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from glob import glob\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm\n",
    "from time import time\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c559bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/stargix/Desktop/hackathons/datathon/train/train'\n",
    "test_path = '/home/stargix/Desktop/hackathons/datathon/test/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8abd833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total archivos disponibles: 144\n",
      "Batch size: 7 archivos (5% por batch)\n",
      "N√∫mero de batches: 21\n"
     ]
    }
   ],
   "source": [
    "# Definir columnas necesarias\n",
    "required_columns = [\n",
    "    'row_id', 'datetime',\n",
    "    'buyer_d7', 'iap_revenue_d7',\n",
    "    'advertiser_bundle', 'advertiser_category', 'advertiser_subcategory', \n",
    "    'advertiser_bottom_taxonomy_level',\n",
    "    'country', 'region',\n",
    "    'dev_make', 'dev_model', 'dev_os', 'dev_osv',\n",
    "    'carrier',\n",
    "    'hour', 'weekday', 'weekend_ratio', 'hour_ratio',\n",
    "    'release_date', 'release_msrp',\n",
    "    'avg_act_days', 'avg_daily_sessions', 'avg_days_ins', 'avg_duration',\n",
    "    'weeks_since_first_seen', 'wifi_ratio',\n",
    "    'retentiond7',\n",
    "    'city_hist', 'country_hist', 'region_hist', 'dev_language_hist', 'dev_osv_hist',\n",
    "    'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk',\n",
    "    'iap_revenue_usd_bundle', 'iap_revenue_usd_category',\n",
    "    'num_buys_bundle', 'num_buys_category',\n",
    "    'last_buy', 'last_ins',\n",
    "    'bcat', 'bcat_bottom_taxonomy',\n",
    "    'bundles_cat', 'bundles_cat_bottom_taxonomy', \n",
    "    'bundles_ins',\n",
    "    'new_bundles', 'user_bundles', 'user_bundles_l28d',\n",
    "    'advertiser_actions_action_count', 'advertiser_actions_action_last_timestamp',\n",
    "    'user_actions_bundles_action_count', 'user_actions_bundles_action_last_timestamp',\n",
    "    'last_advertiser_action',\n",
    "    'first_request_ts', 'first_request_ts_bundle', \n",
    "    'first_request_ts_category_bottom_taxonomy',\n",
    "    'rwd_prank',\n",
    "    'whale_users_bundle_num_buys_prank', 'whale_users_bundle_revenue_prank'\n",
    "]\n",
    "\n",
    "# Obtener todos los archivos parquet\n",
    "parquet_files_train = glob(os.path.join(train_path, '**/part-*.parquet'), recursive=True)\n",
    "print(f\"Total archivos disponibles: {len(parquet_files_train)}\")\n",
    "\n",
    "# üî• Reducir a 5% para evitar crash de memoria\n",
    "batch_size = max(1, int(len(parquet_files_train) * 0.05))\n",
    "num_batches = (len(parquet_files_train) + batch_size - 1) // batch_size\n",
    "print(f\"Batch size: {batch_size} archivos (5% por batch)\")\n",
    "print(f\"N√∫mero de batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcad7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funciones de preprocesamiento definidas\n"
     ]
    }
   ],
   "source": [
    "# Funciones de preprocesamiento\n",
    "import ast\n",
    "\n",
    "columns_to_sum = [\n",
    "    'iap_revenue_usd_bundle',\n",
    "    'num_buys_bundle',\n",
    "    'rwd_prank',\n",
    "    'whale_users_bundle_num_buys_prank',\n",
    "    'whale_users_bundle_revenue_prank'\n",
    "]\n",
    "\n",
    "def sum_values(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return 0\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            x = ast.literal_eval(x)\n",
    "        if isinstance(x, list) and len(x) > 0:\n",
    "            total = sum([item[1] for item in x if isinstance(item, tuple) and len(item) > 1])\n",
    "            return total\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def preprocess_dataframe(df, label_encoders=None, fit_encoders=False):\n",
    "    \"\"\"Preprocesa DataFrame de forma eficiente en memoria\"\"\"\n",
    "    # Sumar columnas con listas\n",
    "    for col in columns_to_sum:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(sum_values)\n",
    "    \n",
    "    # Label encoding\n",
    "    cat_features = [\n",
    "        'advertiser_bundle', 'advertiser_category', 'advertiser_subcategory',\n",
    "        'country', 'region', 'dev_make', 'dev_model', 'dev_os', 'dev_osv'\n",
    "    ]\n",
    "    cat_features = [c for c in cat_features if c in df.columns]\n",
    "    \n",
    "    if fit_encoders:\n",
    "        label_encoders = {}\n",
    "        for col in cat_features:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str).fillna(\"__NA__\"))\n",
    "            label_encoders[col] = le\n",
    "    else:\n",
    "        for col in cat_features:\n",
    "            if col in label_encoders:\n",
    "                df[col] = df[col].astype(str).fillna(\"__NA__\")\n",
    "                df[col] = df[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
    "    \n",
    "    # üî• Optimizaci√≥n: Convertir a tipos m√°s eficientes\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        if df[col].max() < 2147483647 and df[col].min() > -2147483648:\n",
    "            df[col] = df[col].astype('int32')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    \n",
    "    return df, label_encoders, cat_features\n",
    "\n",
    "print(\"‚úì Funciones de preprocesamiento definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5e3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Arquitectura ResNet definida\n"
     ]
    }
   ],
   "source": [
    "# ARQUITECTURA RESNET 1D\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.block(x))\n",
    "\n",
    "class RevenueResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_blocks=4, dropout=0.2):\n",
    "        super(RevenueResNet, self).__init__()\n",
    "        \n",
    "        # Projection inicial\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            ResidualBlock(hidden_dim, dropout=dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Head de regresi√≥n\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"‚úì Arquitectura ResNet definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfdf3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CARGANDO PRIMER BATCH PARA CONFIGURACI√ìN\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Primer batch cargado: (1026857, 62)\n",
      "‚úì Features definidas: 24 (15 num√©ricas + 9 categ√≥ricas)\n",
      "‚úì StandardScaler fiteado con 1026857 muestras\n",
      "‚úì Artifacts guardados\n",
      "‚úì Features definidas: 24 (15 num√©ricas + 9 categ√≥ricas)\n",
      "‚úì StandardScaler fiteado con 1026857 muestras\n",
      "‚úì Artifacts guardados\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar primer batch para configuraci√≥n\n",
    "\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CARGANDO PRIMER BATCH PARA CONFIGURACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "first_batch_files = parquet_files_train[:batch_size]\n",
    "try:\n",
    "    first_ddf = dd.read_parquet(first_batch_files, engine='pyarrow', columns=required_columns)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Cargando todas las columnas: {e}\")\n",
    "    first_ddf = dd.read_parquet(first_batch_files, engine='pyarrow')\n",
    "\n",
    "first_df = first_ddf.compute(scheduler='synchronous')\n",
    "print(f\"‚úì Primer batch cargado: {first_df.shape}\")\n",
    "\n",
    "# Preprocesar y definir encoders\n",
    "first_df, label_encoders, cat_features = preprocess_dataframe(first_df, fit_encoders=True)\n",
    "\n",
    "# Definir features\n",
    "labels_to_exclude = [\n",
    "    'buyer_d1', 'buyer_d7', 'buyer_d14', 'buyer_d28',\n",
    "    'buy_d7', 'buy_d14', 'buy_d28',\n",
    "    'iap_revenue_d7', 'iap_revenue_d14', 'iap_revenue_d28',\n",
    "    'registration', 'retention_d1_to_d7', 'retention_d3_to_d7',\n",
    "    'retention_d7_to_d14', 'retention_d1', 'retention_d3', 'retention_d7',\n",
    "    'row_id', 'datetime',\n",
    "    'advertiser_actions_action_last_timestamp',\n",
    "    'user_actions_bundles_action_last_timestamp',\n",
    "    'first_request_ts', 'first_request_ts_bundle',\n",
    "    'first_request_ts_category_bottom_taxonomy'\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    c for c in first_df.columns\n",
    "    if c not in labels_to_exclude and c not in cat_features\n",
    "    and first_df[c].dtype in ['int64', 'int32', 'int16', 'int8', 'float32', 'float64']\n",
    "]\n",
    "\n",
    "features = numeric_features + cat_features\n",
    "print(f\"‚úì Features definidas: {len(features)} ({len(numeric_features)} num√©ricas + {len(cat_features)} categ√≥ricas)\")\n",
    "\n",
    "# Crear y fitear StandardScaler\n",
    "scaler = StandardScaler()\n",
    "first_df[numeric_features] = first_df[numeric_features].fillna(0)\n",
    "scaler.fit(first_df[numeric_features])\n",
    "print(f\"‚úì StandardScaler fiteado con {len(first_df)} muestras\")\n",
    "\n",
    "# Guardar artifacts\n",
    "with open('/home/stargix/Desktop/hackathons/datathon/scaler_resnet.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('/home/stargix/Desktop/hackathons/datathon/label_encoders_resnet.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "with open('/home/stargix/Desktop/hackathons/datathon/features_resnet.pkl', 'wb') as f:\n",
    "    pickle.dump({'features': features, 'numeric_features': numeric_features, 'cat_features': cat_features}, f)\n",
    "print(\"‚úì Artifacts guardados\")\n",
    "\n",
    "del first_df, first_ddf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e704eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODELO RESNET INICIALIZADO (OPTIMIZADO)\n",
      "============================================================\n",
      "Input dim: 24\n",
      "Hidden dim: 128 (reducido para memoria)\n",
      "Num blocks: 3 (reducido para memoria)\n",
      "Batch size: 1024 (reducido para memoria)\n",
      "Learning rate: 0.001\n",
      "Epochs per batch: 2\n",
      "Device: cuda\n",
      "============================================================\n",
      "Total par√°metros: 128,897\n",
      "Par√°metros entrenables: 128,897\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Inicializar modelo\n",
    "INPUT_DIM = len(features)\n",
    "HIDDEN_DIM = 128  # üî• Reducido de 256 para ahorrar memoria\n",
    "NUM_BLOCKS = 3     # üî• Reducido de 4\n",
    "BATCH_SIZE = 1024  # üî• Reducido de 2048 para que quepa en memoria\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS_PER_BATCH = 2  # üî• Reducido de 3 para procesar m√°s r√°pido\n",
    "VALIDATION_DATE = pd.Timestamp('2025-10-06', tz='UTC')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RevenueResNet(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_blocks=NUM_BLOCKS).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELO RESNET INICIALIZADO (OPTIMIZADO)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input dim: {INPUT_DIM}\")\n",
    "print(f\"Hidden dim: {HIDDEN_DIM} (reducido para memoria)\")\n",
    "print(f\"Num blocks: {NUM_BLOCKS} (reducido para memoria)\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (reducido para memoria)\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs per batch: {EPOCHS_PER_BATCH}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Contar par√°metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total par√°metros: {total_params:,}\")\n",
    "print(f\"Par√°metros entrenables: {trainable_params:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e671046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENTRENAMIENTO POR BATCHES (MEMORIA OPTIMIZADA)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BATCH 1/21\n",
      "============================================================\n",
      "‚úì Batch cargado: (1026857, 62)\n",
      "‚úì Batch cargado: (1026857, 62)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m batch_ddf  \u001b[38;5;66;03m# üî• Liberar inmediatamente\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Preprocesar\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m batch_df, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_encoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Split temporal\u001b[39;00m\n\u001b[1;32m     37\u001b[0m batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mpreprocess_dataframe\u001b[0;34m(df, label_encoders, fit_encoders)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m label_encoders:\n\u001b[1;32m     48\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__NA__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# üî• Optimizaci√≥n: Convertir a tipos m√°s eficientes\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/series.py:4719\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4640\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4641\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4642\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4645\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4646\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4717\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4719\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4721\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4722\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mpreprocess_dataframe.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m label_encoders:\n\u001b[1;32m     48\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__NA__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mlabel_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m label_encoders[col]\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# üî• Optimizaci√≥n: Convertir a tipos m√°s eficientes\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:134\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:235\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(values\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:173\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(values, uniques)\n\u001b[0;32m--> 173\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m_nandict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mval\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:160\u001b[0m, in \u001b[0;36m_nandict.__init__\u001b[0;34m(self, mapping)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(mapping)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_scalar_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/hackathons/datathon/.venv/lib/python3.10/site-packages/sklearn/utils/_missing.py:42\u001b[0m, in \u001b[0;36mis_scalar_nan\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scalar_nan\u001b[39m(x):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Test if x is NaN.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    This function is meant to overcome the issue that np.isnan does not allow\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, numbers\u001b[38;5;241m.\u001b[39mIntegral)\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(x)\n\u001b[1;32m     44\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ENTRENAMIENTO POR BATCHES (OPTIMIZADO)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO POR BATCHES (MEMORIA OPTIMIZADA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "global_epoch = 0\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BATCH {batch_idx + 1}/{num_batches}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # üî• Limpiar memoria antes de cargar\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Cargar batch\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(parquet_files_train))\n",
    "    batch_files = parquet_files_train[start_idx:end_idx]\n",
    "    \n",
    "    try:\n",
    "        batch_ddf = dd.read_parquet(batch_files, engine='pyarrow', columns=required_columns)\n",
    "    except:\n",
    "        batch_ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "    \n",
    "    batch_df = batch_ddf.compute(scheduler='synchronous')\n",
    "    print(f\"‚úì Batch cargado: {batch_df.shape}\")\n",
    "    del batch_ddf  # üî• Liberar inmediatamente\n",
    "    \n",
    "    # Preprocesar\n",
    "    batch_df, _, _ = preprocess_dataframe(batch_df, label_encoders=label_encoders, fit_encoders=False)\n",
    "    \n",
    "    # Split temporal\n",
    "    batch_df['datetime'] = pd.to_datetime(batch_df['datetime'].astype(str))\n",
    "    val_mask = batch_df['datetime'].dt.date == VALIDATION_DATE.date()\n",
    "    train_mask = ~val_mask\n",
    "    \n",
    "    if train_mask.sum() == 0:\n",
    "        print(\"‚ö†Ô∏è No hay datos de entrenamiento en este batch, saltando...\")\n",
    "        del batch_df\n",
    "        gc.collect()\n",
    "        continue\n",
    "    \n",
    "    # Preparar datos (solo train)\n",
    "    X_train_batch = batch_df[train_mask][features].copy()\n",
    "    y_train_batch = batch_df[train_mask]['iap_revenue_d7'].copy()\n",
    "    del batch_df  # üî• Liberar DataFrame completo\n",
    "    \n",
    "    # Preprocesar\n",
    "    X_train_batch[numeric_features] = X_train_batch[numeric_features].fillna(0)\n",
    "    X_train_batch[numeric_features] = scaler.transform(X_train_batch[numeric_features])\n",
    "    X_train_batch = X_train_batch.fillna(0)\n",
    "    \n",
    "    # üî• Limitar tama√±o si es muy grande\n",
    "    if len(X_train_batch) > 500000:\n",
    "        print(f\"‚ö†Ô∏è Batch muy grande ({len(X_train_batch):,}), usando solo 500k samples...\")\n",
    "        indices = np.random.choice(len(X_train_batch), 500000, replace=False)\n",
    "        X_train_batch = X_train_batch.iloc[indices]\n",
    "        y_train_batch = y_train_batch.iloc[indices]\n",
    "    \n",
    "    # A tensors\n",
    "    X_tensor = torch.FloatTensor(X_train_batch.values).to(device)\n",
    "    y_tensor = torch.FloatTensor(np.log1p(y_train_batch.values)).reshape(-1, 1).to(device)\n",
    "    del X_train_batch, y_train_batch  # üî• Liberar DataFrames\n",
    "    \n",
    "    train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Entrenar epochs en este batch\n",
    "    for epoch in range(EPOCHS_PER_BATCH):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        global_epoch += 1\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{EPOCHS_PER_BATCH} (Global: {global_epoch}) | Loss: {train_loss:.6f}\")\n",
    "    \n",
    "    print(f\"‚úì Batch {batch_idx + 1} completado\")\n",
    "    \n",
    "    # üî• Limpiar agresivamente\n",
    "    del X_tensor, y_tensor, train_dataset, train_loader\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì ENTRENAMIENTO COMPLETADO\")\n",
    "print(f\"Total epochs: {global_epoch}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Guardar modelo\n",
    "torch.save(model.state_dict(), '/home/stargix/Desktop/hackathons/datathon/resnet_model.pt')\n",
    "print(\"‚úì Modelo guardado: resnet_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUACI√ìN EN VALIDACI√ìN (OPTIMIZADA)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUACI√ìN EN VALIDACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# üî• Procesar validaci√≥n en batches para no llenar memoria\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(parquet_files_train))\n",
    "    batch_files = parquet_files_train[start_idx:end_idx]\n",
    "    \n",
    "    try:\n",
    "        batch_ddf = dd.read_parquet(batch_files, engine='pyarrow', columns=required_columns)\n",
    "    except:\n",
    "        batch_ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "    \n",
    "    batch_df = batch_ddf.compute(scheduler='synchronous')\n",
    "    del batch_ddf\n",
    "    \n",
    "    batch_df, _, _ = preprocess_dataframe(batch_df, label_encoders=label_encoders, fit_encoders=False)\n",
    "    \n",
    "    batch_df['datetime'] = pd.to_datetime(batch_df['datetime'].astype(str))\n",
    "    val_mask = batch_df['datetime'].dt.date == VALIDATION_DATE.date()\n",
    "    \n",
    "    if val_mask.sum() > 0:\n",
    "        X_val_batch = batch_df[val_mask][features].copy()\n",
    "        y_val_batch = batch_df[val_mask]['iap_revenue_d7'].copy()\n",
    "        \n",
    "        # Preprocesar\n",
    "        X_val_batch[numeric_features] = X_val_batch[numeric_features].fillna(0)\n",
    "        X_val_batch[numeric_features] = scaler.transform(X_val_batch[numeric_features])\n",
    "        X_val_batch = X_val_batch.fillna(0)\n",
    "        \n",
    "        # Predecir\n",
    "        model.eval()\n",
    "        X_val_tensor = torch.FloatTensor(X_val_batch.values).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_log = model(X_val_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        val_predictions.append(np.expm1(pred_log).clip(0, None))\n",
    "        val_targets.append(y_val_batch.values)\n",
    "        \n",
    "        del X_val_batch, y_val_batch, X_val_tensor, pred_log\n",
    "    \n",
    "    del batch_df\n",
    "    gc.collect()\n",
    "\n",
    "# Combinar resultados\n",
    "pred = np.concatenate(val_predictions)\n",
    "y_val = np.concatenate(val_targets)\n",
    "\n",
    "print(f\"‚úì Datos de validaci√≥n: {len(y_val):,} muestras\")\n",
    "\n",
    "# M√©tricas\n",
    "msle = mean_squared_log_error(y_val, pred)\n",
    "rmse = mean_squared_error(y_val, pred, squared=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTADOS EN VALIDACI√ìN (RESNET)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"MSLE: {msle:.6f}\")\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"Revenue promedio predicho: ${pred.mean():.2f}\")\n",
    "print(f\"Revenue promedio real: ${y_val.mean():.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "del val_predictions, val_targets, pred, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCIA R√ÅPIDA EN TEST (OPTIMIZADA)\n",
    "TEST_PATH = \"/home/stargix/Desktop/hackathons/datathon/test/test\"\n",
    "SUBMISSION_PATH = \"/home/stargix/Desktop/hackathons/datathon/submission_resnet.csv\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCIA R√ÅPIDA EN TEST (OPTIMIZADA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar CSV\n",
    "import csv\n",
    "with open(SUBMISSION_PATH, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['row_id', 'iap_revenue_d7'])\n",
    "\n",
    "# Repartir test en chunks m√°s peque√±os\n",
    "test_meta = dd.read_parquet(TEST_PATH, engine=\"pyarrow\", index=False).head(0)\n",
    "available_cols = [c for c in [\"row_id\"] + features if c in test_meta.columns]\n",
    "\n",
    "dd_test = dd.read_parquet(\n",
    "    TEST_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=available_cols,\n",
    "    blocksize=\"64MB\"  # üî• Reducido de 128MB a 64MB\n",
    ")\n",
    "dd_test = dd_test.repartition(npartitions=512)  # üî• M√°s particiones, chunks m√°s peque√±os\n",
    "delayed_parts = dd_test.to_delayed()\n",
    "print(f\"Chunks: {len(delayed_parts)} (m√°s peque√±os para evitar crash)\\\\n\")\n",
    "\n",
    "# Mapeos para label encoding\n",
    "le_maps = {\n",
    "    col: {cls: idx for idx, cls in enumerate(enc.classes_)}\n",
    "    for col, enc in label_encoders.items()\n",
    "}\n",
    "\n",
    "def sum_list_series(series: pd.Series) -> pd.Series:\n",
    "    return series.apply(\n",
    "        lambda x: sum(item[1] for item in x if isinstance(item, tuple) and len(item) > 1)\n",
    "        if isinstance(x, list) else 0\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "total_rows = 0\n",
    "total_time = 0\n",
    "\n",
    "for i, delayed_part in enumerate(delayed_parts, 1):\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    # üî• Limpiar memoria antes de cada chunk\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    part_df = delayed_part.compute()\n",
    "    row_ids = part_df[\"row_id\"].values\n",
    "\n",
    "    X_part = part_df.reindex(columns=features, fill_value=0)\n",
    "\n",
    "    # Sumar columnas con listas\n",
    "    for col in columns_to_sum:\n",
    "        if col in X_part.columns:\n",
    "            X_part[col] = sum_list_series(X_part[col])\n",
    "\n",
    "    # Num√©ricas\n",
    "    if numeric_features:\n",
    "        X_part[numeric_features] = X_part[numeric_features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        X_part[numeric_features] = X_part[numeric_features].fillna(0)\n",
    "        X_part[numeric_features] = scaler.transform(X_part[numeric_features])\n",
    "\n",
    "    # Categ√≥ricas\n",
    "    for col in cat_features:\n",
    "        if col in X_part.columns:\n",
    "            mapped = X_part[col].astype(str).map(le_maps[col]).fillna(-1).astype(\"int32\")\n",
    "            X_part[col] = mapped\n",
    "    \n",
    "    X_part = X_part.fillna(0)\n",
    "\n",
    "    # Predecir (R√ÅPIDO)\n",
    "    X_tensor = torch.FloatTensor(X_part.values).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_log = model(X_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    pred = np.expm1(pred_log).clip(0, None)\n",
    "\n",
    "    # Guardar inmediatamente\n",
    "    chunk_df = pd.DataFrame({\"row_id\": row_ids, \"iap_revenue_d7\": pred})\n",
    "    chunk_df.to_csv(SUBMISSION_PATH, mode='a', header=False, index=False)\n",
    "    \n",
    "    elapsed = time.perf_counter() - t0\n",
    "    total_rows += len(row_ids)\n",
    "    total_time += elapsed\n",
    "    \n",
    "    if i % 10 == 0:  # üî• Print cada 10 chunks para reducir output\n",
    "        print(f\"[{i}/{len(delayed_parts)}] {total_rows:,} filas totales | {total_rows/total_time:,.0f} samples/s\")\n",
    "    \n",
    "    # üî• Limpiar agresivamente\n",
    "    del part_df, X_part, X_tensor, pred_log, pred, row_ids, chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"‚úì SUBMISSION COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total filas: {total_rows:,}\")\n",
    "print(f\"Tiempo total: {total_time:.2f}s\")\n",
    "print(f\"Velocidad promedio: {total_rows/total_time:,.0f} samples/s\")\n",
    "print(f\"Archivo: {SUBMISSION_PATH}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar\n",
    "submission = pd.read_csv(SUBMISSION_PATH)\n",
    "print(f\"\\\\n‚úì Verificaci√≥n:\")\n",
    "print(f\"  Filas: {len(submission):,}\")\n",
    "print(f\"  Row IDs √∫nicos: {submission['row_id'].nunique():,}\")\n",
    "print(f\"  Duplicados: {submission['row_id'].duplicated().sum()}\")\n",
    "print(f\"  NaNs: {submission['iap_revenue_d7'].isna().sum()}\")\n",
    "print(f\"\\\\nEstad√≠sticas:\")\n",
    "print(submission['iap_revenue_d7'].describe())\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
