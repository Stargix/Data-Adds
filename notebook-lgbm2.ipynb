{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120867,"databundleVersionId":14453560,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"notebook3ec1a6a2c4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import dask\nimport dask.dataframe as dd\n\ndask.config.set({\"dataframe.convert-string\": False})\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport gc\n\nTRAIN_PATH = \"/kaggle/input/smadex-challenge-predict-the-revenue/train/train\"\nTEST_PATH  = \"/kaggle/input/smadex-challenge-predict-the-revenue/test/test\"\n\nTARGET_COL = \"iap_revenue_d7\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:38:48.343915Z","iopub.execute_input":"2025-11-16T01:38:48.344248Z","iopub.status.idle":"2025-11-16T01:38:48.350249Z","shell.execute_reply.started":"2025-11-16T01:38:48.344227Z","shell.execute_reply":"2025-11-16T01:38:48.349256Z"},"id":"vtZioGhSGRpE"},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Columnas monstruosas que casi seguro van con listas/mapas/histogramas.\n# Las quitamos del baseline para RAM y simplicidad.\nignore_big_cols = [\n    \"bundles_ins\",\n    \"user_bundles\",\n    \"user_bundles_l28d\",\n    \"city_hist\",\n    \"country_hist\",\n    \"region_hist\",\n    \"dev_language_hist\",\n    \"dev_osv_hist\",\n    \"bcat\",\n    \"bcat_bottom_taxonomy\",\n    \"bundles_cat\",\n    \"bundles_cat_bottom_taxonomy\",\n    \"first_request_ts_bundle\",\n    \"first_request_ts_category_bottom_taxonomy\",\n    # \"last_buy_ts_bundle\",  # REMOVIDA - significativa\n    # \"last_buy_ts_category\",  # REMOVIDA - significativa\n    \"last_install_ts_bundle\",\n    \"last_install_ts_category\",\n    # \"advertiser_actions_action_count\",  # REMOVIDA - significativa\n    # \"advertiser_actions_action_last_timestamp\",  # REMOVIDA - significativa\n    \"user_actions_bundles_action_count\",\n    \"user_actions_bundles_action_last_timestamp\",\n    \"new_bundles\",\n    \"whale_users_bundle_num_buys_prank\",\n    \"whale_users_bundle_revenue_prank\",\n    \"whale_users_bundle_total_num_buys\",\n    \"whale_users_bundle_total_revenue\",\n]\n\nsignificant_list_columns = [\n    'advertiser_actions_action_count',\n    'advertiser_actions_action_last_timestamp',\n    'last_buy_ts_category',\n    'iap_revenue_usd_category_bottom_taxonomy',\n    'num_buys_category_bottom_taxonomy',\n    'last_buy_ts_bundle',\n    'num_buys_category',\n    'iap_revenue_usd_category',\n    'rev_by_adv'\n]\n\ndef reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n    df = df.copy()\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type == \"float64\":\n            df[col] = df[col].astype(\"float32\")\n        elif col_type == \"int64\":\n            df[col] = df[col].astype(\"int32\")\n    return df\n\ndef detect_listlike_columns(df: pd.DataFrame, cols=None):\n    \"\"\"Detecta columnas que contienen listas o dicts.\"\"\"\n    if cols is None:\n        cols = df.columns\n    listlike = []\n    for c in cols:\n        sample_vals = df[c].head(100)\n        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n            listlike.append(c)\n    return listlike\n\ndef preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n    \"\"\"Preprocesado para train/valid.\"\"\"\n    X_train = X_train.copy()\n    X_valid = X_valid.copy()\n\n    # Numéricas: NaN -> 0\n    for c in num_cols:\n        X_train[c] = X_train[c].fillna(0)\n        X_valid[c] = X_valid[c].fillna(0)\n\n    # Categóricas: strings + categorías fijas basadas en train\n    for c in cat_cols:\n        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n        X_train[c] = X_train[c].astype(\"category\")\n\n        cats = X_train[c].cat.categories\n        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n        X_valid[c] = X_valid[c].astype(\n            pd.api.types.CategoricalDtype(categories=cats)\n        )\n\n    return X_train, X_valid\n\ndef preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n    X_new = X_new.copy()\n\n    for c in num_cols:\n        if c in X_new.columns:\n            X_new[c] = X_new[c].fillna(0)\n\n    for c in cat_cols:\n        if c in X_new.columns:\n            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n            cats = cat_ref_df[c].cat.categories\n            X_new[c] = X_new[c].astype(\n                pd.api.types.CategoricalDtype(categories=cats)\n            )\n\n    return X_new\n\n# Versión in-place (modifica el DataFrame original)\ndef convert_lists_to_length_inplace(df, list_columns):\n    \"\"\"Convierte columnas tipo lista a su longitud (in-place)\"\"\"\n    for col in list_columns:\n        if col in df.columns:\n            df[col] = df[col].apply(\n                lambda x: len(x) if isinstance(x, (list, tuple)) and x is not None \n                else (len(eval(x)) if isinstance(x, str) and x.strip() else 0)\n            )\n            print(f\"✓ {col} → longitud\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:38:48.352117Z","iopub.execute_input":"2025-11-16T01:38:48.352565Z","iopub.status.idle":"2025-11-16T01:38:48.374415Z","shell.execute_reply.started":"2025-11-16T01:38:48.352527Z","shell.execute_reply":"2025-11-16T01:38:48.373321Z"},"id":"yxYY_yRtGRpF"},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Train: 1–5 de octubre\nfilters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n\n# Valid: día 6 de octubre\nfilters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n\ndd_train = dd.read_parquet(TRAIN_PATH, filters=filters_train)\ndd_valid = dd.read_parquet(TRAIN_PATH, filters=filters_valid)\n\n# Quitar las columnas monstruosas si existen\nexisting_big_cols_train = [c for c in ignore_big_cols if c in dd_train.columns]\nexisting_big_cols_valid = [c for c in ignore_big_cols if c in dd_valid.columns]\n\ndd_train = dd_train.drop(columns=existing_big_cols_train)\ndd_valid = dd_valid.drop(columns=existing_big_cols_valid)\n\n# Muestreo de train: AJUSTA ESTO si quieres más datos\nfrac_train = 0.10  # 10% de train; puedes subir a 0.2 si ves que va bien\n\ntrain_sample = dd_train.sample(frac=frac_train, random_state=42).compute()\nvalid_df     = dd_valid.compute()\n\ntrain_sample = reduce_memory(train_sample)\nvalid_df     = reduce_memory(valid_df)\n\nconvert_lists_to_length_inplace(train_sample, significant_list_columns)\nconvert_lists_to_length_inplace(valid_df, significant_list_columns)\n\nprint(\"Train sample shape:\", train_sample.shape)\nprint(\"Valid shape:\", valid_df.shape)\nprint(\"Train memory (GB):\", train_sample.memory_usage(deep=True).sum() / (1024**3))\nprint(\"Valid memory (GB):\", valid_df.memory_usage(deep=True).sum() / (1024**3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:38:48.376087Z","iopub.execute_input":"2025-11-16T01:38:48.376505Z","iopub.status.idle":"2025-11-16T01:46:22.158921Z","shell.execute_reply.started":"2025-11-16T01:38:48.376473Z","shell.execute_reply":"2025-11-16T01:46:22.157778Z"},"id":"-Li_-OkSGRpH","outputId":"90641ad0-2ada-4096-a14c-0fdebe1b1d04"},"outputs":[{"name":"stdout","text":"✓ advertiser_actions_action_count → longitud\n✓ advertiser_actions_action_last_timestamp → longitud\n✓ last_buy_ts_category → longitud\n✓ iap_revenue_usd_category_bottom_taxonomy → longitud\n✓ num_buys_category_bottom_taxonomy → longitud\n✓ last_buy_ts_bundle → longitud\n✓ num_buys_category → longitud\n✓ iap_revenue_usd_category → longitud\n✓ rev_by_adv → longitud\n✓ advertiser_actions_action_count → longitud\n✓ advertiser_actions_action_last_timestamp → longitud\n✓ last_buy_ts_category → longitud\n✓ iap_revenue_usd_category_bottom_taxonomy → longitud\n✓ num_buys_category_bottom_taxonomy → longitud\n✓ last_buy_ts_bundle → longitud\n✓ num_buys_category → longitud\n✓ iap_revenue_usd_category → longitud\n✓ rev_by_adv → longitud\nTrain sample shape: (1729408, 62)\nValid shape: (3306478, 62)\nTrain memory (GB): 2.4286708682775497\nValid memory (GB): 4.663316981866956\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Todas las labels auxiliares que SOLO están en train\nLABEL_COLS = [\n    \"buyer_d1\",\n    \"buyer_d7\",\n    \"buyer_d14\",\n    \"buyer_d28\",\n    \"buy_d7\",\n    \"buy_d14\",\n    \"buy_d28\",\n    \"iap_revenue_d7\",   # target principal\n    \"iap_revenue_d14\",\n    \"iap_revenue_d28\",\n    \"registration\",\n    \"retention_d1_to_d7\",\n    \"retention_d3_to_d7\",\n    \"retention_d7_to_d14\",\n    \"retention_d1\",\n    \"retention_d3\",\n    \"retentiond7\",\n]\n\nTARGET_COL = \"iap_revenue_d7\"\n\nassert TARGET_COL in train_sample.columns, \"No está iap_revenue_d7 en train\"\n\n# y_train / y_valid: solo la target principal\ny_train = train_sample[TARGET_COL].values\ny_valid = valid_df[TARGET_COL].values\n\n# Columnas que NO queremos como features\ncols_to_drop_from_X = [\"row_id\", \"datetime\"] + LABEL_COLS\n\n# Features = todas las demás\nfeature_cols = [c for c in train_sample.columns if c not in cols_to_drop_from_X]\n\nprint(\"Número de features:\", len(feature_cols))\n\nX_train = train_sample[feature_cols].copy()\nX_valid = valid_df[feature_cols].copy()\n\n# 1) Detectar columnas con listas/dicts y quitarlas\nlistlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\nprint(\"Columnas con listas/dicts:\", listlike_cols)\n\nX_train = X_train.drop(columns=listlike_cols)\nX_valid = X_valid.drop(columns=listlike_cols)\n\n# 2) Volver a calcular numéricas y categóricas\nnum_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X_train.columns if c not in num_cols]\n\nprint(\"Numéricas:\", len(num_cols))\nprint(\"Categóricas:\", len(cat_cols))\n\n# 3) Preprocesar\nX_train_prep, X_valid_prep = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n\nprint(\"X_train_prep shape:\", X_train_prep.shape)\nprint(\"X_valid_prep shape:\", X_valid_prep.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:46:22.160297Z","iopub.execute_input":"2025-11-16T01:46:22.160703Z","iopub.status.idle":"2025-11-16T01:46:48.207944Z","shell.execute_reply.started":"2025-11-16T01:46:22.160672Z","shell.execute_reply":"2025-11-16T01:46:48.206764Z"},"id":"pNaidivyGRpK","outputId":"906e4395-6204-4ce7-d4cc-224c7ebb87c6"},"outputs":[{"name":"stdout","text":"Número de features: 43\nColumnas con listas/dicts: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'num_buys_bundle', 'rwd_prank']\nNuméricas: 19\nCategóricas: 14\nX_train_prep shape: (1729408, 33)\nX_valid_prep shape: (3306478, 33)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\ny_train_log = np.log1p(y_train)\ny_valid_log = np.log1p(y_valid)\n\nmodel = LGBMRegressor(\n    objective=\"regression\",\n    n_estimators=600,\n    learning_rate=0.05,\n    num_leaves=255,\n    max_depth=-1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n    verbosity=-1\n)\n\nmodel.fit(X_train_prep, y_train_log)\n\nprint(\"Modelo entrenado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:46:48.210235Z","iopub.execute_input":"2025-11-16T01:46:48.210617Z","iopub.status.idle":"2025-11-16T01:48:20.509905Z","shell.execute_reply.started":"2025-11-16T01:46:48.210594Z","shell.execute_reply":"2025-11-16T01:48:20.508966Z"},"id":"bFGe1otoGRpM","outputId":"3f485afe-c7d0-4a28-8173-05c861c019bc"},"outputs":[{"name":"stdout","text":"Modelo entrenado.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Predicción en espacio log\nvalid_pred_log = model.predict(X_valid_prep)\n\n# Volver al espacio original\nvalid_pred = np.expm1(valid_pred_log)significant_list_columns = [\n    'advertiser_actions_action_count',\n    'advertiser_actions_action_last_timestamp',\n    'last_buy_ts_category',\n    'iap_revenue_usd_category_bottom_taxonomy',\n    'num_buys_category_bottom_taxonomy',\n    'last_buy_ts_bundle',\n    'num_buys_category',\n    'iap_revenue_usd_category',\n    'rev_by_adv'\n]\nvalid_pred = np.clip(valid_pred, 0, None)\n\nmsle_model = mean_squared_log_error(y_valid, valid_pred)\nprint(\"MSLE modelo:\", msle_model)\n\nzeros_pred = np.zeros_like(y_valid)\nmsle_zeros = mean_squared_log_error(y_valid, zeros_pred)\nprint(\"MSLE baseline (todo 0):\", msle_zeros)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:48:20.511023Z","iopub.execute_input":"2025-11-16T01:48:20.511343Z","iopub.status.idle":"2025-11-16T01:53:55.063064Z","shell.execute_reply.started":"2025-11-16T01:48:20.511313Z","shell.execute_reply":"2025-11-16T01:53:55.062024Z"},"id":"En06DHuxGRpN","outputId":"e844c3e8-e72f-4627-e800-7f3f2ab5b9c2"},"outputs":[{"name":"stdout","text":"MSLE modelo: 0.17849401207889368\nMSLE baseline (todo 0): 0.21498893\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import os\n# Conservamos SOLO lo imprescindible para el test: model, X_train_prep, num_cols, cat_cols\nto_keep = {\"model\", \"X_train_prep\", \"num_cols\", \"cat_cols\"}\n\nfor name in list(globals().keys()):\n    if name.startswith(\"_\"):\n        continue\n    if name in to_keep:\n        continue\n    # No borramos módulos (dask, pd, np, etc.)\n    if isinstance(globals()[name], type(os)):\n        continue\n    try:\n        del globals()[name]\n    except:\n        pass\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:53:55.064388Z","iopub.execute_input":"2025-11-16T01:53:55.064708Z","iopub.status.idle":"2025-11-16T01:54:06.543703Z","shell.execute_reply.started":"2025-11-16T01:53:55.064678Z","shell.execute_reply":"2025-11-16T01:54:06.542410Z"},"id":"c4gX7fyvGRpP","outputId":"13140b1f-fe56-4921-a3cd-a5b89d93e975"},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Columnas monstruosas que casi seguro van con listas/mapas/histogramas.\n# Las quitamos del baseline para RAM y simplicidad.\nignore_big_cols = [\n    \"bundles_ins\",\n    \"user_bundles\",\n    \"user_bundles_l28d\",\n    \"city_hist\",\n    \"country_hist\",\n    \"region_hist\",\n    \"dev_language_hist\",\n    \"dev_osv_hist\",\n    \"bcat\",\n    \"bcat_bottom_taxonomy\",\n    \"bundles_cat\",\n    \"bundles_cat_bottom_taxonomy\",\n    \"first_request_ts_bundle\",\n    \"first_request_ts_category_bottom_taxonomy\",\n    # \"last_buy_ts_bundle\",  # REMOVIDA - significativa\n    # \"last_buy_ts_category\",  # REMOVIDA - significativa\n    \"last_install_ts_bundle\",\n    \"last_install_ts_category\",\n    # \"advertiser_actions_action_count\",  # REMOVIDA - significativa\n    # \"advertiser_actions_action_last_timestamp\",  # REMOVIDA - significativa\n    \"user_actions_bundles_action_count\",\n    \"user_actions_bundles_action_last_timestamp\",\n    \"new_bundles\",\n    \"whale_users_bundle_num_buys_prank\",\n    \"whale_users_bundle_revenue_prank\",\n    \"whale_users_bundle_total_num_buys\",\n    \"whale_users_bundle_total_revenue\",\n]\n\ndef reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Downcast numéricas para ahorrar memoria.\"\"\"\n    df = df.copy()\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type == \"float64\":\n            df[col] = df[col].astype(\"float32\")\n        elif col_type == \"int64\":\n            df[col] = df[col].astype(\"int32\")\n    return df\n\ndef detect_listlike_columns(df: pd.DataFrame, cols=None):\n    \"\"\"Detecta columnas que contienen listas o dicts.\"\"\"\n    if cols is None:\n        cols = df.columns\n    listlike = []\n    for c in cols:\n        sample_vals = df[c].head(100)\n        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n            listlike.append(c)\n    return listlike\n\ndef preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n    \"\"\"Preprocesado para train/valid.\"\"\"\n    X_train = X_train.copy()\n    X_valid = X_valid.copy()\n\n    # Numéricas: NaN -> 0\n    for c in num_cols:\n        X_train[c] = X_train[c].fillna(0)\n        X_valid[c] = X_valid[c].fillna(0)\n\n    # Categóricas: strings + categorías fijas basadas en train\n    for c in cat_cols:\n        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n        X_train[c] = X_train[c].astype(\"category\")\n\n        cats = X_train[c].cat.categories\n        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n        X_valid[c] = X_valid[c].astype(\n            pd.api.types.CategoricalDtype(categories=cats)\n        )\n\n    return X_train, X_valid\n\ndef preprocess_new(X_new, num_cols, cat_cols, cat_ref_df):\n    \"\"\"Preprocesado para test usando las categorías de train.\"\"\"\n    X_new = X_new.copy()\n\n    for c in num_cols:\n        if c in X_new.columns:\n            X_new[c] = X_new[c].fillna(0)\n\n    for c in cat_cols:\n        if c in X_new.columns:\n            X_new[c] = X_new[c].astype(\"object\").fillna(\"unknown\").astype(str)\n            cats = cat_ref_df[c].cat.categories\n            X_new[c] = X_new[c].astype(\n                pd.api.types.CategoricalDtype(categories=cats)\n            )\n\n    return X_new\n\nimport ast\n\ndef convert_length_partition(partition, list_columns):\n    def safe_length(x):\n        if isinstance(x, (list, tuple)) and x is not None:\n            return len(x)\n        elif isinstance(x, str) and x.strip():\n            try:\n                parsed = ast.literal_eval(x)\n                if isinstance(parsed, (list, tuple)):\n                    return len(parsed)\n            except (ValueError, SyntaxError):\n                pass\n        return 0\n    \n    for col in list_columns:\n        if col in partition.columns:\n            partition[col] = partition[col].apply(safe_length)\n    return partition\n\ndef convert_lists_to_length_inplace(df, list_columns):\n    return df.map_partitions(\n        convert_length_partition, \n        list_columns=list_columns,\n        meta=df._meta\n    )\n\nsignificant_list_columns = [\n    'advertiser_actions_action_count',\n    'advertiser_actions_action_last_timestamp',\n    'last_buy_ts_category',\n    'iap_revenue_usd_category_bottom_taxonomy',\n    'num_buys_category_bottom_taxonomy',\n    'last_buy_ts_bundle',\n    'num_buys_category',\n    'iap_revenue_usd_category',\n    'rev_by_adv'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:58:37.766350Z","iopub.execute_input":"2025-11-16T01:58:37.767127Z","iopub.status.idle":"2025-11-16T01:58:37.784390Z","shell.execute_reply.started":"2025-11-16T01:58:37.767099Z","shell.execute_reply":"2025-11-16T01:58:37.782904Z"},"id":"2fI9MGGqGRpQ"},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import dask\nimport dask.dataframe as dd\n\nTEST_PATH = \"/kaggle/input/smadex-challenge-predict-the-revenue/test/test\"\ndd_test = dd.read_parquet(TEST_PATH)\n\n# 1. Eliminar columnas grandes (solo UNA vez)\nexisting_big_cols_test = [c for c in ignore_big_cols if c in dd_test.columns]\ndd_test = dd_test.drop(columns=existing_big_cols_test)\n\n# 2. Convertir listas a longitud ANTES de to_delayed()\ndd_test = convert_lists_to_length_inplace(dd_test, significant_list_columns)\n\n# 3. AHORA sí, convertir a delayed\ndelayed_parts = dd_test.to_delayed()\nprint(\"Número de chunks de test:\", len(delayed_parts))\n\n# 4. Procesar cada chunk\nfeature_cols = X_train_prep.columns.tolist()\npred_dfs = []\n\nfor i, d in enumerate(delayed_parts):\n    print(f\"Procesando chunk {i+1}/{len(delayed_parts)}...\")\n    part_df = d.compute()\n    part_df = reduce_memory(part_df)\n    row_ids = part_df[\"row_id\"].values\n    X_part = part_df[feature_cols].copy()\n    X_part_prep = preprocess_new(X_part, num_cols, cat_cols, X_train_prep)\n    part_pred_log = model.predict(X_part_prep)\n    part_pred = np.expm1(part_pred_log)\n    part_pred = np.clip(part_pred, 0, None)\n    pred_dfs.append(pd.DataFrame({\n        \"row_id\": row_ids,\n        \"iap_revenue_d7\": part_pred\n    }))\n    del part_df, X_part, X_part_prep, row_ids, part_pred_log, part_pred\n    gc.collect()\n\nsubmission = pd.concat(pred_dfs, ignore_index=True)\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T02:00:56.061661Z","iopub.execute_input":"2025-11-16T02:00:56.062075Z","iopub.status.idle":"2025-11-16T02:29:00.639627Z","shell.execute_reply.started":"2025-11-16T02:00:56.062041Z","shell.execute_reply":"2025-11-16T02:29:00.638400Z"},"id":"jpX7nzdvGRpR","outputId":"e9137be5-36e2-4e0f-da24-3bcc9499426a"},"outputs":[{"name":"stdout","text":"Número de chunks de test: 48\nProcesando chunk 1/48...\nProcesando chunk 2/48...\nProcesando chunk 3/48...\nProcesando chunk 4/48...\nProcesando chunk 5/48...\nProcesando chunk 6/48...\nProcesando chunk 7/48...\nProcesando chunk 8/48...\nProcesando chunk 9/48...\nProcesando chunk 10/48...\nProcesando chunk 11/48...\nProcesando chunk 12/48...\nProcesando chunk 13/48...\nProcesando chunk 14/48...\nProcesando chunk 15/48...\nProcesando chunk 16/48...\nProcesando chunk 17/48...\nProcesando chunk 18/48...\nProcesando chunk 19/48...\nProcesando chunk 20/48...\nProcesando chunk 21/48...\nProcesando chunk 22/48...\nProcesando chunk 23/48...\nProcesando chunk 24/48...\nProcesando chunk 25/48...\nProcesando chunk 26/48...\nProcesando chunk 27/48...\nProcesando chunk 28/48...\nProcesando chunk 29/48...\nProcesando chunk 30/48...\nProcesando chunk 31/48...\nProcesando chunk 32/48...\nProcesando chunk 33/48...\nProcesando chunk 34/48...\nProcesando chunk 35/48...\nProcesando chunk 36/48...\nProcesando chunk 37/48...\nProcesando chunk 38/48...\nProcesando chunk 39/48...\nProcesando chunk 40/48...\nProcesando chunk 41/48...\nProcesando chunk 42/48...\nProcesando chunk 43/48...\nProcesando chunk 44/48...\nProcesando chunk 45/48...\nProcesando chunk 46/48...\nProcesando chunk 47/48...\nProcesando chunk 48/48...\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                                 row_id  iap_revenue_d7\n0  e2f514a9-d922-4a17-bf94-f228bf4cd82f        0.000000\n1  4bfc70d3-d619-410a-9683-4cd759f30f32        0.040222\n2  ad433b66-b41e-4157-a6fd-24cd30701f6a        0.000000\n3  5ed964d6-ddce-42e8-9fad-276eb7f64c2f        0.004664\n4  81b73a45-c395-4d08-a4a3-513873440db3        0.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>iap_revenue_d7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e2f514a9-d922-4a17-bf94-f228bf4cd82f</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4bfc70d3-d619-410a-9683-4cd759f30f32</td>\n      <td>0.040222</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ad433b66-b41e-4157-a6fd-24cd30701f6a</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5ed964d6-ddce-42e8-9fad-276eb7f64c2f</td>\n      <td>0.004664</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>81b73a45-c395-4d08-a4a3-513873440db3</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"print(submission.head())\nprint(submission.shape)\n\nprint(submission.isna().sum())          # no debería haber NaNs\nprint((submission['iap_revenue_d7'] < 0).sum())  # debería ser 0","metadata":{"trusted":true,"id":"nOarGav4GRpS","outputId":"445a2165-941a-4ea3-c9ce-1bbbfec43137"},"outputs":[],"execution_count":null}]}