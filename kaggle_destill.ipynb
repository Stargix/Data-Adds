{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c4fd17",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6da3c1c-e84a-4f43-a910-7cdeb3f2ef27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.750345Z",
     "iopub.status.busy": "2025-11-15T18:28:42.749872Z",
     "iopub.status.idle": "2025-11-15T18:28:42.760122Z",
     "shell.execute_reply": "2025-11-15T18:28:42.757625Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.750289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import gc\n",
    "\n",
    "TRAIN_PATH = \"./train/train\"\n",
    "TEST_PATH  = \"./test/test\"\n",
    "\n",
    "TARGET_COL = \"iap_revenue_d7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8c00cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.762530Z",
     "iopub.status.busy": "2025-11-15T18:28:42.761985Z",
     "iopub.status.idle": "2025-11-15T18:28:42.812139Z",
     "shell.execute_reply": "2025-11-15T18:28:42.810830Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.762492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Sample fraction: 0.1\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "TARGET_COL = \"iap_revenue_d7\"\n",
    "TRAIN_SAMPLE_FRAC = 0.10  # Adjust for more/less data\n",
    "\n",
    "# PyTorch settings\n",
    "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 256\n",
    "TEACHER_EPOCHS = 5\n",
    "STUDENT_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "DISTILL_ALPHA = 0.6  # weight for hard loss\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Sample fraction: {TRAIN_SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1861a",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "⚠️ **IMPORTANTE:** Si ves errores de CUDA como `cudaErrorUnknown`:\n",
    "\n",
    "1. **REINICIA EL KERNEL** → Botón \"Restart\" en la barra superior o `Ctrl+Shift+P` → \"Restart Kernel\"\n",
    "2. Ejecuta todas las celdas desde el principio\n",
    "3. NO intentes ejecutar celdas individuales después de un error de CUDA\n",
    "\n",
    "Esto sucede porque CUDA entra en un estado corrupto después de un error y necesita reiniciarse completamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270e83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CUDA available: NVIDIA GeForce RTX 5070 Laptop GPU\n",
      "  Memory: 0.00 GB allocated\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: If you get CUDA errors, RESTART THE KERNEL first!\n",
    "# This cell checks CUDA health\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(f\"✓ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB allocated\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ CUDA ERROR DETECTED: {e}\")\n",
    "        print(\"  → PLEASE RESTART THE KERNEL (Ctrl+Shift+P → 'Restart Kernel')\")\n",
    "        print(\"  → Then run all cells from the beginning\")\n",
    "        raise RuntimeError(\"CUDA is in a corrupted state. Restart kernel required.\")\n",
    "else:\n",
    "    print(\"ℹ CUDA not available, using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9232f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.817425Z",
     "iopub.status.busy": "2025-11-15T18:28:42.816128Z",
     "iopub.status.idle": "2025-11-15T18:28:42.857537Z",
     "shell.execute_reply": "2025-11-15T18:28:42.855818Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.817391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7929983ab4c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from sklearn.metrics import mean_squared_log_error, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Reproducibility\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)\n",
    "\n",
    "# Set torch seed with error handling\n",
    "try:\n",
    "    torch.manual_seed(RSEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RSEED)\n",
    "    print(\"✓ Libraries imported successfully\")\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"⚠ CRITICAL: CUDA ERROR DURING INITIALIZATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nSOLUTION:\")\n",
    "        print(\"1. Click 'Restart' button in the notebook toolbar\")\n",
    "        print(\"2. Or: Ctrl+Shift+P → type 'Restart Kernel'\")\n",
    "        print(\"3. Run all cells from the beginning\")\n",
    "        print(\"=\" * 60)\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89432efb",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a6b1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.859861Z",
     "iopub.status.busy": "2025-11-15T18:28:42.859589Z",
     "iopub.status.idle": "2025-11-15T18:28:42.901972Z",
     "shell.execute_reply": "2025-11-15T18:28:42.900449Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.859842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Columnas problemáticas (listas/dicts) que se ignoran\n",
    "IGNORE_BIG_COLS = [\n",
    "    \"bundles_ins\", \"user_bundles\", \"user_bundles_l28d\",\n",
    "    \"city_hist\", \"country_hist\", \"region_hist\",\n",
    "    \"dev_language_hist\", \"dev_osv_hist\",\n",
    "    \"bcat\", \"bcat_bottom_taxonomy\",\n",
    "    \"bundles_cat\", \"bundles_cat_bottom_taxonomy\",\n",
    "    \"first_request_ts_bundle\", \"first_request_ts_category_bottom_taxonomy\",\n",
    "    \"last_buy_ts_bundle\", \"last_buy_ts_category\",\n",
    "    \"last_install_ts_bundle\", \"last_install_ts_category\",\n",
    "    \"advertiser_actions_action_count\", \"advertiser_actions_action_last_timestamp\",\n",
    "    \"user_actions_bundles_action_count\", \"user_actions_bundles_action_last_timestamp\",\n",
    "    \"new_bundles\",\n",
    "    \"whale_users_bundle_num_buys_prank\", \"whale_users_bundle_revenue_prank\",\n",
    "    \"whale_users_bundle_total_num_buys\", \"whale_users_bundle_total_revenue\",\n",
    "]\n",
    "\n",
    "LABEL_COLS = [\n",
    "    \"buyer_d1\", \"buyer_d7\", \"buyer_d14\", \"buyer_d28\",\n",
    "    \"buy_d7\", \"buy_d14\", \"buy_d28\",\n",
    "    \"iap_revenue_d7\", \"iap_revenue_d14\", \"iap_revenue_d28\",\n",
    "    \"registration\",\n",
    "    \"retention_d1_to_d7\", \"retention_d3_to_d7\", \"retention_d7_to_d14\",\n",
    "    \"retention_d1\", \"retention_d3\", \"retention_d7\",\n",
    "]\n",
    "\n",
    "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Downcast numeric columns to save memory.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        elif col_type == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
    "    \"\"\"Detect columns containing lists or dicts.\"\"\"\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    listlike = []\n",
    "    for c in cols:\n",
    "        sample_vals = df[c].head(100)\n",
    "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "            listlike.append(c)\n",
    "    return listlike\n",
    "\n",
    "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
    "    \"\"\"Preprocess train and validation sets.\"\"\"\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    \n",
    "    # Numeric: fill NaN with 0\n",
    "    for c in num_cols:\n",
    "        X_train[c] = X_train[c].fillna(0)\n",
    "        X_valid[c] = X_valid[c].fillna(0)\n",
    "    \n",
    "    # Categorical: convert to strings and encode as integers\n",
    "    cat_mappings = {}\n",
    "    for c in cat_cols:\n",
    "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "        \n",
    "        # Create mapping\n",
    "        cats = X_train[c].cat.categories\n",
    "        cat_mappings[c] = {cat: i for i, cat in enumerate(cats)}\n",
    "        \n",
    "        # Encode train\n",
    "        X_train[c] = X_train[c].cat.codes\n",
    "        \n",
    "        # Encode valid (handle unseen categories)\n",
    "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_valid[c] = X_valid[c].map(cat_mappings[c]).fillna(-1).astype(np.int32)\n",
    "    \n",
    "    return X_train, X_valid, cat_mappings\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358814e",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f345fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.904727Z",
     "iopub.status.busy": "2025-11-15T18:28:42.903990Z",
     "iopub.status.idle": "2025-11-15T18:29:25.618077Z",
     "shell.execute_reply": "2025-11-15T18:29:25.617044Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.904693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 21 out of 144 train files\n",
      "Loading train data...\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "\n",
      "Loading validation data...\n",
      "\n",
      "Loading validation data...\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "\n",
      "✓ Data loaded successfully\n",
      "\n",
      "✓ Data loaded successfully\n",
      "Total memory: ~0.44 GB\n",
      "Total memory: ~0.44 GB\n"
     ]
    }
   ],
   "source": [
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Get list of parquet files\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# Reduce number of files for faster training\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# Columns to drop early\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"row_id\", \"datetime\"]\n",
    "\n",
    "# Load TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Drop heavy columns BEFORE compute\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# Sample in Dask\n",
    "train_sample = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=RSEED).compute()\n",
    "train_sample = reduce_memory(train_sample)\n",
    "\n",
    "print(f\"Train loaded: {train_sample.shape}, Memory: {train_sample.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Clean memory\n",
    "del dd_train\n",
    "gc.collect()\n",
    "\n",
    "# Load VALID\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_valid.columns]\n",
    "dd_valid = dd_valid.drop(columns=existing_cols)\n",
    "\n",
    "# Sample less in validation\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=RSEED).compute()\n",
    "valid_df = reduce_memory(valid_df)\n",
    "\n",
    "print(f\"Valid loaded: {valid_df.shape}, Memory: {valid_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"Total memory: ~{(train_sample.memory_usage(deep=True).sum() + valid_df.memory_usage(deep=True).sum()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045041fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:25.619531Z",
     "iopub.status.busy": "2025-11-15T18:29:25.619190Z",
     "iopub.status.idle": "2025-11-15T18:29:26.982573Z",
     "shell.execute_reply": "2025-11-15T18:29:26.981315Z",
     "shell.execute_reply.started": "2025-11-15T18:29:25.619510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buyer ratio in train: 0.0320\n",
      "Buyer ratio in valid: 0.0330\n",
      "Removing 14 list-like columns: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rwd_prank']\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n"
     ]
    }
   ],
   "source": [
    "# Extract targets\n",
    "y_train = train_sample[TARGET_COL].values\n",
    "y_valid = valid_df[TARGET_COL].values\n",
    "\n",
    "# Extract buyer labels\n",
    "y_train_buyer = train_sample[\"buyer_d7\"].values\n",
    "y_valid_buyer = valid_df[\"buyer_d7\"].values\n",
    "\n",
    "print(f\"Buyer ratio in train: {y_train_buyer.mean():.4f}\")\n",
    "print(f\"Buyer ratio in valid: {y_valid_buyer.mean():.4f}\")\n",
    "\n",
    "# Target transform: log1p for stability (MSLE)\n",
    "y_train_log = np.log1p(y_train.clip(min=0.0))\n",
    "y_valid_log = np.log1p(y_valid.clip(min=0.0))\n",
    "\n",
    "# Prepare features\n",
    "cols_to_drop = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
    "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop]\n",
    "\n",
    "X_train = train_sample[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "\n",
    "# Detect and remove list-like columns\n",
    "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
    "print(f\"Removing {len(listlike_cols)} list-like columns: {listlike_cols}\")\n",
    "X_train = X_train.drop(columns=listlike_cols)\n",
    "X_valid = X_valid.drop(columns=listlike_cols)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Features: {len(X_train.columns)} ({len(num_cols)} numeric, {len(cat_cols)} categorical)\")\n",
    "\n",
    "# Preprocess\n",
    "X_train_prep, X_valid_prep, cat_mappings = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n",
    "print(f\"Data prepared: X_train {X_train_prep.shape}, X_valid {X_valid_prep.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700f8a3",
   "metadata": {},
   "source": [
    "## PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d84898c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:26.983944Z",
     "iopub.status.busy": "2025-11-15T18:29:26.983612Z",
     "iopub.status.idle": "2025-11-15T18:29:27.049211Z",
     "shell.execute_reply": "2025-11-15T18:29:27.047755Z",
     "shell.execute_reply.started": "2025-11-15T18:29:26.983900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, y, y_buyer=None, emb_sizes=None):\n",
    "        if len(cat_cols) > 0:\n",
    "            cat_data = df[cat_cols].values.astype(np.int64)\n",
    "            if emb_sizes is not None:  # Pass emb_sizes from outside\n",
    "                for i in range(cat_data.shape[1]):\n",
    "                    max_idx = emb_sizes[i][0] - 1  # Max valid index\n",
    "                    cat_data[:, i] = np.clip(cat_data[:, i], 0, max_idx)  # Clamp to [0, max_idx]\n",
    "            self.cat = cat_data\n",
    "        else:\n",
    "            self.cat = None\n",
    "        self.num = df[num_cols].values.astype(np.float32) if len(num_cols) > 0 else None\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.buyer = y_buyer.astype(np.float32) if y_buyer is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        if self.cat is not None:\n",
    "            item['cat'] = torch.tensor(self.cat[idx], dtype=torch.long)\n",
    "        if self.num is not None:\n",
    "            item['num'] = torch.tensor(self.num[idx], dtype=torch.float32)\n",
    "        item['y'] = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        if self.buyer is not None:\n",
    "            item['buyer'] = torch.tensor(self.buyer[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "print(\"TabularDataset class defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb785ede",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b4acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding sizes calculated: 15 categorical features\n",
      "Sample sizes (categories, dim): [(459, 45), (24, 2), (58, 5), (92, 9), (5895, 50)]...\n",
      "\n",
      "Creating datasets with proper index clamping...\n",
      "✓ Dataloaders ready. Batches per epoch: train=1061, val=111\n"
     ]
    }
   ],
   "source": [
    "# Calculate embedding sizes BEFORE creating datasets\n",
    "def get_embedding_sizes(cat_cols, cat_mappings, max_emb_dim=50):\n",
    "    \"\"\"Calculate embedding sizes for categorical features.\"\"\"\n",
    "    emb_sizes = []\n",
    "    for c in cat_cols:\n",
    "        n_unique = len(cat_mappings[c]) + 2  # +1 for unseen category\n",
    "        emb_dim = min(max(1, n_unique // 10), max_emb_dim)\n",
    "        emb_sizes.append((n_unique, emb_dim))\n",
    "    return emb_sizes\n",
    "\n",
    "emb_sizes = get_embedding_sizes(cat_cols, cat_mappings, max_emb_dim=50)\n",
    "print(f\"Embedding sizes calculated: {len(emb_sizes)} categorical features\")\n",
    "print(f\"Sample sizes (categories, dim): {emb_sizes[:5]}...\")  # Show first 5\n",
    "\n",
    "# NOW create datasets WITH embedding sizes to clamp indices properly\n",
    "print(\"\\nCreating datasets with proper index clamping...\")\n",
    "train_ds = TabularDataset(X_train_prep, cat_cols, num_cols, y_train_log, y_train_buyer, emb_sizes=emb_sizes)\n",
    "val_ds = TabularDataset(X_valid_prep, cat_cols, num_cols, y_valid_log, y_valid_buyer, emb_sizes=emb_sizes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"✓ Dataloaders ready. Batches per epoch: train={len(train_loader)}, val={len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b386c042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:27.051175Z",
     "iopub.status.busy": "2025-11-15T18:29:27.050677Z",
     "iopub.status.idle": "2025-11-15T18:29:27.069804Z",
     "shell.execute_reply": "2025-11-15T18:29:27.068368Z",
     "shell.execute_reply.started": "2025-11-15T18:29:27.051135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined.\n"
     ]
    }
   ],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Larger teacher model with dual heads (regression + classification).\"\"\"\n",
    "    def __init__(self, emb_sizes, num_len):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(categories, dim) for categories, dim in emb_sizes])\n",
    "        emb_dim_sum = sum([dim for _, dim in emb_sizes]) if len(emb_sizes) > 0 else 0\n",
    "        input_dim = emb_dim_sum + (num_len if num_len > 0 else 0)\n",
    "        \n",
    "        # Main network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Regression head (revenue prediction)\n",
    "        self.reg_head = nn.Linear(128, 1)\n",
    "        \n",
    "        # Classification head (buyer prediction)\n",
    "        self.buyer_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        if x_cat is not None and len(self.embs) > 0:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + ([x_num] if x_num is not None else []), dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        \n",
    "        feat = self.net(x)\n",
    "        out_reg = self.reg_head(feat)\n",
    "        out_buyer = self.buyer_head(feat)\n",
    "        \n",
    "        return out_reg.view(-1), out_buyer.view(-1)\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Smaller student model (faster inference).\"\"\"\n",
    "    def __init__(self, emb_sizes, num_len):\n",
    "        super().__init__()\n",
    "        # Reduce embedding dimensions by half\n",
    "        small_embs = [(n, max(1, d // 2)) for n, d in emb_sizes]\n",
    "        self.embs = nn.ModuleList([nn.Embedding(categories, dim) for categories, dim in small_embs])\n",
    "        emb_dim_sum = sum([dim for _, dim in small_embs]) if len(small_embs) > 0 else 0\n",
    "        input_dim = emb_dim_sum + (num_len if num_len > 0 else 0)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_cat, x_num):\n",
    "        if x_cat is not None and len(self.embs) > 0:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + ([x_num] if x_num is not None else []), dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        \n",
    "        out = self.net(x)\n",
    "        return out.view(-1)\n",
    "\n",
    "print(\"Model classes defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834f2aea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:27.071444Z",
     "iopub.status.busy": "2025-11-15T18:29:27.071088Z",
     "iopub.status.idle": "2025-11-15T18:29:27.236503Z",
     "shell.execute_reply": "2025-11-15T18:29:27.235290Z",
     "shell.execute_reply.started": "2025-11-15T18:29:27.071421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher params: 1,307,912\n",
      "Student params: 504,643\n",
      "Compression ratio: 2.59x\n"
     ]
    }
   ],
   "source": [
    "# Initialize models (emb_sizes already calculated above)\n",
    "teacher = TeacherModel(emb_sizes, len(num_cols)).to(DEVICE)\n",
    "student = StudentModel(emb_sizes, len(num_cols)).to(DEVICE)\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
    "student_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Teacher params: {teacher_params:,}\")\n",
    "print(f\"Student params: {student_params:,}\")\n",
    "print(f\"Compression ratio: {teacher_params / student_params:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45318f67",
   "metadata": {},
   "source": [
    "## Train Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27109caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:27.238761Z",
     "iopub.status.busy": "2025-11-15T18:29:27.238398Z",
     "iopub.status.idle": "2025-11-15T18:29:56.633293Z",
     "shell.execute_reply": "2025-11-15T18:29:56.631729Z",
     "shell.execute_reply.started": "2025-11-15T18:29:27.238730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_teacher(model, train_loader, val_loader, epochs=5, lr=1e-3, device='cpu'):\n",
    "    \"\"\"Train teacher model with dual objectives.\"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss_reg = 0.0\n",
    "        train_loss_buyer = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_cat = batch.get('cat', None)\n",
    "            x_num = batch.get('num', None)\n",
    "            y = batch['y']\n",
    "            buyer = batch['buyer']\n",
    "            \n",
    "            x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "            x_num = x_num.to(device) if x_num is not None else None\n",
    "            y = y.to(device)\n",
    "            buyer = buyer.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            pred_log1p, pred_buyer = model(x_cat, x_num)\n",
    "            \n",
    "            loss_reg = mse_loss(pred_log1p, y)\n",
    "            loss_buyer = bce_loss(pred_buyer, buyer)\n",
    "            loss = loss_reg + 0.5 * loss_buyer\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            train_loss += loss.item() * len(y)\n",
    "            train_loss_reg += loss_reg.item() * len(y)\n",
    "            train_loss_buyer += loss_buyer.item() * len(y)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_loss_reg /= len(train_loader.dataset)\n",
    "        train_loss_buyer /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_cat = batch.get('cat', None)\n",
    "                x_num = batch.get('num', None)\n",
    "                y = batch['y']\n",
    "                buyer = batch['buyer']\n",
    "                \n",
    "                x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "                x_num = x_num.to(device) if x_num is not None else None\n",
    "                y = y.to(device)\n",
    "                buyer = buyer.to(device)\n",
    "                \n",
    "                pred_log1p, pred_buyer = model(x_cat, x_num)\n",
    "                loss_reg = mse_loss(pred_log1p, y)\n",
    "                loss_buyer = bce_loss(pred_buyer, buyer)\n",
    "                loss = loss_reg + 0.5 * loss_buyer\n",
    "                \n",
    "                val_loss += loss.item() * len(y)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train - Total: {train_loss:.6f}, Reg: {train_loss_reg:.6f}, Buyer: {train_loss_buyer:.6f}\")\n",
    "        print(f\"  Valid - Total: {val_loss:.6f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'teacher_model_v2.pt')\n",
    "    print(\"\\n✓ Teacher saved to teacher_model_v2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685b3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING TEACHER MODEL\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.263581, Reg: 0.194033, Buyer: 0.139098\n",
      "  Valid - Total: 0.312583\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.263581, Reg: 0.194033, Buyer: 0.139098\n",
      "  Valid - Total: 0.312583\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.251087, Reg: 0.186901, Buyer: 0.128370\n",
      "  Valid - Total: 0.279049\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.251087, Reg: 0.186901, Buyer: 0.128370\n",
      "  Valid - Total: 0.279049\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.246588, Reg: 0.183845, Buyer: 0.125486\n",
      "  Valid - Total: 0.274354\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.246588, Reg: 0.183845, Buyer: 0.125486\n",
      "  Valid - Total: 0.274354\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.243852, Reg: 0.181922, Buyer: 0.123860\n",
      "  Valid - Total: 0.271111\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.243852, Reg: 0.181922, Buyer: 0.123860\n",
      "  Valid - Total: 0.271111\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.242556, Reg: 0.180875, Buyer: 0.123361\n",
      "  Valid - Total: 0.278387\n",
      "\n",
      "✓ Teacher saved to teacher_model_v2.pt\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.242556, Reg: 0.180875, Buyer: 0.123361\n",
      "  Valid - Total: 0.278387\n",
      "\n",
      "✓ Teacher saved to teacher_model_v2.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the teacher model\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING TEACHER MODEL\")\n",
    "print(\"=\" * 50)\n",
    "train_teacher(teacher, train_loader, val_loader, epochs=TEACHER_EPOCHS, lr=LEARNING_RATE, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb1e53",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed43be0a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T18:29:56.634383Z",
     "iopub.status.idle": "2025-11-15T18:29:56.634731Z",
     "shell.execute_reply": "2025-11-15T18:29:56.634602Z",
     "shell.execute_reply.started": "2025-11-15T18:29:56.634588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING STUDENT WITH DISTILLATION\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.117276, Hard: 0.188695, Soft: 0.010149\n",
      "  Valid - MSE: 0.223658\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.117276, Hard: 0.188695, Soft: 0.010149\n",
      "  Valid - MSE: 0.223658\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.112457, Hard: 0.183429, Soft: 0.005998\n",
      "  Valid - MSE: 0.210370\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.112457, Hard: 0.183429, Soft: 0.005998\n",
      "  Valid - MSE: 0.210370\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.110264, Hard: 0.180311, Soft: 0.005193\n",
      "  Valid - MSE: 0.209373\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.110264, Hard: 0.180311, Soft: 0.005193\n",
      "  Valid - MSE: 0.209373\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.109603, Hard: 0.179142, Soft: 0.005294\n",
      "  Valid - MSE: 0.206700\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.109603, Hard: 0.179142, Soft: 0.005294\n",
      "  Valid - MSE: 0.206700\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.109275, Hard: 0.178588, Soft: 0.005306\n",
      "  Valid - MSE: 0.206560\n",
      "\n",
      "✓ Student saved to student_model_v2.pt\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.109275, Hard: 0.178588, Soft: 0.005306\n",
      "  Valid - MSE: 0.206560\n",
      "\n",
      "✓ Student saved to student_model_v2.pt\n"
     ]
    }
   ],
   "source": [
    "def train_student_with_distillation(student, teacher, train_loader, val_loader, epochs=5, lr=1e-3, alpha=0.6, device='cpu'):\n",
    "    \"\"\"Train student using knowledge distillation from teacher.\n",
    "    \n",
    "    Loss = alpha * L_hard + (1 - alpha) * L_soft\n",
    "    where L_hard = MSE(student, true_label) and L_soft = MSE(student, teacher)\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(student.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    teacher.to(device)\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        student.train()\n",
    "        total_loss = 0.0\n",
    "        hard_loss_sum = 0.0\n",
    "        soft_loss_sum = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "            x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Get teacher predictions (soft targets)\n",
    "            with torch.no_grad():\n",
    "                t_pred_log1p, _ = teacher(x_cat, x_num)\n",
    "            \n",
    "            # Get student predictions\n",
    "            s_pred = student(x_cat, x_num)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss_hard = mse(s_pred, y)\n",
    "            loss_soft = mse(s_pred, t_pred_log1p)\n",
    "            loss = alpha * loss_hard + (1.0 - alpha) * loss_soft\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(y)\n",
    "            hard_loss_sum += loss_hard.item() * len(y)\n",
    "            soft_loss_sum += loss_soft.item() * len(y)\n",
    "        \n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        hard_loss_sum /= len(train_loader.dataset)\n",
    "        soft_loss_sum /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        student.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "                x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                s_pred = student(x_cat, x_num)\n",
    "                loss = mse(s_pred, y)\n",
    "                val_loss += loss.item() * len(y)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train - Total: {total_loss:.6f}, Hard: {hard_loss_sum:.6f}, Soft: {soft_loss_sum:.6f}\")\n",
    "        print(f\"  Valid - MSE: {val_loss:.6f}\")\n",
    "    \n",
    "    torch.save(student.state_dict(), 'student_model_v2.pt')\n",
    "    print(\"\\n✓ Student saved to student_model_v2.pt\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING STUDENT WITH DISTILLATION\")\n",
    "print(\"=\" * 50)\n",
    "train_student_with_distillation(student, teacher, train_loader, val_loader, \n",
    "                                epochs=STUDENT_EPOCHS, lr=LEARNING_RATE, \n",
    "                                alpha=DISTILL_ALPHA, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35588233",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cea83ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T18:29:56.636620Z",
     "iopub.status.idle": "2025-11-15T18:29:56.636995Z",
     "shell.execute_reply": "2025-11-15T18:29:56.636839Z",
     "shell.execute_reply.started": "2025-11-15T18:29:56.636824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation utilities defined.\n"
     ]
    }
   ],
   "source": [
    "def predict_model(model, loader, device='cpu', return_buyer=False):\n",
    "    \"\"\"Generate predictions from model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    buyers_true = []\n",
    "    buyers_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_cat = batch.get('cat', None).to(device) if 'cat' in batch else None\n",
    "            x_num = batch.get('num', None).to(device) if 'num' in batch else None\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Check if model has dual heads (teacher) or single head (student)\n",
    "            out = model(x_cat, x_num)\n",
    "            if isinstance(out, tuple):  # Teacher model\n",
    "                pred, buyer_pred = out\n",
    "                if return_buyer:\n",
    "                    buyers_pred.append(buyer_pred.cpu().numpy())\n",
    "            else:  # Student model\n",
    "                pred = out\n",
    "            \n",
    "            preds.append(pred.cpu().numpy())\n",
    "            trues.append(y.cpu().numpy())\n",
    "            \n",
    "            if 'buyer' in batch:\n",
    "                buyers_true.append(batch['buyer'].numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds).ravel()\n",
    "    trues = np.concatenate(trues).ravel()\n",
    "    buyers_true = np.concatenate(buyers_true).ravel() if buyers_true else None\n",
    "    buyers_pred = np.concatenate(buyers_pred).ravel() if buyers_pred else None\n",
    "    \n",
    "    return preds, trues, buyers_true, buyers_pred\n",
    "\n",
    "def msle_from_log_predictions(pred_log1p, true_log1p):\n",
    "    \"\"\"Calculate MSLE from log-transformed predictions.\"\"\"\n",
    "    pred = np.expm1(pred_log1p)\n",
    "    true = np.expm1(true_log1p)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "    true = np.clip(true, 0, None)\n",
    "    return mean_squared_log_error(true, pred)\n",
    "\n",
    "print(\"Evaluation utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d6a0ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T18:29:56.638615Z",
     "iopub.status.idle": "2025-11-15T18:29:56.639040Z",
     "shell.execute_reply": "2025-11-15T18:29:56.638844Z",
     "shell.execute_reply.started": "2025-11-15T18:29:56.638825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEACHER MODEL EVALUATION\n",
      "==================================================\n",
      "Teacher MSLE: 0.213654\n",
      "Teacher Buyer AUC: 0.7588\n",
      "\n",
      "==================================================\n",
      "STUDENT MODEL EVALUATION\n",
      "==================================================\n",
      "Teacher MSLE: 0.213654\n",
      "Teacher Buyer AUC: 0.7588\n",
      "\n",
      "==================================================\n",
      "STUDENT MODEL EVALUATION\n",
      "==================================================\n",
      "Student MSLE: 0.206512\n",
      "\n",
      "Baseline (all zeros) MSLE: 0.228072\n",
      "\n",
      "==================================================\n",
      "COMPARISON\n",
      "==================================================\n",
      "Teacher improvement: 6.32%\n",
      "Student improvement: 9.45%\n",
      "Student vs Teacher gap: -3.34%\n",
      "\n",
      "Teacher predictions:\n",
      "  Mean: 0.7155, Median: 0.0591, Max: 18096.1328\n",
      "  % Non-zero: 60.12%\n",
      "\n",
      "Student predictions:\n",
      "  Mean: 0.0655, Median: 0.0530, Max: 2.6761\n",
      "  % Non-zero: 64.92%\n",
      "Student MSLE: 0.206512\n",
      "\n",
      "Baseline (all zeros) MSLE: 0.228072\n",
      "\n",
      "==================================================\n",
      "COMPARISON\n",
      "==================================================\n",
      "Teacher improvement: 6.32%\n",
      "Student improvement: 9.45%\n",
      "Student vs Teacher gap: -3.34%\n",
      "\n",
      "Teacher predictions:\n",
      "  Mean: 0.7155, Median: 0.0591, Max: 18096.1328\n",
      "  % Non-zero: 60.12%\n",
      "\n",
      "Student predictions:\n",
      "  Mean: 0.0655, Median: 0.0530, Max: 2.6761\n",
      "  % Non-zero: 64.92%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Teacher\n",
    "print(\"=\" * 50)\n",
    "print(\"TEACHER MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "teacher_preds_log, teacher_trues_log, buyers_true, buyers_pred = predict_model(\n",
    "    teacher, val_loader, device=DEVICE, return_buyer=True\n",
    ")\n",
    "\n",
    "teacher_msle = msle_from_log_predictions(teacher_preds_log, teacher_trues_log)\n",
    "print(f\"Teacher MSLE: {teacher_msle:.6f}\")\n",
    "\n",
    "if buyers_pred is not None:\n",
    "    buyer_auc = roc_auc_score(buyers_true, buyers_pred)\n",
    "    print(f\"Teacher Buyer AUC: {buyer_auc:.4f}\")\n",
    "\n",
    "# Evaluate Student\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STUDENT MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "student_preds_log, student_trues_log, _, _ = predict_model(\n",
    "    student, val_loader, device=DEVICE\n",
    ")\n",
    "\n",
    "student_msle = msle_from_log_predictions(student_preds_log, student_trues_log)\n",
    "print(f\"Student MSLE: {student_msle:.6f}\")\n",
    "\n",
    "# Baseline (all zeros)\n",
    "baseline_msle = msle_from_log_predictions(\n",
    "    np.zeros_like(student_trues_log), student_trues_log\n",
    ")\n",
    "print(f\"\\nBaseline (all zeros) MSLE: {baseline_msle:.6f}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Teacher improvement: {((baseline_msle - teacher_msle) / baseline_msle * 100):.2f}%\")\n",
    "print(f\"Student improvement: {((baseline_msle - student_msle) / baseline_msle * 100):.2f}%\")\n",
    "print(f\"Student vs Teacher gap: {((student_msle - teacher_msle) / teacher_msle * 100):.2f}%\")\n",
    "\n",
    "# Distribution stats\n",
    "teacher_preds = np.expm1(teacher_preds_log)\n",
    "student_preds = np.expm1(student_preds_log)\n",
    "\n",
    "print(\"\\nTeacher predictions:\")\n",
    "print(f\"  Mean: {teacher_preds.mean():.4f}, Median: {np.median(teacher_preds):.4f}, Max: {teacher_preds.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(teacher_preds > 0).mean() * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nStudent predictions:\")\n",
    "print(f\"  Mean: {student_preds.mean():.4f}, Median: {np.median(student_preds):.4f}, Max: {student_preds.max():.4f}\")\n",
    "print(f\"  % Non-zero: {(student_preds > 0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4db1f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Este notebook combina:\n",
    "1. **Carga de datos robusta** del `simplified_model_comparison.ipynb` (manejo de parquet con Dask, sampling, preprocesado)\n",
    "2. **Modelos de deep learning** (Teacher-Student distillation en PyTorch)\n",
    "\n",
    "### Ventajas del Student Model:\n",
    "- **~50% menos parámetros** que el teacher\n",
    "- **Inferencia más rápida** para producción\n",
    "- **Aprende del teacher** (soft targets) además de los labels reales\n",
    "\n",
    "### Para producción:\n",
    "- Exportar el student a ONNX: `torch.onnx.export(student, ...)`\n",
    "- Aplicar quantization para reducir tamaño y acelerar más\n",
    "- Usar solo el student model (descartar teacher)\n",
    "\n",
    "### Próximos pasos:\n",
    "- Ajustar hiperparámetros (learning rate, arquitectura, alpha)\n",
    "- Probar diferentes embedding dimensions\n",
    "- Añadir más épocas si hay suficiente memoria/tiempo\n",
    "- Implementar early stopping basado en validation loss"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14453560,
     "sourceId": 120867,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
