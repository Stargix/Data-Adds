{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d88f0f1",
   "metadata": {},
   "source": [
    "# üîß VERSI√ìN CORREGIDA V3\n",
    "\n",
    "## ‚úÖ Correcciones Cr√≠ticas Aplicadas\n",
    "\n",
    "### 1. **Arquitectura del Modelo**\n",
    "- ‚úÖ Revenue head ahora usa `Softplus()` para garantizar outputs positivos en log-space\n",
    "- ‚úÖ Heads independientes (buyer y revenue) sin condicionamiento que causaba problemas\n",
    "- ‚úÖ Arquitectura m√°s simple (256‚Üí128) para evitar overfitting con datos limitados\n",
    "\n",
    "### 2. **Loss Function**\n",
    "- ‚úÖ Loss consistente en log-space (no m√°s mezcla de escalas)\n",
    "- ‚úÖ Revenue loss solo en buyers (masked correctamente)\n",
    "- ‚úÖ Eliminado MSLE aproximado que confund√≠a al modelo\n",
    "- ‚úÖ Balanced weights: 40% buyer + 60% revenue\n",
    "\n",
    "### 3. **Distillation**\n",
    "- ‚úÖ Student aprende de AMBOS heads del teacher\n",
    "- ‚úÖ Soft loss en buyer Y revenue (no solo uno)\n",
    "- ‚úÖ Alpha=0.7 (m√°s peso a ground truth que a teacher)\n",
    "- ‚úÖ Gradient clipping para estabilidad\n",
    "\n",
    "### 4. **Data & Training**\n",
    "- ‚úÖ Sample fraction aumentado: 10% ‚Üí 25% (m√°s datos)\n",
    "- ‚úÖ √âpocas aumentadas: 5 ‚Üí 8\n",
    "- ‚úÖ Embeddings con espacio para \"unknown\" categories\n",
    "- ‚úÖ AdamW + weight decay para regularizaci√≥n\n",
    "\n",
    "### 5. **Predicci√≥n**\n",
    "- ‚úÖ Conversi√≥n correcta: log-space ‚Üí original scale\n",
    "- ‚úÖ Predicci√≥n final: `P(buyer) * revenue_if_buyer`\n",
    "- ‚úÖ M√©tricas comprehensivas (MSLE, AUC, MAE, distribuciones)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE:** Ejecuta TODAS las celdas en orden despu√©s de esta correcci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db57666",
   "metadata": {},
   "source": [
    "## üéØ Pr√≥ximos Pasos para Mejorar Resultados\n",
    "\n",
    "### Mejoras Inmediatas (F√°cil)\n",
    "1. **M√°s datos:** Aumentar `TRAIN_SAMPLE_FRAC` de 0.25 a 0.5 o m√°s\n",
    "2. **Feature engineering:** Usar columnas como `buyer_d1`, `iap_revenue_d14` como features (no descartarlas)\n",
    "3. **Balanceo de clases:** Hacer oversampling de buyers (son minor√≠a)\n",
    "\n",
    "### Mejoras Avanzadas (Requieren m√°s trabajo)\n",
    "1. **Ensemble:** Combinar predicciones de teacher + student (promedio ponderado)\n",
    "2. **Cross-validation:** 5-fold CV para mejor estimaci√≥n de performance\n",
    "3. **Hyperparameter tuning:** Grid search sobre learning rate, dropout, arquitectura\n",
    "4. **Feature selection:** Eliminar features ruidosas o redundantes\n",
    "5. **Quantile regression:** Predecir percentiles en vez de media (mejor para distribuci√≥n sesgada)\n",
    "\n",
    "### Debugging\n",
    "- Si MSLE sigue alto: Verificar distribuci√≥n de predicciones vs ground truth\n",
    "- Si AUC buyer bajo: A√±adir m√°s features relacionadas con comportamiento de usuario\n",
    "- Si overfitting: Aumentar dropout o weight decay\n",
    "\n",
    "**Ejecuta el notebook y revisa las m√©tricas en validaci√≥n!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4fd17",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6da3c1c-e84a-4f43-a910-7cdeb3f2ef27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.750345Z",
     "iopub.status.busy": "2025-11-15T18:28:42.749872Z",
     "iopub.status.idle": "2025-11-15T18:28:42.760122Z",
     "shell.execute_reply": "2025-11-15T18:28:42.757625Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.750289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import gc\n",
    "\n",
    "TRAIN_PATH = \"./train/train\"\n",
    "TEST_PATH  = \"./test/test\"\n",
    "\n",
    "TARGET_COL = \"iap_revenue_d7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c00cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.762530Z",
     "iopub.status.busy": "2025-11-15T18:28:42.761985Z",
     "iopub.status.idle": "2025-11-15T18:28:42.812139Z",
     "shell.execute_reply": "2025-11-15T18:28:42.810830Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.762492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Sample fraction: 0.1\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "TARGET_COL = \"iap_revenue_d7\"\n",
    "TRAIN_SAMPLE_FRAC = 0.25  # ‚úÖ Aumentado de 0.10 a 0.25 para m√°s datos\n",
    "\n",
    "# PyTorch settings\n",
    "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 256\n",
    "TEACHER_EPOCHS = 8  # ‚úÖ M√°s √©pocas\n",
    "STUDENT_EPOCHS = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "DISTILL_ALPHA = 0.7  # ‚úÖ M√°s peso al hard loss (ground truth)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Sample fraction: {TRAIN_SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1861a",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANTE:** Si ves errores de CUDA como `cudaErrorUnknown`:\n",
    "\n",
    "1. **REINICIA EL KERNEL** ‚Üí Bot√≥n \"Restart\" en la barra superior o `Ctrl+Shift+P` ‚Üí \"Restart Kernel\"\n",
    "2. Ejecuta todas las celdas desde el principio\n",
    "3. NO intentes ejecutar celdas individuales despu√©s de un error de CUDA\n",
    "\n",
    "Esto sucede porque CUDA entra en un estado corrupto despu√©s de un error y necesita reiniciarse completamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270e83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CUDA available: NVIDIA GeForce RTX 5070 Laptop GPU\n",
      "  Memory: 0.00 GB allocated\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: If you get CUDA errors, RESTART THE KERNEL first!\n",
    "# This cell checks CUDA health\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(f\"‚úì CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB allocated\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† CUDA ERROR DETECTED: {e}\")\n",
    "        print(\"  ‚Üí PLEASE RESTART THE KERNEL (Ctrl+Shift+P ‚Üí 'Restart Kernel')\")\n",
    "        print(\"  ‚Üí Then run all cells from the beginning\")\n",
    "        raise RuntimeError(\"CUDA is in a corrupted state. Restart kernel required.\")\n",
    "else:\n",
    "    print(\"‚Ñπ CUDA not available, using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9232f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.817425Z",
     "iopub.status.busy": "2025-11-15T18:28:42.816128Z",
     "iopub.status.idle": "2025-11-15T18:28:42.857537Z",
     "shell.execute_reply": "2025-11-15T18:28:42.855818Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.817391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7929983ab4c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from sklearn.metrics import mean_squared_log_error, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Reproducibility\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)\n",
    "\n",
    "# Set torch seed with error handling\n",
    "try:\n",
    "    torch.manual_seed(RSEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RSEED)\n",
    "    print(\"‚úì Libraries imported successfully\")\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"‚ö† CRITICAL: CUDA ERROR DURING INITIALIZATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nSOLUTION:\")\n",
    "        print(\"1. Click 'Restart' button in the notebook toolbar\")\n",
    "        print(\"2. Or: Ctrl+Shift+P ‚Üí type 'Restart Kernel'\")\n",
    "        print(\"3. Run all cells from the beginning\")\n",
    "        print(\"=\" * 60)\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89432efb",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a6b1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.859861Z",
     "iopub.status.busy": "2025-11-15T18:28:42.859589Z",
     "iopub.status.idle": "2025-11-15T18:28:42.901972Z",
     "shell.execute_reply": "2025-11-15T18:28:42.900449Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.859842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Columnas problem√°ticas (listas/dicts) que se ignoran\n",
    "IGNORE_BIG_COLS = [\n",
    "    \"bundles_ins\", \"user_bundles\", \"user_bundles_l28d\",\n",
    "    \"city_hist\", \"country_hist\", \"region_hist\",\n",
    "    \"dev_language_hist\", \"dev_osv_hist\",\n",
    "    \"bcat\", \"bcat_bottom_taxonomy\",\n",
    "    \"bundles_cat\", \"bundles_cat_bottom_taxonomy\",\n",
    "    \"first_request_ts_bundle\", \"first_request_ts_category_bottom_taxonomy\",\n",
    "    \"last_buy_ts_bundle\", \"last_buy_ts_category\",\n",
    "    \"last_install_ts_bundle\", \"last_install_ts_category\",\n",
    "    \"advertiser_actions_action_count\", \"advertiser_actions_action_last_timestamp\",\n",
    "    \"user_actions_bundles_action_count\", \"user_actions_bundles_action_last_timestamp\",\n",
    "    \"new_bundles\",\n",
    "    \"whale_users_bundle_num_buys_prank\", \"whale_users_bundle_revenue_prank\",\n",
    "    \"whale_users_bundle_total_num_buys\", \"whale_users_bundle_total_revenue\",\n",
    "]\n",
    "\n",
    "LABEL_COLS = [\n",
    "    \"buyer_d1\", \"buyer_d7\", \"buyer_d14\", \"buyer_d28\",\n",
    "    \"buy_d7\", \"buy_d14\", \"buy_d28\",\n",
    "    \"iap_revenue_d7\", \"iap_revenue_d14\", \"iap_revenue_d28\",\n",
    "    \"registration\",\n",
    "    \"retention_d1_to_d7\", \"retention_d3_to_d7\", \"retention_d7_to_d14\",\n",
    "    \"retention_d1\", \"retention_d3\", \"retention_d7\",\n",
    "]\n",
    "\n",
    "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Downcast numeric columns to save memory.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        elif col_type == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "def detect_listlike_columns(df: pd.DataFrame, cols=None):\n",
    "    \"\"\"Detect columns containing lists or dicts.\"\"\"\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    listlike = []\n",
    "    for c in cols:\n",
    "        sample_vals = df[c].head(100)\n",
    "        if sample_vals.apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "            listlike.append(c)\n",
    "    return listlike\n",
    "\n",
    "def preprocess_train_valid(X_train, X_valid, num_cols, cat_cols):\n",
    "    \"\"\"Preprocess train and validation sets.\"\"\"\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    \n",
    "    # Numeric: fill NaN with 0\n",
    "    for c in num_cols:\n",
    "        X_train[c] = X_train[c].fillna(0)\n",
    "        X_valid[c] = X_valid[c].fillna(0)\n",
    "    \n",
    "    # Categorical: convert to strings and encode as integers\n",
    "    cat_mappings = {}\n",
    "    for c in cat_cols:\n",
    "        X_train[c] = X_train[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "        \n",
    "        # Create mapping\n",
    "        cats = X_train[c].cat.categories\n",
    "        cat_mappings[c] = {cat: i for i, cat in enumerate(cats)}\n",
    "        \n",
    "        # Encode train\n",
    "        X_train[c] = X_train[c].cat.codes\n",
    "        \n",
    "        # Encode valid (handle unseen categories)\n",
    "        X_valid[c] = X_valid[c].astype(\"object\").fillna(\"unknown\").astype(str)\n",
    "        X_valid[c] = X_valid[c].map(cat_mappings[c]).fillna(-1).astype(np.int32)\n",
    "    \n",
    "    return X_train, X_valid, cat_mappings\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358814e",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f345fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:28:42.904727Z",
     "iopub.status.busy": "2025-11-15T18:28:42.903990Z",
     "iopub.status.idle": "2025-11-15T18:29:25.618077Z",
     "shell.execute_reply": "2025-11-15T18:29:25.617044Z",
     "shell.execute_reply.started": "2025-11-15T18:28:42.904693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 21 out of 144 train files\n",
      "Loading train data...\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "Train loaded: (271487, 56), Memory: 0.40 GB\n",
      "\n",
      "Loading validation data...\n",
      "\n",
      "Loading validation data...\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "Valid loaded: (28373, 56), Memory: 0.04 GB\n",
      "\n",
      "‚úì Data loaded successfully\n",
      "\n",
      "‚úì Data loaded successfully\n",
      "Total memory: ~0.44 GB\n",
      "Total memory: ~0.44 GB\n"
     ]
    }
   ],
   "source": [
    "# Train: Oct 1-5, Valid: Oct 6\n",
    "filters_train = [(\"datetime\", \">=\", \"2025-10-01-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-06-00-00\")]\n",
    "filters_valid = [(\"datetime\", \">=\", \"2025-10-06-00-00\"),\n",
    "                 (\"datetime\", \"<\",  \"2025-10-07-00-00\")]\n",
    "\n",
    "# Get list of parquet files\n",
    "parquet_files_all = glob(os.path.join(TRAIN_PATH, '**/part-*.parquet'), recursive=True)\n",
    "\n",
    "# Reduce number of files for faster training\n",
    "num_files_train = max(1, int(len(parquet_files_all) * 0.15))\n",
    "parquet_files_train = parquet_files_all[:num_files_train]\n",
    "\n",
    "print(f\"Using {num_files_train} out of {len(parquet_files_all)} train files\")\n",
    "\n",
    "# Columns to drop early\n",
    "cols_to_drop_early = IGNORE_BIG_COLS + [\"row_id\", \"datetime\"]\n",
    "\n",
    "# Load TRAIN\n",
    "print(\"Loading train data...\")\n",
    "dd_train = dd.read_parquet(\n",
    "    parquet_files_train, \n",
    "    filters=filters_train,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Drop heavy columns BEFORE compute\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_train.columns]\n",
    "dd_train = dd_train.drop(columns=existing_cols)\n",
    "\n",
    "# Sample in Dask\n",
    "train_sample = dd_train.sample(frac=TRAIN_SAMPLE_FRAC, random_state=RSEED).compute()\n",
    "train_sample = reduce_memory(train_sample)\n",
    "\n",
    "print(f\"Train loaded: {train_sample.shape}, Memory: {train_sample.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Clean memory\n",
    "del dd_train\n",
    "gc.collect()\n",
    "\n",
    "# Load VALID\n",
    "print(\"\\nLoading validation data...\")\n",
    "dd_valid = dd.read_parquet(\n",
    "    parquet_files_train,\n",
    "    filters=filters_valid,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "existing_cols = [c for c in cols_to_drop_early if c in dd_valid.columns]\n",
    "dd_valid = dd_valid.drop(columns=existing_cols)\n",
    "\n",
    "# Sample less in validation\n",
    "valid_df = dd_valid.sample(frac=min(0.5, TRAIN_SAMPLE_FRAC), random_state=RSEED).compute()\n",
    "valid_df = reduce_memory(valid_df)\n",
    "\n",
    "print(f\"Valid loaded: {valid_df.shape}, Memory: {valid_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "del dd_valid\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úì Data loaded successfully\")\n",
    "print(f\"Total memory: ~{(train_sample.memory_usage(deep=True).sum() + valid_df.memory_usage(deep=True).sum()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045041fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:25.619531Z",
     "iopub.status.busy": "2025-11-15T18:29:25.619190Z",
     "iopub.status.idle": "2025-11-15T18:29:26.982573Z",
     "shell.execute_reply": "2025-11-15T18:29:26.981315Z",
     "shell.execute_reply.started": "2025-11-15T18:29:25.619510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buyer ratio in train: 0.0320\n",
      "Buyer ratio in valid: 0.0330\n",
      "Removing 14 list-like columns: ['avg_daily_sessions', 'avg_duration', 'cpm', 'cpm_pct_rk', 'ctr', 'ctr_pct_rk', 'hour_ratio', 'iap_revenue_usd_bundle', 'iap_revenue_usd_category', 'iap_revenue_usd_category_bottom_taxonomy', 'num_buys_bundle', 'num_buys_category', 'num_buys_category_bottom_taxonomy', 'rwd_prank']\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "Features: 26 (11 numeric, 15 categorical)\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n",
      "Data prepared: X_train (271487, 26), X_valid (28373, 26)\n"
     ]
    }
   ],
   "source": [
    "# Extract targets\n",
    "y_train = train_sample[TARGET_COL].values\n",
    "y_valid = valid_df[TARGET_COL].values\n",
    "\n",
    "# Extract buyer labels\n",
    "y_train_buyer = train_sample[\"buyer_d7\"].values\n",
    "y_valid_buyer = valid_df[\"buyer_d7\"].values\n",
    "\n",
    "print(f\"Buyer ratio in train: {y_train_buyer.mean():.4f}\")\n",
    "print(f\"Buyer ratio in valid: {y_valid_buyer.mean():.4f}\")\n",
    "\n",
    "# Target transform: log1p for stability (MSLE)\n",
    "y_train_log = np.log1p(y_train.clip(min=0.0))\n",
    "y_valid_log = np.log1p(y_valid.clip(min=0.0))\n",
    "\n",
    "# Prepare features\n",
    "cols_to_drop = [\"row_id\", \"datetime\"] + LABEL_COLS\n",
    "feature_cols = [c for c in train_sample.columns if c not in cols_to_drop]\n",
    "\n",
    "X_train = train_sample[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "\n",
    "# Detect and remove list-like columns\n",
    "listlike_cols = detect_listlike_columns(X_train, cols=feature_cols)\n",
    "print(f\"Removing {len(listlike_cols)} list-like columns: {listlike_cols}\")\n",
    "X_train = X_train.drop(columns=listlike_cols)\n",
    "X_valid = X_valid.drop(columns=listlike_cols)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Features: {len(X_train.columns)} ({len(num_cols)} numeric, {len(cat_cols)} categorical)\")\n",
    "\n",
    "# Preprocess\n",
    "X_train_prep, X_valid_prep, cat_mappings = preprocess_train_valid(X_train, X_valid, num_cols, cat_cols)\n",
    "print(f\"Data prepared: X_train {X_train_prep.shape}, X_valid {X_valid_prep.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700f8a3",
   "metadata": {},
   "source": [
    "## PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84898c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:26.983944Z",
     "iopub.status.busy": "2025-11-15T18:29:26.983612Z",
     "iopub.status.idle": "2025-11-15T18:29:27.049211Z",
     "shell.execute_reply": "2025-11-15T18:29:27.047755Z",
     "shell.execute_reply.started": "2025-11-15T18:29:26.983900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, y, y_buyer=None, emb_sizes=None):\n",
    "        if len(cat_cols) > 0:\n",
    "            cat_data = df[cat_cols].values.astype(np.int64)\n",
    "            if emb_sizes is not None:\n",
    "                # Map unknown categories (-1) to a special index\n",
    "                for i in range(cat_data.shape[1]):\n",
    "                    n_categories = emb_sizes[i][0]\n",
    "                    # -1 (unknown) -> n_categories - 1 (last embedding reserved for unknown)\n",
    "                    cat_data[cat_data[:, i] == -1, i] = n_categories - 1\n",
    "                    # Clamp valid categories to [0, n_categories - 2]\n",
    "                    cat_data[:, i] = np.clip(cat_data[:, i], 0, n_categories - 1)\n",
    "            self.cat = cat_data\n",
    "        else:\n",
    "            self.cat = None\n",
    "        self.num = df[num_cols].values.astype(np.float32) if len(num_cols) > 0 else None\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.buyer = y_buyer.astype(np.float32) if y_buyer is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        if self.cat is not None:\n",
    "            item['cat'] = torch.tensor(self.cat[idx], dtype=torch.long)\n",
    "        if self.num is not None:\n",
    "            item['num'] = torch.tensor(self.num[idx], dtype=torch.float32)\n",
    "        item['y'] = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        if self.buyer is not None:\n",
    "            item['buyer'] = torch.tensor(self.buyer[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "print(\"TabularDataset class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb785ede",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding sizes calculated: 15 categorical features\n",
      "Sample sizes (categories, dim): [(459, 45), (24, 2), (58, 5), (92, 9), (5895, 50)]...\n",
      "\n",
      "Creating datasets with proper index clamping...\n",
      "‚úì Dataloaders ready. Batches per epoch: train=1061, val=111\n"
     ]
    }
   ],
   "source": [
    "# Calculate embedding sizes BEFORE creating datasets\n",
    "def get_embedding_sizes(cat_cols, cat_mappings, max_emb_dim=50):\n",
    "    \"\"\"Calculate embedding sizes for categorical features.\"\"\"\n",
    "    emb_sizes = []\n",
    "    for c in cat_cols:\n",
    "        n_unique = len(cat_mappings[c]) + 2  # +1 for unknown, +1 for padding\n",
    "        emb_dim = min(max(1, n_unique // 10), max_emb_dim)\n",
    "        emb_sizes.append((n_unique, emb_dim))\n",
    "    return emb_sizes\n",
    "\n",
    "emb_sizes = get_embedding_sizes(cat_cols, cat_mappings, max_emb_dim=50)\n",
    "print(f\"Embedding sizes calculated: {len(emb_sizes)} categorical features\")\n",
    "print(f\"Sample sizes (categories, dim): {emb_sizes[:5]}...\")  # Show first 5\n",
    "\n",
    "# NOW create datasets WITH embedding sizes to clamp indices properly\n",
    "print(\"\\nCreating datasets with proper index clamping...\")\n",
    "train_ds = TabularDataset(X_train_prep, cat_cols, num_cols, y_train_log, y_train_buyer, emb_sizes=emb_sizes)\n",
    "val_ds = TabularDataset(X_valid_prep, cat_cols, num_cols, y_valid_log, y_valid_buyer, emb_sizes=emb_sizes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"‚úì Dataloaders ready. Batches per epoch: train={len(train_loader)}, val={len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386c042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:27.051175Z",
     "iopub.status.busy": "2025-11-15T18:29:27.050677Z",
     "iopub.status.idle": "2025-11-15T18:29:27.069804Z",
     "shell.execute_reply": "2025-11-15T18:29:27.068368Z",
     "shell.execute_reply.started": "2025-11-15T18:29:27.051135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined.\n"
     ]
    }
   ],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Teacher model V3 - ARQUITECTURA CORREGIDA\n",
    "    \n",
    "    Mejoras cr√≠ticas:\n",
    "    1. Predice log(revenue) directamente (escala correcta)\n",
    "    2. Usa Softplus para asegurar valores positivos\n",
    "    3. Arquitectura m√°s simple para evitar overfitting\n",
    "    4. Heads separados e independientes\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_sizes, num_len):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(categories, dim) for categories, dim in emb_sizes])\n",
    "        emb_dim_sum = sum([dim for _, dim in emb_sizes]) if len(emb_sizes) > 0 else 0\n",
    "        input_dim = emb_dim_sum + (num_len if num_len > 0 else 0)\n",
    "        \n",
    "        # Red principal M√ÅS SIMPLE (256->128)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Menos dropout\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # Buyer head: predice P(buyer=1)\n",
    "        self.buyer_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "            # NO sigmoid aqu√≠, lo aplicamos en loss/forward\n",
    "        )\n",
    "        \n",
    "        # Revenue head: predice log(revenue) dado que es buyer\n",
    "        # CR√çTICO: Output debe ser positivo (es log-space)\n",
    "        self.revenue_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()  # ‚úÖ Garantiza output >= 0 para log(revenue)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num, return_components=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            return_components: Si True, retorna (buyer_logit, log_revenue, prob_buyer)\n",
    "                              Si False, retorna predicci√≥n final\n",
    "        \"\"\"\n",
    "        if x_cat is not None and len(self.embs) > 0:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + ([x_num] if x_num is not None else []), dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        \n",
    "        feat = self.net(x)\n",
    "        \n",
    "        # Buyer prediction (logit)\n",
    "        buyer_logit = self.buyer_head(feat).view(-1)\n",
    "        prob_buyer = torch.sigmoid(buyer_logit)\n",
    "        \n",
    "        # Revenue prediction (log-space, positivo por Softplus)\n",
    "        log_revenue = self.revenue_head(feat).view(-1)\n",
    "        \n",
    "        if return_components:\n",
    "            return buyer_logit, log_revenue, prob_buyer\n",
    "        \n",
    "        # Predicci√≥n final: P(buyer) * revenue\n",
    "        # En escala original: expm1(log_revenue) = revenue - 1\n",
    "        # Pero durante train usamos log-space directamente\n",
    "        return buyer_logit, log_revenue\n",
    "\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Student model V3 - ARQUITECTURA CORREGIDA\n",
    "    \n",
    "    Mejoras:\n",
    "    1. Mimics teacher structure pero m√°s peque√±o\n",
    "    2. Embeddings reducidos a la mitad\n",
    "    3. Red m√°s compacta (128->64)\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_sizes, num_len):\n",
    "        super().__init__()\n",
    "        # Embeddings reducidos\n",
    "        small_embs = [(n, max(1, d // 2)) for n, d in emb_sizes]\n",
    "        self.embs = nn.ModuleList([nn.Embedding(categories, dim) for categories, dim in small_embs])\n",
    "        emb_dim_sum = sum([dim for _, dim in small_embs]) if len(small_embs) > 0 else 0\n",
    "        input_dim = emb_dim_sum + (num_len if num_len > 0 else 0)\n",
    "        \n",
    "        # Red compacta\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Buyer head\n",
    "        self.buyer_head = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # Revenue head\n",
    "        self.revenue_head = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Softplus()  # ‚úÖ Igual que teacher\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num, return_components=False):\n",
    "        if x_cat is not None and len(self.embs) > 0:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + ([x_num] if x_num is not None else []), dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        \n",
    "        feat = self.net(x)\n",
    "        \n",
    "        buyer_logit = self.buyer_head(feat).view(-1)\n",
    "        prob_buyer = torch.sigmoid(buyer_logit)\n",
    "        \n",
    "        log_revenue = self.revenue_head(feat).view(-1)\n",
    "        \n",
    "        if return_components:\n",
    "            return buyer_logit, log_revenue, prob_buyer\n",
    "        \n",
    "        return buyer_logit, log_revenue\n",
    "\n",
    "\n",
    "print(\"‚úì Model classes defined (V3 - CORRECTED)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "834f2aea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:27.071444Z",
     "iopub.status.busy": "2025-11-15T18:29:27.071088Z",
     "iopub.status.idle": "2025-11-15T18:29:27.236503Z",
     "shell.execute_reply": "2025-11-15T18:29:27.235290Z",
     "shell.execute_reply.started": "2025-11-15T18:29:27.071421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher params: 1,316,168\n",
      "Student params: 504,709\n",
      "Compression ratio: 2.61x\n"
     ]
    }
   ],
   "source": [
    "# Initialize models (emb_sizes already calculated above)\n",
    "teacher = TeacherModel(emb_sizes, len(num_cols)).to(DEVICE)\n",
    "student = StudentModel(emb_sizes, len(num_cols)).to(DEVICE)\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
    "student_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Teacher params: {teacher_params:,}\")\n",
    "print(f\"Student params: {student_params:,}\")\n",
    "print(f\"Compression ratio: {teacher_params / student_params:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45318f67",
   "metadata": {},
   "source": [
    "## Train Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27109caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T18:29:27.238761Z",
     "iopub.status.busy": "2025-11-15T18:29:27.238398Z",
     "iopub.status.idle": "2025-11-15T18:29:56.633293Z",
     "shell.execute_reply": "2025-11-15T18:29:56.631729Z",
     "shell.execute_reply.started": "2025-11-15T18:29:27.238730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_teacher(model, train_loader, val_loader, epochs=5, lr=1e-3, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train teacher model V3 - LOSS FUNCTION CORREGIDA\n",
    "    \n",
    "    Cambios cr√≠ticos:\n",
    "    1. Loss consistente en log-space\n",
    "    2. Revenue loss solo en buyers (masked)\n",
    "    3. Weighted loss balanceado\n",
    "    4. No m√°s MSLE aproximado que confunde al modelo\n",
    "    \"\"\"\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()  # ‚úÖ Logits directamente\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # ========== TRAINING ==========\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_loss_buyer = 0.0\n",
    "        train_loss_revenue = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_cat = batch.get('cat', None)\n",
    "            x_num = batch.get('num', None)\n",
    "            y_log = batch['y']  # log1p(revenue)\n",
    "            buyer = batch['buyer']  # 0 o 1\n",
    "            \n",
    "            x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "            x_num = x_num.to(device) if x_num is not None else None\n",
    "            y_log = y_log.to(device)\n",
    "            buyer = buyer.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            buyer_logit, log_revenue = model(x_cat, x_num)\n",
    "            \n",
    "            # Loss 1: Buyer classification (todos los samples)\n",
    "            loss_buyer = bce_loss(buyer_logit, buyer)\n",
    "            \n",
    "            # Loss 2: Revenue regression (SOLO en buyers)\n",
    "            mask_buyers = buyer > 0.5\n",
    "            if mask_buyers.sum() > 0:\n",
    "                # log_revenue ya est√° en log-space por Softplus\n",
    "                # y_log es log1p(revenue)\n",
    "                # Ambos est√°n en log-scale, comparamos directamente\n",
    "                loss_revenue = mse_loss(log_revenue[mask_buyers], y_log[mask_buyers])\n",
    "            else:\n",
    "                loss_revenue = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Loss total: balanced\n",
    "            # Buyer classification es importante, revenue tambi√©n\n",
    "            loss = 0.4 * loss_buyer + 0.6 * loss_revenue\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ‚úÖ Gradient clipping\n",
    "            opt.step()\n",
    "            \n",
    "            train_loss_total += loss.item() * len(y_log)\n",
    "            train_loss_buyer += loss_buyer.item() * len(y_log)\n",
    "            train_loss_revenue += loss_revenue.item() * len(y_log)\n",
    "        \n",
    "        train_loss_total /= len(train_loader.dataset)\n",
    "        train_loss_buyer /= len(train_loader.dataset)\n",
    "        train_loss_revenue /= len(train_loader.dataset)\n",
    "        \n",
    "        # ========== VALIDATION ==========\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_loss_buyer = 0.0\n",
    "        val_loss_revenue = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_cat = batch.get('cat', None)\n",
    "                x_num = batch.get('num', None)\n",
    "                y_log = batch['y']\n",
    "                buyer = batch['buyer']\n",
    "                \n",
    "                x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "                x_num = x_num.to(device) if x_num is not None else None\n",
    "                y_log = y_log.to(device)\n",
    "                buyer = buyer.to(device)\n",
    "                \n",
    "                buyer_logit, log_revenue = model(x_cat, x_num)\n",
    "                \n",
    "                loss_buyer = bce_loss(buyer_logit, buyer)\n",
    "                \n",
    "                mask_buyers = buyer > 0.5\n",
    "                if mask_buyers.sum() > 0:\n",
    "                    loss_revenue = mse_loss(log_revenue[mask_buyers], y_log[mask_buyers])\n",
    "                else:\n",
    "                    loss_revenue = torch.tensor(0.0, device=device)\n",
    "                \n",
    "                loss = 0.4 * loss_buyer + 0.6 * loss_revenue\n",
    "                \n",
    "                val_loss_total += loss.item() * len(y_log)\n",
    "                val_loss_buyer += loss_buyer.item() * len(y_log)\n",
    "                val_loss_revenue += loss_revenue.item() * len(y_log)\n",
    "        \n",
    "        val_loss_total /= len(val_loader.dataset)\n",
    "        val_loss_buyer /= len(val_loader.dataset)\n",
    "        val_loss_revenue /= len(val_loader.dataset)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train - Total: {train_loss_total:.6f} | Buyer: {train_loss_buyer:.6f} | Revenue: {train_loss_revenue:.6f}\")\n",
    "        print(f\"  Valid - Total: {val_loss_total:.6f} | Buyer: {val_loss_buyer:.6f} | Revenue: {val_loss_revenue:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss_total < best_val_loss:\n",
    "            best_val_loss = val_loss_total\n",
    "            torch.save(model.state_dict(), 'teacher_model_v3.pt')\n",
    "            print(f\"  ‚úì New best model saved (val_loss: {val_loss_total:.6f})\")\n",
    "    \n",
    "    print(f\"\\n‚úì Training complete. Best val loss: {best_val_loss:.6f}\")\n",
    "    print(\"‚úì Best model saved to teacher_model_v3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "685b3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING TEACHER MODEL\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.966467, Reg: 2.169034, Buyer: 0.148178\n",
      "  Valid - Total: 0.987933\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.966467, Reg: 2.169034, Buyer: 0.148178\n",
      "  Valid - Total: 0.987933\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.859143, Reg: 1.906765, Buyer: 0.142937\n",
      "  Valid - Total: 0.842455\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.859143, Reg: 1.906765, Buyer: 0.142937\n",
      "  Valid - Total: 0.842455\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.817449, Reg: 1.802508, Buyer: 0.143129\n",
      "  Valid - Total: 0.830797\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.817449, Reg: 1.802508, Buyer: 0.143129\n",
      "  Valid - Total: 0.830797\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.789213, Reg: 1.732576, Buyer: 0.142440\n",
      "  Valid - Total: 0.843236\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.789213, Reg: 1.732576, Buyer: 0.142440\n",
      "  Valid - Total: 0.843236\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.774294, Reg: 1.696304, Buyer: 0.141585\n",
      "  Valid - Total: 0.808383\n",
      "\n",
      "‚úì Teacher saved to teacher_model_v2.pt\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.774294, Reg: 1.696304, Buyer: 0.141585\n",
      "  Valid - Total: 0.808383\n",
      "\n",
      "‚úì Teacher saved to teacher_model_v2.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the teacher model\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING TEACHER MODEL\")\n",
    "print(\"=\" * 50)\n",
    "train_teacher(teacher, train_loader, val_loader, epochs=TEACHER_EPOCHS, lr=LEARNING_RATE, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb1e53",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43be0a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T18:29:56.634383Z",
     "iopub.status.idle": "2025-11-15T18:29:56.634731Z",
     "shell.execute_reply": "2025-11-15T18:29:56.634602Z",
     "shell.execute_reply.started": "2025-11-15T18:29:56.634588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING STUDENT WITH DISTILLATION\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.934213, Hard: 0.689769, Soft: 1.300881\n",
      "  Valid - MSE: 0.881699\n",
      "Epoch 1/5\n",
      "  Train - Total: 0.934213, Hard: 0.689769, Soft: 1.300881\n",
      "  Valid - MSE: 0.881699\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.911937, Hard: 0.704582, Soft: 1.222970\n",
      "  Valid - MSE: 1.892810\n",
      "Epoch 2/5\n",
      "  Train - Total: 0.911937, Hard: 0.704582, Soft: 1.222970\n",
      "  Valid - MSE: 1.892810\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.909223, Hard: 0.704558, Soft: 1.216220\n",
      "  Valid - MSE: 1.035643\n",
      "Epoch 3/5\n",
      "  Train - Total: 0.909223, Hard: 0.704558, Soft: 1.216220\n",
      "  Valid - MSE: 1.035643\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.906923, Hard: 0.704100, Soft: 1.211158\n",
      "  Valid - MSE: 1.047304\n",
      "Epoch 4/5\n",
      "  Train - Total: 0.906923, Hard: 0.704100, Soft: 1.211158\n",
      "  Valid - MSE: 1.047304\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.906184, Hard: 0.704108, Soft: 1.209299\n",
      "  Valid - MSE: 0.807849\n",
      "\n",
      "‚úì Student saved to student_model_v2.pt\n",
      "Epoch 5/5\n",
      "  Train - Total: 0.906184, Hard: 0.704108, Soft: 1.209299\n",
      "  Valid - MSE: 0.807849\n",
      "\n",
      "‚úì Student saved to student_model_v2.pt\n"
     ]
    }
   ],
   "source": [
    "def train_student_with_distillation(student, teacher, train_loader, val_loader, epochs=5, lr=1e-3, alpha=0.7, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train student V3 - DISTILLATION CORREGIDA\n",
    "    \n",
    "    Cambios:\n",
    "    1. Student aprende de ambos heads del teacher\n",
    "    2. Loss consistente en log-space\n",
    "    3. Distillation en buyer Y revenue\n",
    "    4. Alpha alto = m√°s peso a ground truth\n",
    "    \"\"\"\n",
    "    opt = torch.optim.AdamW(student.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    teacher.to(device)\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # ========== TRAINING ==========\n",
    "        student.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_loss_hard = 0.0\n",
    "        train_loss_soft = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_cat = batch.get('cat', None)\n",
    "            x_num = batch.get('num', None)\n",
    "            y_log = batch['y']\n",
    "            buyer = batch['buyer']\n",
    "            \n",
    "            x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "            x_num = x_num.to(device) if x_num is not None else None\n",
    "            y_log = y_log.to(device)\n",
    "            buyer = buyer.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Student predictions\n",
    "            s_buyer_logit, s_log_revenue = student(x_cat, x_num)\n",
    "            \n",
    "            # Teacher predictions (soft targets)\n",
    "            with torch.no_grad():\n",
    "                t_buyer_logit, t_log_revenue = teacher(x_cat, x_num)\n",
    "            \n",
    "            # ===== HARD LOSS (ground truth) =====\n",
    "            # Buyer classification\n",
    "            hard_buyer = bce_loss(s_buyer_logit, buyer)\n",
    "            \n",
    "            # Revenue regression (solo en buyers)\n",
    "            mask_buyers = buyer > 0.5\n",
    "            if mask_buyers.sum() > 0:\n",
    "                hard_revenue = mse_loss(s_log_revenue[mask_buyers], y_log[mask_buyers])\n",
    "            else:\n",
    "                hard_revenue = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            loss_hard = 0.4 * hard_buyer + 0.6 * hard_revenue\n",
    "            \n",
    "            # ===== SOFT LOSS (teacher knowledge) =====\n",
    "            # Distill both buyer and revenue predictions\n",
    "            soft_buyer = mse_loss(s_buyer_logit, t_buyer_logit.detach())\n",
    "            soft_revenue = mse_loss(s_log_revenue, t_log_revenue.detach())\n",
    "            \n",
    "            loss_soft = 0.4 * soft_buyer + 0.6 * soft_revenue\n",
    "            \n",
    "            # ===== COMBINED LOSS =====\n",
    "            # alpha alto = m√°s peso a ground truth\n",
    "            loss = alpha * loss_hard + (1.0 - alpha) * loss_soft\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            \n",
    "            train_loss_total += loss.item() * len(y_log)\n",
    "            train_loss_hard += loss_hard.item() * len(y_log)\n",
    "            train_loss_soft += loss_soft.item() * len(y_log)\n",
    "        \n",
    "        train_loss_total /= len(train_loader.dataset)\n",
    "        train_loss_hard /= len(train_loader.dataset)\n",
    "        train_loss_soft /= len(train_loader.dataset)\n",
    "        \n",
    "        # ========== VALIDATION ==========\n",
    "        student.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_loss_buyer = 0.0\n",
    "        val_loss_revenue = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_cat = batch.get('cat', None)\n",
    "                x_num = batch.get('num', None)\n",
    "                y_log = batch['y']\n",
    "                buyer = batch['buyer']\n",
    "                \n",
    "                x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "                x_num = x_num.to(device) if x_num is not None else None\n",
    "                y_log = y_log.to(device)\n",
    "                buyer = buyer.to(device)\n",
    "                \n",
    "                s_buyer_logit, s_log_revenue = student(x_cat, x_num)\n",
    "                \n",
    "                loss_buyer = bce_loss(s_buyer_logit, buyer)\n",
    "                \n",
    "                mask_buyers = buyer > 0.5\n",
    "                if mask_buyers.sum() > 0:\n",
    "                    loss_revenue = mse_loss(s_log_revenue[mask_buyers], y_log[mask_buyers])\n",
    "                else:\n",
    "                    loss_revenue = torch.tensor(0.0, device=device)\n",
    "                \n",
    "                loss = 0.4 * loss_buyer + 0.6 * loss_revenue\n",
    "                \n",
    "                val_loss_total += loss.item() * len(y_log)\n",
    "                val_loss_buyer += loss_buyer.item() * len(y_log)\n",
    "                val_loss_revenue += loss_revenue.item() * len(y_log)\n",
    "        \n",
    "        val_loss_total /= len(val_loader.dataset)\n",
    "        val_loss_buyer /= len(val_loader.dataset)\n",
    "        val_loss_revenue /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train - Total: {train_loss_total:.6f} | Hard: {train_loss_hard:.6f} | Soft: {train_loss_soft:.6f}\")\n",
    "        print(f\"  Valid - Total: {val_loss_total:.6f} | Buyer: {val_loss_buyer:.6f} | Revenue: {val_loss_revenue:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss_total < best_val_loss:\n",
    "            best_val_loss = val_loss_total\n",
    "            torch.save(student.state_dict(), 'student_model_v3.pt')\n",
    "            print(f\"  ‚úì New best model saved (val_loss: {val_loss_total:.6f})\")\n",
    "    \n",
    "    print(f\"\\n‚úì Training complete. Best val loss: {best_val_loss:.6f}\")\n",
    "    print(\"‚úì Best model saved to student_model_v3.pt\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING STUDENT WITH DISTILLATION\")\n",
    "print(\"=\" * 50)\n",
    "train_student_with_distillation(student, teacher, train_loader, val_loader, \n",
    "                                epochs=STUDENT_EPOCHS, lr=LEARNING_RATE, \n",
    "                                alpha=DISTILL_ALPHA, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35588233",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea83ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T18:29:56.636620Z",
     "iopub.status.idle": "2025-11-15T18:29:56.636995Z",
     "shell.execute_reply": "2025-11-15T18:29:56.636839Z",
     "shell.execute_reply.started": "2025-11-15T18:29:56.636824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation utilities defined.\n"
     ]
    }
   ],
   "source": [
    "def predict_model(model, loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate predictions from model V3 - PREDICCI√ìN CORREGIDA\n",
    "    \n",
    "    Retorna predicciones en escala original (revenue).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    all_buyers_true = []\n",
    "    all_buyers_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_cat = batch.get('cat', None)\n",
    "            x_num = batch.get('num', None)\n",
    "            y_log = batch['y']  # log1p(revenue)\n",
    "            \n",
    "            x_cat = x_cat.to(device) if x_cat is not None else None\n",
    "            x_num = x_num.to(device) if x_num is not None else None\n",
    "            \n",
    "            # Forward pass\n",
    "            buyer_logit, log_revenue = model(x_cat, x_num)\n",
    "            prob_buyer = torch.sigmoid(buyer_logit)\n",
    "            \n",
    "            # Predicci√≥n final en escala ORIGINAL\n",
    "            # log_revenue es log-scale (por Softplus)\n",
    "            # Convertir a revenue: expm1(log_revenue) ‚âà revenue\n",
    "            # Pero log_revenue NO es log1p, es solo un valor positivo\n",
    "            # Necesitamos tratarlo como log1p para ser consistente\n",
    "            \n",
    "            # CORRECCI√ìN: log_revenue sale del modelo como valor positivo\n",
    "            # Lo tratamos como log1p(revenue), entonces:\n",
    "            revenue_pred = torch.expm1(log_revenue)  # revenue - 1 + 1 = revenue\n",
    "            \n",
    "            # Predicci√≥n final: P(buyer) * revenue_if_buyer\n",
    "            final_pred = prob_buyer * revenue_pred\n",
    "            \n",
    "            # Almacenar\n",
    "            all_preds.append(final_pred.cpu().numpy())\n",
    "            all_buyers_pred.append(prob_buyer.cpu().numpy())\n",
    "            all_trues.append(torch.expm1(y_log).cpu().numpy())  # Convertir a escala original\n",
    "            \n",
    "            if 'buyer' in batch:\n",
    "                all_buyers_true.append(batch['buyer'].numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds)\n",
    "    trues = np.concatenate(all_trues)\n",
    "    buyers_pred = np.concatenate(all_buyers_pred)\n",
    "    buyers_true = np.concatenate(all_buyers_true) if all_buyers_true else None\n",
    "    \n",
    "    return preds, trues, buyers_true, buyers_pred\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device='cpu', model_name='Model'):\n",
    "    \"\"\"\n",
    "    Evaluate model and print comprehensive metrics.\n",
    "    \"\"\"\n",
    "    preds, trues, buyers_true, buyers_pred = predict_model(model, loader, device)\n",
    "    \n",
    "    # Clip predictions to avoid invalid MSLE\n",
    "    preds = np.clip(preds, 0, None)\n",
    "    trues = np.clip(trues, 0, None)\n",
    "    \n",
    "    # MSLE\n",
    "    msle = mean_squared_log_error(trues, preds)\n",
    "    \n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(preds - trues))\n",
    "    \n",
    "    # Buyer metrics (si disponible)\n",
    "    if buyers_true is not None:\n",
    "        auc = roc_auc_score(buyers_true, buyers_pred)\n",
    "        \n",
    "        # Accuracy con threshold 0.5\n",
    "        buyer_pred_binary = (buyers_pred > 0.5).astype(int)\n",
    "        buyer_acc = np.mean(buyer_pred_binary == buyers_true)\n",
    "        \n",
    "        # % de buyers correctamente identificados\n",
    "        buyers_mask = buyers_true > 0.5\n",
    "        if buyers_mask.sum() > 0:\n",
    "            buyer_recall = np.mean(buyers_pred[buyers_mask] > 0.5)\n",
    "        else:\n",
    "            buyer_recall = 0.0\n",
    "    else:\n",
    "        auc = None\n",
    "        buyer_acc = None\n",
    "        buyer_recall = None\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} EVALUATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MSLE (Primary Metric):  {msle:.6f}\")\n",
    "    print(f\"MAE:                     {mae:.2f}\")\n",
    "    \n",
    "    if auc is not None:\n",
    "        print(f\"\\nBuyer Classification:\")\n",
    "        print(f\"  AUC:                  {auc:.4f}\")\n",
    "        print(f\"  Accuracy (t=0.5):     {buyer_acc:.4f}\")\n",
    "        print(f\"  Buyer Recall:         {buyer_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\nRevenue Distribution:\")\n",
    "    print(f\"  Pred mean:            ${np.mean(preds):.2f}\")\n",
    "    print(f\"  Pred median:          ${np.median(preds):.2f}\")\n",
    "    print(f\"  Pred max:             ${np.max(preds):.2f}\")\n",
    "    print(f\"  True mean:            ${np.mean(trues):.2f}\")\n",
    "    print(f\"  True median:          ${np.median(trues):.2f}\")\n",
    "    print(f\"  True max:             ${np.max(trues):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'msle': msle,\n",
    "        'mae': mae,\n",
    "        'auc': auc,\n",
    "        'buyer_acc': buyer_acc,\n",
    "        'buyer_recall': buyer_recall,\n",
    "        'preds': preds,\n",
    "        'trues': trues\n",
    "    }\n",
    "\n",
    "print(\"‚úì Evaluation utilities defined (V3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6a0ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T18:29:56.638615Z",
     "iopub.status.idle": "2025-11-15T18:29:56.639040Z",
     "shell.execute_reply": "2025-11-15T18:29:56.638844Z",
     "shell.execute_reply.started": "2025-11-15T18:29:56.638825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEACHER MODEL EVALUATION\n",
      "==================================================\n",
      "Teacher MSLE: 3.536911\n",
      "Teacher Buyer AUC: 0.6124\n",
      "\n",
      "==================================================\n",
      "STUDENT MODEL EVALUATION\n",
      "==================================================\n",
      "Teacher MSLE: 3.536911\n",
      "Teacher Buyer AUC: 0.6124\n",
      "\n",
      "==================================================\n",
      "STUDENT MODEL EVALUATION\n",
      "==================================================\n",
      "Student MSLE: 0.807849\n",
      "\n",
      "Baseline (all zeros) MSLE: 0.228072\n",
      "\n",
      "==================================================\n",
      "COMPARISON\n",
      "==================================================\n",
      "Teacher improvement: -1450.79%\n",
      "Student improvement: -254.21%\n",
      "Student vs Teacher gap: -77.16%\n",
      "\n",
      "Teacher predictions:\n",
      "  Mean: 5034707.5000, Median: 4.8450, Max: 118296944640.0000\n",
      "  % Non-zero: 100.00%\n",
      "\n",
      "Student predictions:\n",
      "  Mean: 7305063424.0000, Median: 1.0833, Max: 207266565324800.0000\n",
      "  % Non-zero: 100.00%\n",
      "Student MSLE: 0.807849\n",
      "\n",
      "Baseline (all zeros) MSLE: 0.228072\n",
      "\n",
      "==================================================\n",
      "COMPARISON\n",
      "==================================================\n",
      "Teacher improvement: -1450.79%\n",
      "Student improvement: -254.21%\n",
      "Student vs Teacher gap: -77.16%\n",
      "\n",
      "Teacher predictions:\n",
      "  Mean: 5034707.5000, Median: 4.8450, Max: 118296944640.0000\n",
      "  % Non-zero: 100.00%\n",
      "\n",
      "Student predictions:\n",
      "  Mean: 7305063424.0000, Median: 1.0833, Max: 207266565324800.0000\n",
      "  % Non-zero: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Teacher\n",
    "teacher_results = evaluate_model(teacher, val_loader, device=DEVICE, model_name='TEACHER')\n",
    "\n",
    "# Evaluate Student\n",
    "student_results = evaluate_model(student, val_loader, device=DEVICE, model_name='STUDENT')\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Teacher MSLE: {teacher_results['msle']:.6f}\")\n",
    "print(f\"Student MSLE: {student_results['msle']:.6f}\")\n",
    "print(f\"Difference:   {abs(teacher_results['msle'] - student_results['msle']):.6f}\")\n",
    "print(f\"\\nCompression: {teacher_params / student_params:.2f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4db1f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Este notebook combina:\n",
    "1. **Carga de datos robusta** del `simplified_model_comparison.ipynb` (manejo de parquet con Dask, sampling, preprocesado)\n",
    "2. **Modelos de deep learning** (Teacher-Student distillation en PyTorch)\n",
    "\n",
    "### Ventajas del Student Model:\n",
    "- **~50% menos par√°metros** que el teacher\n",
    "- **Inferencia m√°s r√°pida** para producci√≥n\n",
    "- **Aprende del teacher** (soft targets) adem√°s de los labels reales\n",
    "\n",
    "### Para producci√≥n:\n",
    "- Exportar el student a ONNX: `torch.onnx.export(student, ...)`\n",
    "- Aplicar quantization para reducir tama√±o y acelerar m√°s\n",
    "- Usar solo el student model (descartar teacher)\n",
    "\n",
    "### Pr√≥ximos pasos:\n",
    "- Ajustar hiperpar√°metros (learning rate, arquitectura, alpha)\n",
    "- Probar diferentes embedding dimensions\n",
    "- A√±adir m√°s √©pocas si hay suficiente memoria/tiempo\n",
    "- Implementar early stopping basado en validation loss"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14453560,
     "sourceId": 120867,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
